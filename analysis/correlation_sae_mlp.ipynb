{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch as t\n",
    "import numpy as np\n",
    "import einops\n",
    "import matplotlib.pyplot as plt\n",
    "from huggingface_hub import hf_hub_download\n",
    "import pickle\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "from circuits.dictionary_learning.buffer import NNsightActivationBuffer\n",
    "from circuits.dictionary_learning.dictionary import AutoEncoder, AutoEncoderNew, GatedAutoEncoder\n",
    "import circuits.othello_utils as othello_utils\n",
    "from circuits.utils import (\n",
    "    othello_hf_dataset_to_generator,\n",
    "    get_model,\n",
    "    get_submodule,\n",
    ")\n",
    "\n",
    "import circuits.analysis as analysis\n",
    "import circuits.utils as utils\n",
    "import circuits.othello_utils as othello_utils\n",
    "import circuits.chess_utils as chess_utils\n",
    "\n",
    "repo_dir = \"/home/adam/chess-gpt-circuits/\"\n",
    "# repo_dir = '/share/u/can/chess-gpt-circuits'\n",
    "device = 'cuda:0'\n",
    "\n",
    "othello = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if othello:\n",
    "    # download data from huggingface if needed\n",
    "    if not os.path.exists(f'{repo_dir}/autoencoders/othello_5-21'):\n",
    "        hf_hub_download(repo_id='adamkarvonen/othello_saes', filename='othello_5-21.zip', local_dir=f'{repo_dir}/autoencoders')\n",
    "        # unzip the data\n",
    "        os.system(f'unzip {repo_dir}/autoencoders/othello_5-21.zip -d autoencoders')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load SAE\n",
    "ae_type = 'p_anneal'\n",
    "trainer_id = 4\n",
    "\n",
    "if othello:\n",
    "    ae_path = f'{repo_dir}/autoencoders/othello_5-21/othello-{ae_type}/trainer{trainer_id}'\n",
    "else:\n",
    "    ae_path = f'{repo_dir}/autoencoders/chess-trained_model-layer_5-2024-05-23/chess-trained_model-layer_5-{ae_type}/trainer{trainer_id}'\n",
    "\n",
    "\n",
    "if ae_type == 'standard' or ae_type == 'p_anneal':\n",
    "    ae = AutoEncoder.from_pretrained(os.path.join(ae_path, 'ae.pt'), device='cuda:0')\n",
    "elif ae_type == 'gated' or ae_type == 'gated_anneal':\n",
    "    ae = GatedAutoEncoder.from_pretrained(os.path.join(ae_path, 'ae.pt'), device='cuda:0')\n",
    "elif ae_type == 'standard_new':\n",
    "    ae = AutoEncoderNew.from_pretrained(os.path.join(ae_path, 'ae.pt'), device='cuda:0')\n",
    "else:\n",
    "    raise ValueError('Invalid ae_type')\n",
    "\n",
    "print(ae.encoder.weight.shape)\n",
    "ae_hidden_dim = ae.encoder.weight.shape[0]\n",
    "d_model = ae.encoder.weight.shape[1]\n",
    "\n",
    "t.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "d_model = 512  # output dimension of the layer\n",
    "layer = 5\n",
    "\n",
    "if not othello:\n",
    "    with open(\"models/meta.pkl\", \"rb\") as f:\n",
    "        meta = pickle.load(f)\n",
    "\n",
    "    context_length = 256\n",
    "    model_name = \"adamkarvonen/8LayerChessGPT2\"\n",
    "    dataset_name = \"adamkarvonen/chess_sae_text\"\n",
    "    data = utils.chess_hf_dataset_to_generator(\n",
    "        dataset_name, meta, context_length=context_length, split=\"train\", streaming=True\n",
    "    )\n",
    "    model_type = \"chess\"\n",
    "else:\n",
    "    context_length = 59\n",
    "    model_name = \"Baidicoot/Othello-GPT-Transformer-Lens\"\n",
    "    dataset_name = \"taufeeque/othellogpt\"\n",
    "    data = utils.othello_hf_dataset_to_generator(\n",
    "        dataset_name, context_length=context_length, split=\"train\", streaming=True\n",
    "    )\n",
    "    model_type = \"othello\"\n",
    "\n",
    "model = utils.get_model(model_name, device)\n",
    "submodule = utils.get_submodule(model_name, layer, model)\n",
    "\n",
    "mlp_post_submodules = [model.blocks[layer].mlp.hook_post for layer in range(model.cfg.n_layers)]\n",
    "\n",
    "batch_size = 8\n",
    "total_games_size = batch_size * 10\n",
    "\n",
    "buffer = NNsightActivationBuffer(\n",
    "    data,\n",
    "    model,\n",
    "    submodule,\n",
    "    n_ctxs=8e3,\n",
    "    ctx_len=context_length,\n",
    "    refresh_batch_size=batch_size,\n",
    "    io=\"out\",\n",
    "    d_submodule=d_model,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single SAE feature ~ all MLP neurons\n",
    "Fixing a single SAE feature, which MLP neurons (in earlier and later layers) show high pearson correlation with the SAE feature?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From feature viz notebook: Feature #21 of f'{repo_dir}/autoencoders/group-2024-05-17_othello/group-2024-05-17_othello-{standard_new}/trainer{0}' looks like it is representing a piece on H1 or G1\n",
    "\n",
    "<img src=\"./feat21.png\" alt=\"Image description\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cosine_similarities_for_mlp_neuron(model, feat_idx: int, layer: int, y_vectors_DF: t.Tensor) -> t.Tensor:\n",
    "    d_model_vec = model.blocks[layer].mlp.W_out[feat_idx, :]\n",
    "\n",
    "    y_vectors_FD = einops.rearrange(y_vectors_DF, 'd f -> f d')\n",
    "\n",
    "    cosine_similarities = F.cosine_similarity(d_model_vec, y_vectors_FD)\n",
    "\n",
    "    return cosine_similarities\n",
    "\n",
    "def get_max_cos_sim_for_all_mlp_neurons(model, layer: int, y_vectors_DF: t.Tensor) -> t.Tensor:\n",
    "    max_cos_sims = []\n",
    "\n",
    "    for neuron_idx in range(model.blocks[layer].mlp.W_out.shape[0]):\n",
    "        max_cos_sims.append(get_cosine_similarities_for_mlp_neuron(model, neuron_idx, layer, y_vectors_DF).max())\n",
    "    return t.stack(max_cos_sims)\n",
    "\n",
    "def get_cosine_similarities_for_ae_decoder_neuron(ae, feat_idx: int, layer: int, y_vectors_FD: t.Tensor) -> t.Tensor:\n",
    "    d_model_vec = ae.decoder.weight[:, feat_idx]\n",
    "    return F.cosine_similarity(d_model_vec, y_vectors_FD)\n",
    "\n",
    "def get_max_cos_sim_for_all_ae_decoder_neurons(ae, layer: int, y_vectors_FD: t.Tensor) -> t.Tensor:\n",
    "    max_cos_sims = []\n",
    "\n",
    "    for neuron_idx in range(ae.decoder.weight.shape[1]):\n",
    "        max_cos_sims.append(get_cosine_similarities_for_ae_decoder_neuron(ae, neuron_idx, layer, y_vectors_FD).max())\n",
    "    return t.stack(max_cos_sims)\n",
    "\n",
    "layer = 5\n",
    "\n",
    "# cos sims for an individual neuron\n",
    "mlp_neuron_idx = 1024\n",
    "cos_sims_1024 = get_cosine_similarities_for_mlp_neuron(model, mlp_neuron_idx, layer, ae.decoder.weight)\n",
    "print(f'Cos sim of mlp neuron {mlp_neuron_idx} with SAE feature 2192: {cos_sims_1024[2192]}')\n",
    "print(f'Cos sim of mlp neuron {mlp_neuron_idx} with SAE feature 3098: {cos_sims_1024[3098]}')\n",
    "print(f'Maximum cosine similarity of mlp neuron {mlp_neuron_idx}: {cos_sims_1024.max()}')\n",
    "\n",
    "# max cos sims for all mlp neurons\n",
    "max_cos_sims = get_max_cos_sim_for_all_ae_decoder_neurons(ae, layer, ae.encoder.weight)\n",
    "average_max_cos_sim = max_cos_sims.mean()\n",
    "print(average_max_cos_sim)\n",
    "\n",
    "print(len(max_cos_sims))\n",
    "print(t.sum(t.tensor(max_cos_sims) > 0.5).item())\n",
    "\n",
    "plt.title(f'SAE feature max cosine similarities with MLP neurons')\n",
    "plt.hist(max_cos_sims.cpu().numpy(), bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell caches mlp activations and SAE feature activations for all games in the dataset\n",
    "\n",
    "mlp_acts_bLF = {}\n",
    "for layer in range(model.cfg.n_layers):\n",
    "    mlp_acts_bLF[layer] = t.zeros((total_games_size, context_length, d_model * 4), dtype=t.float32, device=device)\n",
    "\n",
    "tokens_bL = t.zeros((total_games_size, context_length), dtype=t.int16, device=device)\n",
    "feature_activations_bLF = t.zeros((total_games_size, context_length, ae_hidden_dim), dtype=t.float32, device=device)\n",
    "\n",
    "for i in range(0, total_games_size, batch_size):\n",
    "    game_batch_BL = [next(data) for _ in range(batch_size)]\n",
    "    game_batch_BL = t.tensor(game_batch_BL, device=device)\n",
    "    with t.no_grad(), model.trace(game_batch_BL, scan=False, validate=False):\n",
    "        x_BLD = submodule.output\n",
    "        feature_acts_BLF = ae.encode(x_BLD).save()\n",
    "        for layer in range(model.cfg.n_layers):\n",
    "            mlp_acts_bLF[layer][i:i+batch_size] = mlp_post_submodules[layer].output.save()\n",
    "    tokens_bL[i:i+batch_size] = game_batch_BL\n",
    "    feature_activations_bLF[i:i+batch_size] = feature_acts_BLF\n",
    "\n",
    "feature_activations_Fb = einops.rearrange(feature_activations_bLF, \"B S F -> F (B S)\")\n",
    "\n",
    "for layer in mlp_acts_bLF:\n",
    "    mlp_acts_bLF[layer] = einops.rearrange(mlp_acts_bLF[layer], \"B S F -> F (B S)\")\n",
    "mlp_acts_Fb = {layer: mlp_acts_bLF[layer] for layer in mlp_acts_bLF}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pearson correlation calculation function\n",
    "def pearson_corr(x, y):\n",
    "    mean_x = x.mean(dim=-1, keepdim=True)\n",
    "    mean_y = y.mean(dim=-1, keepdim=True)\n",
    "    xm = x - mean_x\n",
    "    ym = y - mean_y\n",
    "    r_num = t.sum(xm * ym, dim=-1)\n",
    "    r_den = t.sqrt(t.sum(xm * xm, dim=-1) * t.sum(ym * ym, dim=-1))\n",
    "    \n",
    "    with t.no_grad():\n",
    "        zero_variance = (r_den == 0)\n",
    "    r = t.where(zero_variance, t.zeros_like(r_num), r_num / r_den)\n",
    "\n",
    "    return r\n",
    "\n",
    "\n",
    "def get_correlation_for_activation(\n",
    "    x_activations_Fb: t.Tensor, x_activation_index: int, y_activations_Fb: t.Tensor\n",
    ") -> t.Tensor:\n",
    "    x_activation_b = x_activations_Fb[x_activation_index]\n",
    "    correlations = t.zeros(y_activations_Fb.shape[0])\n",
    "\n",
    "    for i in range(y_activations_Fb.shape[0]):\n",
    "        y_activation_b = y_activations_Fb[i]\n",
    "        corr = pearson_corr(x_activation_b, y_activation_b)\n",
    "        correlations[i] = corr\n",
    "\n",
    "    return correlations\n",
    "\n",
    "mlp_neuron_idx = 1024\n",
    "\n",
    "feat_idx = 21\n",
    "\n",
    "# sae_correlations = get_correlation_for_activation(feature_activations_Fb, feat_idx, mlp_acts_Fb[5])\n",
    "mlp_correlations = get_correlation_for_activation(mlp_acts_Fb[5], mlp_neuron_idx, feature_activations_Fb)\n",
    "sae_correlations = get_correlation_for_activation(feature_activations_Fb, feat_idx, feature_activations_Fb)\n",
    "\n",
    "# Calculate Pearson correlation\n",
    "# pearson_correlations = {}\n",
    "\n",
    "# for layer in mlp_acts:\n",
    "#     mlp_acts_layer = mlp_acts[layer]\n",
    "#     correlations = t.zeros(mlp_acts_layer.shape[0])\n",
    "#     for i in range(mlp_acts_layer.shape[0]):\n",
    "#         mlp_feature = mlp_acts_layer[i]\n",
    "#         corr = pearson_corr(feature_acts_BLF, mlp_feature)\n",
    "#         correlations[i] = corr\n",
    "#     pearson_correlations[layer] = correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max_cors_sae)\n",
    "# print average correlation\n",
    "\n",
    "# remove all 0s\n",
    "max_cors_sae_filtered = [x for x in max_cors_sae if x != 0]\n",
    "\n",
    "print(t.mean(t.tensor(max_cors_sae_filtered)).item())\n",
    "\n",
    "plt.title('Max correlation of any MLP neuron with each SAE feature')\n",
    "plt.hist(max_cors_sae_filtered, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of matches with correlation > 0.5\n",
    "print(len(max_cors_sae_filtered))\n",
    "print(t.sum(t.tensor(max_cors_sae_filtered) > 0.5).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mlp_correlations.shape)\n",
    "\n",
    "\n",
    "def analyze_correlations(correlations: t.Tensor):\n",
    "    \n",
    "    k = 20\n",
    "    values, indices = t.topk(correlations, k)\n",
    "\n",
    "    # Printing the top n values and their corresponding indices\n",
    "    for index, value in zip(indices, values):\n",
    "        print(f\"Index: {index}, Value: {value.item()}\")\n",
    "    print()\n",
    "\n",
    "    plt.hist(correlations.cpu().numpy(), bins=100)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "analyze_correlations(sae_correlations)\n",
    "analyze_correlations(mlp_correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mlp_acts_Fb[5].shape)\n",
    "\n",
    "max_cors_sae = []\n",
    "\n",
    "# for i in tqdm(range(mlp_acts_Fb[5].shape[0])):\n",
    "#     mlp_correlations = get_correlation_for_activation(mlp_acts_Fb[5], i, feature_activations_Fb)\n",
    "#     max_correlation = t.max(mlp_correlations)\n",
    "#     max_cors.append(max_correlation.item())\n",
    "\n",
    "for i in tqdm(range(feature_activations_Fb.shape[0])):\n",
    "    sae_correlations = get_correlation_for_activation(feature_activations_Fb, i, mlp_acts_Fb[5])\n",
    "    max_correlation = t.max(sae_correlations)\n",
    "    max_cors_sae.append(max_correlation.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prepare data\n",
    "# layers = list(pearson_correlations.keys())\n",
    "# data = [pearson_correlations[l].abs() for l in layers]\n",
    "\n",
    "# # Create stacked histogram\n",
    "# plt.hist(data, bins=100, histtype='bar', stacked=True, label=layers)\n",
    "\n",
    "# # Add legend and log scale for y-axis\n",
    "# plt.legend(title='Layer')\n",
    "# plt.yscale('log')\n",
    "\n",
    "# # Display plot\n",
    "# plt.xlabel('Absolute Pearson Correlation')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.title('Stacked Histogram of Pearson Correlations by Layer')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save indices and layer for pearson_correlations above a certain threshold\n",
    "# corr_threshold = 0.5\n",
    "# indices = {}\n",
    "# for layer in pearson_correlations:\n",
    "#     indices[layer] = t.where(pearson_correlations[layer].abs() > corr_threshold)[0]\n",
    "\n",
    "# indices"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
