{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "# Imports\n",
    "\n",
    "\n",
    "import torch as t\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "import numpy as np\n",
    "import einops\n",
    "\n",
    "from circuits.utils import (\n",
    "    othello_hf_dataset_to_generator,\n",
    "    get_model,\n",
    "    get_aes\n",
    ")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "# device = 'cuda:0'\n",
    "device = 'cpu'\n",
    "repo_dir = '/share/u/can/OthelloUnderstanding'\n",
    "\n",
    "# Import model\n",
    "model_name = \"Baidicoot/Othello-GPT-Transformer-Lens\"\n",
    "model = get_model(model_name, device)\n",
    "aes = get_aes(node_type='mlp_neuron', repo_dir=repo_dir)\n",
    "ae0 = aes[0]\n",
    "\n",
    "# Load data\n",
    "context_length = 59\n",
    "activation_dim = 512  # output dimension of the layer\n",
    "batch_size = 64\n",
    "dataset_name = \"taufeeque/othellogpt\"\n",
    "data = othello_hf_dataset_to_generator(\n",
    "    dataset_name, context_length=context_length, split=\"train\", streaming=True\n",
    ")\n",
    "games_batch = [next(data) for _ in range(batch_size)]\n",
    "games_batch = t.tensor(games_batch, device=device)\n",
    "\n",
    "tracer_kwargs = {'scan': False, 'validate': False}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation:\n",
    "- Cosine sim of embedding and unembedding is insigificant, see cell below. \n",
    "- However, mean ablating specific tokens in the embedding seems to have a high effect in the logit directions\n",
    "\n",
    "### Setup\n",
    "1. Only most recent move, start with single token: 10 == C1\n",
    "    - get a batch of game sequences up until a specific move\n",
    "    - get activation diffs for embedding, attn0, resid_mid for mean ablating the token\n",
    "    - DLA, unembed those activation differences\n",
    "2. Ablation at any position\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison of model embeddings\n",
    "we_norm = t.norm(model.W_E, dim=-1)\n",
    "wu_norm = t.norm(model.W_U, dim=0)\n",
    "embedding_cos_sim = model.W_E @ model.W_U / (we_norm[:, None] * wu_norm[None, :])\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(embedding_cos_sim.cpu().detach().numpy(), cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.title('Cosine similarity between embedding and unembedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get mean embeddings per position across games batch\n",
    "with t.no_grad(), model.trace(games_batch, **tracer_kwargs):\n",
    "    # embed_acts = model.blocks[0].hook_resid_pre.output.save()\n",
    "    embed_acts = model.hook_embed.output.save()\n",
    "mean_embeds = embed_acts.sum(dim=0) # shape [pos, dm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logit diff direction on a single batch of inputs for mean ablating input tokens\n",
    "# find index where games_batch[0] is 10\n",
    "\n",
    "game = t.tensor(games_batch[0])\n",
    "move_token_id = 10\n",
    "move_pos = t.where(game == move_token_id)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get clean embed, attn0, resid_mid_0 acts\n",
    "with t.no_grad(), model.trace(game, **tracer_kwargs):\n",
    "    embed_act_clean = model.blocks[0].hook_resid_pre.output.save()\n",
    "    attn0_act_clean = model.blocks[0].hook_attn_out.output.save()\n",
    "    resid_mid_0_act_clean = model.blocks[0].hook_resid_mid.output.save()\n",
    "\n",
    "# Get patch embed, attn0, resid_mid_0 acts\n",
    "embed_act_patch = embed_act_clean.clone()\n",
    "batch_arange = t.arange(embed_act_patch.shape[0])\n",
    "embed_act_patch[batch_arange, move_pos] = mean_embeds[batch_arange, move_pos]\n",
    "\n",
    "with t.no_grad(), model.trace(game, **tracer_kwargs):\n",
    "    model.blocks[0].hook_resid_pre.output = embed_act_patch\n",
    "    attn0_act_patch = model.blocks[0].hook_attn_out.output.save()\n",
    "    resid_mid_0_act_patch = model.blocks[0].hook_resid_mid.output.save()\n",
    "\n",
    "# Get diff in activations\n",
    "embed_act_diff = embed_act_patch - embed_act_clean\n",
    "attn0_act_diff = attn0_act_patch - attn0_act_clean\n",
    "resid_mid_0_act_diff = resid_mid_0_act_patch - resid_mid_0_act_clean\n",
    "\n",
    "# Normalize diff in activations\n",
    "embed_act_diff_norm = t.norm(embed_act_diff, dim=-1, keepdim=True)\n",
    "attn0_act_diff_norm = t.norm(attn0_act_diff, dim=-1, keepdim=True)\n",
    "resid_mid_0_act_diff_norm = t.norm(resid_mid_0_act_diff, dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get diff in logit directions\n",
    "WU_norm = t.norm(model.W_U[:, 1:], dim=0, keepdim=True)\n",
    "\n",
    "embed_dla = embed_act_diff @ model.W_U[:, 1:] / (embed_act_diff_norm * WU_norm)\n",
    "attn0_dla = attn0_act_diff @ model.W_U[:, 1:] / (attn0_act_diff_norm * WU_norm)\n",
    "resid_mid_0_dla = resid_mid_0_act_diff @ model.W_U[:, 1:] / (resid_mid_0_act_diff_norm * WU_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import feature_viz_othello_utils\n",
    "importlib.reload(feature_viz_othello_utils) \n",
    "\n",
    "batch_test_idx = 0\n",
    "pos_test_idx = 0\n",
    "player = feature_viz_othello_utils.BoardPlayer(game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player.next()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "feature_viz_othello_utils.visualize_vocab(ax, resid_mid_0_dla[batch_test_idx, pos_test_idx], device)\n",
    "ax.set_title(f'Resid Mid 0 DLA of logit diff, pos {pos_test_idx} in game {batch_test_idx}')\n",
    "pos_test_idx+=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evidence towards attn heads map embedding space to unembedding space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_OV = einops.einsum(model.W_V[0], model.W_O[0], 'nh dm1 dh, nh dh dm2 -> nh dm1 dm2')\n",
    "W_OV.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_mean = mean_embeds[10, :]\n",
    "t_E = model.W_E[10, :]\n",
    "t_U = model.W_U[:, 10]\n",
    "t_U_norm = t_U / t.norm(t_U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_outs = W_OV @ t_E\n",
    "t_outs_norm = t_outs / t.norm(t_outs, dim=-1, keepdim=True)\n",
    "\n",
    "t_outs_mean = W_OV @ t_mean\n",
    "t_outs_mean_norm = t_outs_mean / t.norm(t_outs_mean, dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim_out_u = t_outs_mean_norm @ t_U_norm\n",
    "cos_sim_out_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mats-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
