{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from huggingface_hub import hf_hub_download\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "import numpy as np\n",
    "import einops\n",
    "from tqdm import tqdm\n",
    "from typing import Callable, Optional\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "from circuits.dictionary_learning.dictionary import AutoEncoder, AutoEncoderNew, GatedAutoEncoder, IdentityDict\n",
    "from circuits.utils import (\n",
    "    othello_hf_dataset_to_generator,\n",
    "    get_model,\n",
    "    get_submodule,\n",
    "    get_mlp_activations_submodule,\n",
    ")\n",
    "\n",
    "from feature_viz_othello_utils import (\n",
    "    get_acts_IEs_VN,\n",
    "    plot_lenses,\n",
    "    plot_mean_metrics,\n",
    "    plot_top_k_games,\n",
    "    BoardPlayer,\n",
    ")\n",
    "\n",
    "import circuits.utils as utils\n",
    "import circuits.analysis as analysis\n",
    "import feature_viz_othello_utils as viz_utils\n",
    "import circuits.othello_utils as othello_utils\n",
    "from circuits.othello_engine_utils import to_board_label, to_string, to_int, stoi_indices #to_string: mode_output_vocab to interpretable square index\n",
    "import circuits.eval_sae_as_classifier as eval_sae\n",
    "\n",
    "device = 'cuda:0'\n",
    "tracer_kwargs = {'validate' : False, 'scan' : False}\n",
    "repo_dir = '/share/u/can/chess-gpt-circuits'\n",
    "repo_dir = \"/home/adam/chess-gpt-circuits\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model, submodule, ae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Baidicoot/Othello-GPT-Transformer-Lens\"\n",
    "model = get_model(model_name, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 5\n",
    "\n",
    "def get_ae(layer: int):\n",
    "    # node_type = \"sae_feature\"\n",
    "    node_type = \"mlp_neuron\"\n",
    "\n",
    "\n",
    "    if node_type == \"sae_feature\":\n",
    "        ae_group_name = 'all_layers_othello_p_anneal_0530_with_lines'\n",
    "        ae_type = 'p_anneal'\n",
    "        trainer_id = 0\n",
    "        ae_path = f'{repo_dir}/autoencoders/{ae_group_name}/layer_{layer}/trainer{trainer_id}'\n",
    "        submodule = get_submodule(model_name, layer, model)\n",
    "    elif node_type == \"mlp_neuron\":\n",
    "        ae_group_name = 'othello_mlp_acts_identity_aes_lines' # with_lines\n",
    "        ae_type = 'identity'\n",
    "        ae_path = f'{repo_dir}/autoencoders/{ae_group_name}/layer_{layer}'\n",
    "        submodule = get_mlp_activations_submodule(model_name, layer, model)\n",
    "    else:\n",
    "        raise ValueError('Invalid node_type')\n",
    "\n",
    "    # download data from huggingface if needed\n",
    "    if not os.path.exists(f'{repo_dir}/autoencoders/{ae_group_name}'):\n",
    "        hf_hub_download(repo_id='adamkarvonen/othello_saes', filename=f'{ae_group_name}.zip', local_dir=f'{repo_dir}/autoencoders')\n",
    "        # unzip the data\n",
    "        os.system(f'unzip {repo_dir}/autoencoders/{ae_group_name}.zip -d {repo_dir}/autoencoders')\n",
    "\n",
    "    # Initialize the autoencoder\n",
    "    if ae_type == 'standard' or ae_type == 'p_anneal':\n",
    "        ae = AutoEncoder.from_pretrained(os.path.join(ae_path, 'ae.pt'), device='cuda:0')\n",
    "    elif ae_type == 'gated' or ae_type == 'gated_anneal':\n",
    "        ae = GatedAutoEncoder.from_pretrained(os.path.join(ae_path, 'ae.pt'), device='cuda:0')\n",
    "    elif ae_type == 'standard_new':\n",
    "        ae = AutoEncoderNew.from_pretrained(os.path.join(ae_path, 'ae.pt'), device='cuda:0')\n",
    "    elif ae_type == 'identity':\n",
    "        ae = IdentityDict()\n",
    "    else:\n",
    "        raise ValueError('Invalid ae_type')\n",
    "\n",
    "    return ae, submodule\n",
    "\n",
    "ae, submodule = get_ae(layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load legal move neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for each neuron, we need the full hypothesis [feature_idx, valid_square_idx, [config_idxs]]\n",
    "# import json\n",
    "# with open(os.path.join(ae_path, 'hpc_hrc_same_square_indexes_dict.json'), 'r') as f:\n",
    "#     hpc_hrc_same_square_indexes_dict = json.load(f)\n",
    "\n",
    "# print(hpc_hrc_same_square_indexes_dict.keys())\n",
    "# print(hpc_hrc_same_square_indexes_dict['high_precision_and_recall'])\n",
    "# print(hpc_hrc_same_square_indexes_dict['intersection_FSqC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feat_idxs, valid_square_idxs, line_idxs = t.tensor(hpc_hrc_same_square_indexes_dict['intersection_FSqC']).T\n",
    "# unique_feat_idxs = t.unique(feat_idxs)\n",
    "# unique_valid_square_idxs = t.unique(valid_square_idxs)\n",
    "# n_unique_feat_idxs = len(unique_feat_idxs)\n",
    "\n",
    "# for feat_idx, square_idx in zip(unique_feat_idxs, unique_valid_square_idxs):\n",
    "#     print(feat_idx, square_idx)\n",
    "#     config_idxs_per_feat = line_idxs[feat_idxs == feat_idx]\n",
    "#     feat_idx = int(feat_idx)\n",
    "#     square_idx = int(square_idx)\n",
    "#     if feat_idx > 500:\n",
    "#         break\n",
    "\n",
    "# print(feat_idx, square_idx, config_idxs_per_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = 500\n",
    "\n",
    "ablation_dataset_name = \"othello_ablation_dataset.pkl\"\n",
    "\n",
    "if os.path.exists(ablation_dataset_name):\n",
    "    print(\"Loading ablation dataset\")\n",
    "    with open(ablation_dataset_name, \"rb\") as f:\n",
    "        ablation_data = pickle.load(f)\n",
    "else:\n",
    "    ablation_data = eval_sae.construct_othello_dataset(\n",
    "        custom_functions=[\n",
    "            othello_utils.games_batch_to_state_stack_length_lines_mine_BLRRC,\n",
    "            othello_utils.games_batch_to_valid_moves_BLRRC,\n",
    "        ],\n",
    "        n_inputs=dataset_size,\n",
    "        split=\"train\",\n",
    "        precompute_dataset=True,\n",
    "        device=device,\n",
    "    )\n",
    "    print(\"Saving ablation dataset\")\n",
    "    with open(ablation_dataset_name, \"wb\") as f:\n",
    "        pickle.dump(ablation_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_any_present(train_data, valid_move_idx, line_idxs):\n",
    "    '''for every position in every game, evaluate whether any of the lines is present'''\n",
    "    r = valid_move_idx // 8\n",
    "    c = valid_move_idx % 8\n",
    "    data_BLC = train_data['games_batch_to_state_stack_length_lines_mine_BLRRC'][:, :, r, c, :]\n",
    "    B, L, C = data_BLC.shape\n",
    "\n",
    "    lines_C = t.zeros(C, dtype=t.int64)\n",
    "    lines_C[line_idxs] = 1\n",
    "\n",
    "    data_with_lines_BLC = data_BLC * lines_C\n",
    "    any_line_present_BL = einops.reduce(data_with_lines_BLC, 'b L C -> b L', 'sum') > 0\n",
    "    return any_line_present_BL\n",
    "\n",
    "def check_valid_move(train_data: dict, valid_move_idx: int) -> t.Tensor:\n",
    "    '''for every position in every game, evaluate whether there is a valid move on the square'''\n",
    "    r = valid_move_idx // 8\n",
    "    c = valid_move_idx % 8\n",
    "    data_BL1 = train_data['games_batch_to_valid_moves_BLRRC'][:, :, r, c]\n",
    "    data_BL = data_BL1.squeeze().bool()\n",
    "    return data_BL\n",
    "\n",
    "def game_state_where_line_present(train_data, valid_move_idx, line_idxs):\n",
    "    any_line_present = check_any_present(train_data, valid_move_idx, line_idxs)\n",
    "    enc_inputs = t.tensor(train_data['encoded_inputs'], device=device)\n",
    "    game_state_where_line_present = []\n",
    "    for game, line_present in zip(enc_inputs, any_line_present):\n",
    "        if line_present.sum() > 0:\n",
    "            first_occurence = t.where(line_present)[0][0]\n",
    "            game_state_where_line_present.append(game[:first_occurence+1])\n",
    "    return game_state_where_line_present\n",
    "\n",
    "# square_test = check_valid_move(ablation_data, square_idx)\n",
    "# print(square_test.shape)\n",
    "\n",
    "# def game_state_my_move_before_line_present(train_data, valid_move_idx, line_idxs):\n",
    "#     game_state_where_line_present = game_state_where_line_present(train_data, valid_move_idx, line_idxs)\n",
    "#     return [game[:-2] for game in game_state_where_line_present]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Test\n",
    "# game_states_line_present = game_state_where_line_present(ablation_data, square_idx, config_idxs_per_feat)\n",
    "# game = game_states_line_present[1]\n",
    "# game = t.tensor(game, device=device)\n",
    "# player = BoardPlayer(game)\n",
    "\n",
    "# square_idx, config_idxs_per_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# player.next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 1 game, where any line is present "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def steering(model, game_batch, submodule, ae, feat_idx, square_idx, steering_factor, device='cpu'):\n",
    "    \n",
    "\n",
    "#     for game, V_square_idx, V_rotated_square_idx, C_square_idx in zip(game_batch):\n",
    "#         V_square_idx = square_idx\n",
    "#     V_row = V_square_idx // 8\n",
    "#     V_col = V_square_idx % 8\n",
    "#     V_rotated_row = 7 - V_row\n",
    "#     V_rotated_col = 7 - V_col\n",
    "#     V_rotated_square_idx = V_rotated_row * 8 + V_rotated_col\n",
    "#     C_square_idxs = game[-2]\n",
    "#     # Clean forward pass\n",
    "#     with t.no_grad(), model.trace(game_batch, **tracer_kwargs):\n",
    "#         x = submodule.output\n",
    "#         f = ae.encode(x).save() # shape: [batch_size, seq_len, n_features]\n",
    "#         logits_clean = model.unembed.output.save() # batch_size x seq_len x vocab_size\n",
    "\n",
    "#     steering_value = f[:, -1, feat_idx] # Activation value where valid_move is present\n",
    "    \n",
    "#     # Steering forward pass\n",
    "#     with t.no_grad(), model.trace(game_batch, **tracer_kwargs):\n",
    "#         x = submodule.output\n",
    "#         f = ae.encode(x).save() # shape: [batch_size, seq_len, n_features]\n",
    "#         f[:, :, feat_idx] = steering_value * steering_factor\n",
    "#         submodule.output = ae.decode(f)\n",
    "#         logits_steer = model.unembed.output.save() # batch_size x seq_len x vocab_size\n",
    "\n",
    "#     # Logit diffs for t-2\n",
    "#     arange_batch = t.arange(batch_size, device=device)\n",
    "#     logit_diff_clean = logits_clean[arange_batch, -3, to_int(V_square_idxs)] - logits_clean[arange_batch, -3, to_int(C_square_idxs)]\n",
    "#     logit_diff_steer = logits_steer[arange_batch, -3, to_int(V_square_idxs)] - logits_steer[arange_batch, -3, to_int(C_square_idxs)]\n",
    "\n",
    "#     rotated_logit_diff_clean = logits_clean[arange_batch, -3, to_int(V_rotated_square_idxs)] - logits_clean[arange_batch, -3, to_int(C_square_idxs)]\n",
    "#     rotated_logit_diff_steer = logits_steer[arange_batch, -3, to_int(V_rotated_square_idxs)] - logits_steer[arange_batch, -3, to_int(C_square_idxs)]\n",
    "    \n",
    "#     steer_clean_diff = logit_diff_steer - logit_diff_clean\n",
    "#     rotated_steer_clean_diff = rotated_logit_diff_steer - rotated_logit_diff_clean\n",
    "#     return steer_clean_diff, rotated_steer_clean_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def steering(model, game_batch, submodule, ae, feat_idx, square_idx, steering_factor, timestep=-2, device='cpu'):\n",
    "#     batch_size = len(game_batch)\n",
    "#     steer_clean_diffs = t.zeros(batch_size, device=device)\n",
    "#     rotated_steer_clean_diffs = t.zeros(batch_size, device=device)\n",
    "#     boards_clean = t.zeros(batch_size, 64, device=device)\n",
    "#     boards_steer = t.zeros(batch_size, 64, device=device)\n",
    "\n",
    "#     for i, game in tqdm(enumerate(game_batch), desc='Steering Batch', total=batch_size):\n",
    "#         V_square_idx = square_idx\n",
    "#         V_row = V_square_idx // 8\n",
    "#         V_col = V_square_idx % 8\n",
    "#         V_rotated_row = 7 - V_row\n",
    "#         V_rotated_col = 7 - V_col\n",
    "#         V_rotated_square_idx = V_rotated_row * 8 + V_rotated_col\n",
    "#         C_square_idx = game[-2]\n",
    "\n",
    "#         # Clean forward pass\n",
    "#         with t.no_grad(), model.trace(game, **tracer_kwargs):\n",
    "#             x = submodule.output\n",
    "#             f = ae.encode(x).save() # shape: [batch_size, seq_len, n_features]\n",
    "#             logits_clean = model.unembed.output.save() # batch_size x seq_len x vocab_size\n",
    "\n",
    "#         steering_value = f[:, -1, feat_idx] # Activation value where valid_move is present\n",
    "        \n",
    "#         # Steering forward pass\n",
    "#         with t.no_grad(), model.trace(game, **tracer_kwargs):\n",
    "#             x = submodule.output\n",
    "#             f = ae.encode(x).save() # shape: [batch_size, seq_len, n_features]\n",
    "#             f[:, :, feat_idx] = steering_value * steering_factor\n",
    "#             submodule.output = ae.decode(f)\n",
    "#             logits_steer = model.unembed.output.save() # batch_size x seq_len x vocab_size\n",
    "\n",
    "#         # Logit diffs for t-2\n",
    "#         logit_diff_clean = logits_clean[:, timestep, to_int(V_square_idx)] - logits_clean[:, timestep, to_int(C_square_idx)]\n",
    "#         logit_diff_steer = logits_steer[:, timestep, to_int(V_square_idx)] - logits_steer[:, timestep, to_int(C_square_idx)]\n",
    "\n",
    "#         rotated_logit_diff_clean = logits_clean[:, timestep, to_int(V_rotated_square_idx)] - logits_clean[:, timestep, to_int(C_square_idx)]\n",
    "#         rotated_logit_diff_steer = logits_steer[:, timestep, to_int(V_rotated_square_idx)] - logits_steer[:, timestep, to_int(C_square_idx)]\n",
    "    \n",
    "#         steer_clean_diffs[i] = logit_diff_steer - logit_diff_clean\n",
    "#         rotated_steer_clean_diffs[i] = rotated_logit_diff_steer - rotated_logit_diff_clean\n",
    "#         boards_clean[i][stoi_indices] = logits_clean[0, timestep, 1:]\n",
    "#         boards_steer[i][stoi_indices] = logits_steer[0, timestep, 1:]\n",
    "\n",
    "#     return steer_clean_diffs, rotated_steer_clean_diffs, boards_clean, boards_steer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# square_idx, config_idxs_per_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# game_batch_example = game_state_where_line_present(train_data_example, square_idx, config_idxs_per_feat)\n",
    "# print('Number of games where line is present:', len(game_batch_example))\n",
    "# for steering_factor in [0.1, 0.5, 1.0, 2.0, 5.0, 10.0]:\n",
    "#     steer_clean_diff, rotated_steer_clean_diff, boards_clean, boards_steer = steering(\n",
    "#         model, \n",
    "#         game_batch_example, \n",
    "#         submodule, \n",
    "#         ae, \n",
    "#         feat_idx, \n",
    "#         square_idx, \n",
    "#         steering_factor,\n",
    "#         timestep=-1,\n",
    "#         device=device\n",
    "#         )\n",
    "#     steer_clean_diff = steer_clean_diff.mean()\n",
    "#     rotated_steer_clean_diff = rotated_steer_clean_diff.mean()\n",
    "#     print(f\"Steering factor: {steering_factor}, steer_clean_diff: {steer_clean_diff.item() - rotated_steer_clean_diff.item()}, rotated_steer_clean_diff: {rotated_steer_clean_diff.item()}\")\n",
    "#     # plt.imshow(boards_clean[3].view(8, 8).cpu().numpy() - boards_steer[0].view(8, 8).cpu().numpy())\n",
    "#     # plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ablation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "single batch of size 100\n",
    "track number of times the condition is present\n",
    "\n",
    "do clean forward pass\n",
    "do mean ablation / zero ablation forward pass.\n",
    "\n",
    "compute difference in IEs. \n",
    "Do Activation patching for HRC and HPC neurons first\n",
    "Test wheter activation patching is also feasible for..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_patching(\n",
    "    model,\n",
    "    train_data: dict,\n",
    "    submodule,\n",
    "    ae,\n",
    "    feat_idxs: t.Tensor,\n",
    "    square_idx: int,\n",
    "    config_idxs_per_feat,\n",
    "    ablation_method=\"zero\",\n",
    "    device=\"cpu\",\n",
    "    filter_for_valid_moves: bool = False,\n",
    "):\n",
    "    allowed_methods = [\"mean\", \"zero\", \"max\"]\n",
    "    assert ablation_method in allowed_methods, f\"Invalid ablation method. Must be one of {allowed_methods}\"\n",
    "    game_batch = t.tensor(train_data[\"encoded_inputs\"])\n",
    "\n",
    "    # Get clean logits and mean submodule activations\n",
    "    with t.no_grad(), model.trace(game_batch, **tracer_kwargs):\n",
    "        x = submodule.output\n",
    "        if ablation_method == \"mean\":\n",
    "            f = ae.encode(x)  # shape: [batch_size, seq_len, n_features]\n",
    "            # print(f.shape)\n",
    "            # f_mean_clean = einops.reduce(f, 'b l f -> f', 'mean')#.save()\n",
    "            f_mean_clean = f.mean(dim=(0, 1)).save()\n",
    "            # print(f_mean_clean.shape)\n",
    "        elif ablation_method == \"max\":\n",
    "            f = ae.encode(x)  # shape: [batch_size, seq_len, n_features]\n",
    "            f_max_clean = f.max(dim=0).values\n",
    "            f_max_clean = f_max_clean.max(dim=0).values.save()\n",
    "        logits_clean_BLV = model.unembed.output.save()  # batch_size x seq_len x vocab_size\n",
    "\n",
    "    # Get patch logits\n",
    "    with t.no_grad(), model.trace(game_batch, **tracer_kwargs):\n",
    "        x = submodule.output\n",
    "        f = ae.encode(x)\n",
    "        if ablation_method == \"mean\":\n",
    "            f[:, :, feat_idxs] = f_mean_clean[feat_idxs]\n",
    "        elif ablation_method == \"max\":\n",
    "            f[:, :, feat_idxs] = f_max_clean[feat_idxs]\n",
    "        else:\n",
    "            f[:, :, feat_idxs] = 0\n",
    "        submodule.output = ae.decode(f)\n",
    "        logits_patch_BLV = model.unembed.output.save()\n",
    "\n",
    "    B, L , V = logits_clean_BLV.shape\n",
    "\n",
    "    logit_diff_BLV = logits_patch_BLV - logits_clean_BLV\n",
    "    logit_diff_BL = logit_diff_BLV[:, :, to_int(square_idx)]\n",
    "\n",
    "    probs_clean_BLV = t.nn.functional.softmax(logits_clean_BLV, dim=-1)\n",
    "    probs_patch_BLV = t.nn.functional.softmax(logits_patch_BLV, dim=-1)\n",
    "\n",
    "    probs_diff_BLV = probs_patch_BLV - probs_clean_BLV\n",
    "    probs_diff_BL = probs_diff_BLV[:, :, to_int(square_idx)]\n",
    "\n",
    "    # \n",
    "    probs_diff_others_BLV = probs_diff_BLV[:, :, t.arange(V) != to_int(square_idx)]\n",
    "\n",
    "\n",
    "    if filter_for_valid_moves:\n",
    "        # Filter for valid moves on square_idx\n",
    "        custom_mask_BL = check_valid_move(train_data, square_idx)\n",
    "    else:\n",
    "        # Filter for a list of lines in config_idxs_per_feat\n",
    "        custom_mask_BL = check_any_present(train_data, square_idx, config_idxs_per_feat)\n",
    "\n",
    "    probs_diff_others_present_BLV = probs_diff_others_BLV[custom_mask_BL]\n",
    "    probs_diff_others_absent_BLV = probs_diff_others_BLV[~custom_mask_BL]\n",
    "\n",
    "    logit_diff_line_present_BL = logit_diff_BL[custom_mask_BL]\n",
    "    logit_diff_line_absent_BL = logit_diff_BL[~custom_mask_BL]\n",
    "\n",
    "    probs_diff_line_present_BL = probs_diff_BL[custom_mask_BL]\n",
    "    probs_diff_line_absent_BL = probs_diff_BL[~custom_mask_BL]\n",
    "\n",
    "    logits_present_BLV = logits_patch_BLV[custom_mask_BL]\n",
    "    logits_absent_BLV = logits_patch_BLV[~custom_mask_BL]\n",
    "\n",
    "    return (\n",
    "        logit_diff_line_present_BL,\n",
    "        logit_diff_line_absent_BL,\n",
    "        custom_mask_BL,\n",
    "        probs_diff_line_present_BL,\n",
    "        probs_diff_line_absent_BL,\n",
    "        logits_absent_BLV,\n",
    "        logits_present_BLV,\n",
    "        probs_diff_others_present_BLV,\n",
    "        probs_diff_others_absent_BLV,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 400\n",
    "n_batches = 1\n",
    "\n",
    "\n",
    "def extract_batch(data, batch_idx, batch_size):\n",
    "    start_idx = batch_idx * batch_size\n",
    "    end_idx = start_idx + batch_size\n",
    "    batch_data = {\n",
    "        key: (\n",
    "            value[start_idx:end_idx]\n",
    "            if isinstance(value, list)\n",
    "            else value[start_idx:end_idx].clone()\n",
    "        )\n",
    "        for key, value in data.items()\n",
    "    }\n",
    "    return batch_data\n",
    "\n",
    "\n",
    "def get_all_neurons_for_square(intersection_FSqC: list, square_of_interest: int) -> t.Tensor:\n",
    "    unique_neurons = set()\n",
    "    for feat_idx, square_idx, config_idxs_per_feat in intersection_FSqC:\n",
    "        if square_idx == square_of_interest:\n",
    "            unique_neurons.add(feat_idx)\n",
    "    return list(unique_neurons)\n",
    "\n",
    "\n",
    "def get_square_for_neuron(intersection_FSqC: list, feat_idx: int) -> int:\n",
    "    for feat_idx_, square_idx, config_idxs_per_feat in intersection_FSqC:\n",
    "        if feat_idx_ == feat_idx:\n",
    "            return square_idx\n",
    "    raise ValueError(f\"Neuron {feat_idx} not found in intersection_FSqC\")\n",
    "\n",
    "\n",
    "def run_ablations(\n",
    "    ablation_method: str, feat_idxs: t.Tensor, square_idx: int, config_idxs_per_feat: Optional[t.Tensor] = None\n",
    "):\n",
    "\n",
    "    logit_diffs_line_present = []\n",
    "    logit_diffs_line_absent = []\n",
    "    line_masks = []\n",
    "    probs_diffs_line_present = []\n",
    "    probs_diffs_line_absent = []\n",
    "\n",
    "    if config_idxs_per_feat is None:\n",
    "        filter_for_valid_moves = True\n",
    "    else:\n",
    "        filter_for_valid_moves = False\n",
    "\n",
    "    # Iterate over the number of batches\n",
    "    for batch_idx in range(n_batches):\n",
    "        train_data = extract_batch(ablation_data, batch_idx, batch_size)\n",
    "\n",
    "        (\n",
    "            logit_diff_line_present,\n",
    "            logit_diff_line_absent,\n",
    "            line_mask,\n",
    "            probs_diff_line_present,\n",
    "            probs_diff_line_absent,\n",
    "            logits_absent_BLV,\n",
    "            logits_present_BLV,\n",
    "            probs_diff_others_present_BLV,\n",
    "            probs_diff_others_absent_BLV,\n",
    "        ) = activation_patching(\n",
    "            model,\n",
    "            train_data,\n",
    "            submodule,\n",
    "            ae,\n",
    "            feat_idxs,\n",
    "            square_idx,\n",
    "            config_idxs_per_feat,\n",
    "            ablation_method=ablation_method,\n",
    "            device=device,\n",
    "            filter_for_valid_moves=filter_for_valid_moves,\n",
    "        )\n",
    "\n",
    "        if logit_diff_line_present.numel() > 0:\n",
    "            logit_diffs_line_present.append(logit_diff_line_present.mean())\n",
    "            probs_diffs_line_present.append(probs_diff_line_present.mean())\n",
    "        probs_diffs_line_absent.append(probs_diff_line_absent.mean())\n",
    "        logit_diffs_line_absent.append(logit_diff_line_absent.mean())\n",
    "        line_masks.append(line_mask.float().mean())\n",
    "\n",
    "        # print(f'batch {batch_idx}')\n",
    "        # print(f'mean logit diff for line present: {logit_diff_line_present.mean()}')\n",
    "        # print(f'mean logit diff for line absent: {logit_diff_line_absent.mean()}')\n",
    "        # print(f'fraction of line present: {line_mask.float().mean()}')\n",
    "\n",
    "    try:\n",
    "        print(f\"mean logit diff for condition present: {t.stack(logit_diffs_line_present).mean()}\")\n",
    "        print(f\"mean logit diff for condition absent: {t.stack(logit_diffs_line_absent).mean()}\")\n",
    "        print(\n",
    "            f\"mean probability diff for condition present: {t.stack(probs_diffs_line_present).mean()}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"mean probability diff for condition absent: {t.stack(probs_diffs_line_absent).mean()}\"\n",
    "        )\n",
    "        print(f\"fraction of condition present: {t.stack(line_masks).mean()}\")\n",
    "    except:\n",
    "        print(\"No valid moves\")\n",
    "\n",
    "    return (\n",
    "        logit_diff_line_present,\n",
    "        logit_diff_line_absent,\n",
    "        line_mask,\n",
    "        probs_diff_line_present,\n",
    "        probs_diff_line_absent,\n",
    "        logits_absent_BLV,\n",
    "        logits_present_BLV,\n",
    "        probs_diff_others_present_BLV,\n",
    "        probs_diff_others_absent_BLV,\n",
    "    )\n",
    "\n",
    "pickle_dict = {}\n",
    "ablation_method = 'zero'\n",
    "ablation_method = 'mean'\n",
    "ablation_method = 'max'\n",
    "\n",
    "\n",
    "for layer in range(0, 8):\n",
    "# for layer in [5]:\n",
    "    pickle_dict[layer] = {}\n",
    "\n",
    "    ae, submodule = get_ae(layer)\n",
    "\n",
    "    filename = 'union_neurons-T0.95.json'\n",
    "    with open(filename, 'r') as f:\n",
    "        union_neurons = json.load(f)\n",
    "\n",
    "    print(union_neurons.keys())\n",
    "    print(union_neurons[str(layer)])\n",
    "\n",
    "    neurons = {}\n",
    "\n",
    "    for feat_idx, square_idx, length, dir in union_neurons[str(layer)]:\n",
    "        feat_idx = int(feat_idx)\n",
    "        square_idx = int(square_idx)\n",
    "        line_index = length * 8 + dir\n",
    "        if feat_idx not in neurons:\n",
    "            neurons[feat_idx] = {}\n",
    "            neurons[feat_idx]['square'] = square_idx\n",
    "            neurons[feat_idx]['lines'] = [line_index]\n",
    "        else:\n",
    "            neurons[feat_idx]['lines'].append(line_index)\n",
    "\n",
    "    print(neurons.keys())\n",
    "    print(len(neurons))\n",
    "\n",
    "    all_probs_diff_line_present = []\n",
    "    all_probs_diff_others_present = []\n",
    "\n",
    "    for i, feat_idx in enumerate(neurons):\n",
    "        square_idx = neurons[feat_idx]['square']\n",
    "        config_idxs_per_feat = neurons[feat_idx]['lines']\n",
    "        print(feat_idx, square_idx, config_idxs_per_feat)\n",
    "        \n",
    "        (\n",
    "            logit_diff_line_present,\n",
    "            logit_diff_line_absent,\n",
    "            line_mask,\n",
    "            probs_diff_line_present,\n",
    "            probs_diff_line_absent,\n",
    "            logits_absent_BLV,\n",
    "            logits_present_BLV,\n",
    "            probs_diff_others_present,\n",
    "            probs_diff_others_absent,\n",
    "        ) = run_ablations(ablation_method, feat_idx, square_idx, config_idxs_per_feat)\n",
    "\n",
    "        all_probs_diff_line_present.append(probs_diff_line_present)\n",
    "        all_probs_diff_others_present.append(probs_diff_others_present)\n",
    "\n",
    "        # if i >= 10:\n",
    "        #     break\n",
    "\n",
    "    if len(all_probs_diff_line_present) == 0:\n",
    "        continue\n",
    "    probs_diff_line_present = t.cat(all_probs_diff_line_present, dim=0)\n",
    "    probs_diff_others_present = t.cat(all_probs_diff_others_present, dim=0)\n",
    "\n",
    "    pickle_dict[layer] = {\n",
    "        'probs_diff_line_present': probs_diff_line_present,\n",
    "        'probs_diff_line_absent': probs_diff_line_absent,\n",
    "        'probs_diff_others_present': probs_diff_others_present,\n",
    "        'probs_diff_others_absent': probs_diff_others_absent,\n",
    "\n",
    "    }\n",
    "\n",
    "with open(f'all_layers_probs_method_{ablation_method}_diff.pkl', 'wb') as f:\n",
    "    pickle.dump(pickle_dict, f)\n",
    "\n",
    "# Analyze every square with all neurons per square\n",
    "# for square_idx in range(64):\n",
    "#     square_valid_move_neurons = get_all_neurons_for_square(\n",
    "#         hpc_hrc_same_square_indexes_dict[\"intersection_FSqC\"], square_idx\n",
    "#     )\n",
    "\n",
    "#     print(square_valid_move_neurons)\n",
    "\n",
    "#     print(f\"\\nFor square {square_idx}, there are {len(square_valid_move_neurons)} neurons\")\n",
    "#     if len(square_valid_move_neurons) > 0:\n",
    "#         logits_absent_BLV, logits_present_BLV = run_ablations(t.tensor(square_valid_move_neurons), square_idx)\n",
    "\n",
    "#     if square_idx >= 0:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import torch as t\n",
    "\n",
    "ablation_method = 'max'\n",
    "ablation_method = 'mean'\n",
    "\n",
    "with open(f'all_layers_probs_method_{ablation_method}_diff.pkl', 'rb') as f:\n",
    "    pickle_dict = pickle.load(f)\n",
    "\n",
    "key1 = 'probs_diff_line_present'\n",
    "key2 = 'probs_diff_others_present'\n",
    "\n",
    "if ablation_method == \"max\":\n",
    "    key1 = 'probs_diff_line_absent'\n",
    "    key2 = 'probs_diff_others_absent'\n",
    "\n",
    "def get_layers_results(layers: list[int]) -> tuple[t.Tensor, t.Tensor]:\n",
    "    all_probs_diff_line_present = []\n",
    "    all_probs_diff_others_present = []\n",
    "\n",
    "    for layer in layers:\n",
    "        if layer not in pickle_dict or len(pickle_dict[layer]) == 0:\n",
    "            continue\n",
    "        layer_probs_diff = pickle_dict[layer][key1]\n",
    "        layer_probs_diff_others = pickle_dict[layer][key2]\n",
    "        all_probs_diff_line_present.append(layer_probs_diff)\n",
    "        all_probs_diff_others_present.append(layer_probs_diff_others)\n",
    "    total_probs_diff_line_present = t.cat(all_probs_diff_line_present, dim=0)\n",
    "    total_probs_diff_others_present = t.cat(all_probs_diff_others_present, dim=0)\n",
    "\n",
    "    return total_probs_diff_line_present, total_probs_diff_others_present\n",
    "\n",
    "layers = [0, 1, 2, 3, 4, 5, 6]\n",
    "# layers = [6, 7]\n",
    "# layers = [7]\n",
    "# layers = [5]\n",
    "\n",
    "probs_diff_line_present, probs_diff_others_present = get_layers_results(layers)\n",
    "\n",
    "density = False\n",
    "# density = True\n",
    "\n",
    "print(probs_diff_line_present.shape)\n",
    "print(probs_diff_others_present.shape)\n",
    "\n",
    "print(probs_diff_line_present.mean())\n",
    "print(probs_diff_others_present.mean())\n",
    "\n",
    "plt.xlabel('Token Probability Difference')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plot_title = 'Histogram of Probability Differences for Layer(s) '\n",
    "for layer in layers:\n",
    "    plot_title += f'{layer}, '\n",
    "\n",
    "plot_title = plot_title[:-2]\n",
    "\n",
    "plot_title += f\"\\n\\nAverage probability difference for neuron square token: {probs_diff_line_present.mean().item():.4f}\"\n",
    "plot_title += f\"\\nAverage probability difference for all other tokens: {probs_diff_others_present.mean().item():.4f}\"\n",
    "\n",
    "plt.title(plot_title)\n",
    "\n",
    "x = plt.hist(probs_diff_line_present[abs(probs_diff_line_present)>1e-8].cpu().numpy().flatten(), bins=100, alpha=0.5, label=\"Neuron Square Token\", density=density)\n",
    "plt.hist(probs_diff_others_present[abs(probs_diff_others_present)>1e-8].cpu().numpy().flatten(), bins=100, alpha=0.5, label=\"All Other Tokens\", density=density)\n",
    "print(x)\n",
    "# plt.xlim(-0.3, 0.3)\n",
    "# plt.ylim(0, 40000)\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# plt.hist(logits_present_BLV.cpu().numpy().flatten(), bins=100, alpha=0.5, label=\"present\")\n",
    "# plt.hist(logits_absent_BLV.cpu().numpy().flatten(), bins=100, alpha=0.5, label=\"absent\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
