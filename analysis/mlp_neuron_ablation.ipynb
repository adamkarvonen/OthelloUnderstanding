{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from huggingface_hub import hf_hub_download\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "import numpy as np\n",
    "import einops\n",
    "from tqdm import tqdm\n",
    "from typing import Callable\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from circuits.dictionary_learning.dictionary import AutoEncoder, AutoEncoderNew, GatedAutoEncoder, IdentityDict\n",
    "from circuits.utils import (\n",
    "    othello_hf_dataset_to_generator,\n",
    "    get_model,\n",
    "    get_submodule,\n",
    "    get_mlp_activations_submodule,\n",
    ")\n",
    "\n",
    "from feature_viz_othello_utils import (\n",
    "    get_acts_IEs_VN,\n",
    "    plot_lenses,\n",
    "    plot_mean_metrics,\n",
    "    plot_top_k_games,\n",
    "    BoardPlayer,\n",
    ")\n",
    "\n",
    "import circuits.utils as utils\n",
    "import circuits.analysis as analysis\n",
    "import feature_viz_othello_utils as viz_utils\n",
    "from circuits.othello_utils import games_batch_to_state_stack_length_lines_mine_BLRCC\n",
    "from circuits.othello_engine_utils import to_board_label, to_string, to_int, stoi_indices #to_string: mode_output_vocab to interpretable square index\n",
    "import circuits.eval_sae_as_classifier as eval_sae\n",
    "\n",
    "device = 'cuda:0'\n",
    "tracer_kwargs = {'validate' : False, 'scan' : False}\n",
    "repo_dir = '/share/u/can/chess-gpt-circuits'\n",
    "repo_dir = \"/home/adam/chess-gpt-circuits\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model, submodule, ae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Baidicoot/Othello-GPT-Transformer-Lens\"\n",
    "model = get_model(model_name, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 5\n",
    "# node_type = \"sae_feature\"\n",
    "node_type = \"mlp_neuron\"\n",
    "\n",
    "\n",
    "if node_type == \"sae_feature\":\n",
    "    ae_group_name = 'all_layers_othello_p_anneal_0530_with_lines'\n",
    "    ae_type = 'p_anneal'\n",
    "    trainer_id = 0\n",
    "    ae_path = f'{repo_dir}/autoencoders/{ae_group_name}/layer_{layer}/trainer{trainer_id}'\n",
    "    submodule = get_submodule(model_name, layer, model)\n",
    "elif node_type == \"mlp_neuron\":\n",
    "    ae_group_name = 'othello_mlp_acts_identity_aes_lines' # with_lines\n",
    "    ae_type = 'identity'\n",
    "    ae_path = f'{repo_dir}/autoencoders/{ae_group_name}/layer_{layer}'\n",
    "    submodule = get_mlp_activations_submodule(model_name, layer, model)\n",
    "else:\n",
    "    raise ValueError('Invalid node_type')\n",
    "\n",
    "# download data from huggingface if needed\n",
    "if not os.path.exists(f'{repo_dir}/autoencoders/{ae_group_name}'):\n",
    "    hf_hub_download(repo_id='adamkarvonen/othello_saes', filename=f'{ae_group_name}.zip', local_dir=f'{repo_dir}/autoencoders')\n",
    "    # unzip the data\n",
    "    os.system(f'unzip {repo_dir}/autoencoders/{ae_group_name}.zip -d {repo_dir}/autoencoders')\n",
    "\n",
    "# Initialize the autoencoder\n",
    "if ae_type == 'standard' or ae_type == 'p_anneal':\n",
    "    ae = AutoEncoder.from_pretrained(os.path.join(ae_path, 'ae.pt'), device='cuda:0')\n",
    "elif ae_type == 'gated' or ae_type == 'gated_anneal':\n",
    "    ae = GatedAutoEncoder.from_pretrained(os.path.join(ae_path, 'ae.pt'), device='cuda:0')\n",
    "elif ae_type == 'standard_new':\n",
    "    ae = AutoEncoderNew.from_pretrained(os.path.join(ae_path, 'ae.pt'), device='cuda:0')\n",
    "elif ae_type == 'identity':\n",
    "    ae = IdentityDict()\n",
    "else:\n",
    "    raise ValueError('Invalid ae_type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load legal move neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each neuron, we need the full hypothesis [feature_idx, valid_square_idx, [config_idxs]]\n",
    "import json\n",
    "with open(os.path.join(ae_path, 'hpc_hrc_same_square_indexes_dict.json'), 'r') as f:\n",
    "    hpc_hrc_same_square_indexes_dict = json.load(f)\n",
    "\n",
    "print(hpc_hrc_same_square_indexes_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_idxs, valid_square_idxs, line_idxs = t.tensor(hpc_hrc_same_square_indexes_dict['intersection_FSqC']).T\n",
    "unique_feat_idxs = t.unique(feat_idxs)\n",
    "unique_valid_square_idxs = t.unique(valid_square_idxs)\n",
    "n_unique_feat_idxs = len(unique_feat_idxs)\n",
    "\n",
    "for feat_idx, square_idx in zip(unique_feat_idxs, unique_valid_square_idxs):\n",
    "    config_idxs_per_feat = line_idxs[feat_idxs == feat_idx]\n",
    "    feat_idx = int(feat_idx)\n",
    "    square_idx = int(square_idx)\n",
    "    if feat_idx > 500:\n",
    "        break\n",
    "\n",
    "print(feat_idx, square_idx, config_idxs_per_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset_size = 500\n",
    "\n",
    "ablation_dataset_name = \"othello_ablation_dataset.pkl\"\n",
    "\n",
    "if os.path.exists(ablation_dataset_name):\n",
    "    print(\"Loading ablation dataset\")\n",
    "    with open(ablation_dataset_name, \"rb\") as f:\n",
    "        ablation_data = pickle.load(f)\n",
    "else:\n",
    "    ablation_data = eval_sae.construct_othello_dataset(\n",
    "        custom_functions=[games_batch_to_state_stack_length_lines_mine_BLRCC],\n",
    "        n_inputs=dataset_size,\n",
    "        split='train',\n",
    "        precompute_dataset = True,\n",
    "        device=device,\n",
    "    )\n",
    "    print(\"Saving ablation dataset\")\n",
    "    with open(ablation_dataset_name, \"wb\") as f:\n",
    "        pickle.dump(ablation_data, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_any_present(train_data, valid_move_idx, line_idxs):\n",
    "    '''for every position in every game, evaluate whether any of the lines is present'''\n",
    "    r = valid_move_idx // 8\n",
    "    c = valid_move_idx % 8\n",
    "    data = train_data['games_batch_to_state_stack_length_lines_mine_BLRCC'][:, :, r, c]\n",
    "    line_idxs_expanded = line_idxs.view(1, 1, -1).expand(data.shape[0], data.shape[1], -1)\n",
    "    data = t.gather(data, dim=-1, index=line_idxs_expanded)\n",
    "    any_line_present = t.any(data, dim=-1)\n",
    "    return any_line_present\n",
    "\n",
    "def game_state_where_line_present(train_data, valid_move_idx, line_idxs):\n",
    "    any_line_present = check_any_present(train_data, valid_move_idx, line_idxs)\n",
    "    enc_inputs = t.tensor(train_data['encoded_inputs'], device=device)\n",
    "    game_state_where_line_present = []\n",
    "    for game, line_present in zip(enc_inputs, any_line_present):\n",
    "        if line_present.sum() > 0:\n",
    "            first_occurence = t.where(line_present)[0][0]\n",
    "            game_state_where_line_present.append(game[:first_occurence+1])\n",
    "    return game_state_where_line_present\n",
    "\n",
    "# def game_state_my_move_before_line_present(train_data, valid_move_idx, line_idxs):\n",
    "#     game_state_where_line_present = game_state_where_line_present(train_data, valid_move_idx, line_idxs)\n",
    "#     return [game[:-2] for game in game_state_where_line_present]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test\n",
    "game_states_line_present = game_state_where_line_present(ablation_data, square_idx, config_idxs_per_feat)\n",
    "game = game_states_line_present[1]\n",
    "game = t.tensor(game, device=device)\n",
    "player = BoardPlayer(game)\n",
    "\n",
    "square_idx, config_idxs_per_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player.next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 1 game, where any line is present "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def steering(model, game_batch, submodule, ae, feat_idx, square_idx, steering_factor, device='cpu'):\n",
    "    \n",
    "\n",
    "#     for game, V_square_idx, V_rotated_square_idx, C_square_idx in zip(game_batch):\n",
    "#         V_square_idx = square_idx\n",
    "#     V_row = V_square_idx // 8\n",
    "#     V_col = V_square_idx % 8\n",
    "#     V_rotated_row = 7 - V_row\n",
    "#     V_rotated_col = 7 - V_col\n",
    "#     V_rotated_square_idx = V_rotated_row * 8 + V_rotated_col\n",
    "#     C_square_idxs = game[-2]\n",
    "#     # Clean forward pass\n",
    "#     with t.no_grad(), model.trace(game_batch, **tracer_kwargs):\n",
    "#         x = submodule.output\n",
    "#         f = ae.encode(x).save() # shape: [batch_size, seq_len, n_features]\n",
    "#         logits_clean = model.unembed.output.save() # batch_size x seq_len x vocab_size\n",
    "\n",
    "#     steering_value = f[:, -1, feat_idx] # Activation value where valid_move is present\n",
    "    \n",
    "#     # Steering forward pass\n",
    "#     with t.no_grad(), model.trace(game_batch, **tracer_kwargs):\n",
    "#         x = submodule.output\n",
    "#         f = ae.encode(x).save() # shape: [batch_size, seq_len, n_features]\n",
    "#         f[:, :, feat_idx] = steering_value * steering_factor\n",
    "#         submodule.output = ae.decode(f)\n",
    "#         logits_steer = model.unembed.output.save() # batch_size x seq_len x vocab_size\n",
    "\n",
    "#     # Logit diffs for t-2\n",
    "#     arange_batch = t.arange(batch_size, device=device)\n",
    "#     logit_diff_clean = logits_clean[arange_batch, -3, to_int(V_square_idxs)] - logits_clean[arange_batch, -3, to_int(C_square_idxs)]\n",
    "#     logit_diff_steer = logits_steer[arange_batch, -3, to_int(V_square_idxs)] - logits_steer[arange_batch, -3, to_int(C_square_idxs)]\n",
    "\n",
    "#     rotated_logit_diff_clean = logits_clean[arange_batch, -3, to_int(V_rotated_square_idxs)] - logits_clean[arange_batch, -3, to_int(C_square_idxs)]\n",
    "#     rotated_logit_diff_steer = logits_steer[arange_batch, -3, to_int(V_rotated_square_idxs)] - logits_steer[arange_batch, -3, to_int(C_square_idxs)]\n",
    "    \n",
    "#     steer_clean_diff = logit_diff_steer - logit_diff_clean\n",
    "#     rotated_steer_clean_diff = rotated_logit_diff_steer - rotated_logit_diff_clean\n",
    "#     return steer_clean_diff, rotated_steer_clean_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def steering(model, game_batch, submodule, ae, feat_idx, square_idx, steering_factor, timestep=-2, device='cpu'):\n",
    "    batch_size = len(game_batch)\n",
    "    steer_clean_diffs = t.zeros(batch_size, device=device)\n",
    "    rotated_steer_clean_diffs = t.zeros(batch_size, device=device)\n",
    "    boards_clean = t.zeros(batch_size, 64, device=device)\n",
    "    boards_steer = t.zeros(batch_size, 64, device=device)\n",
    "\n",
    "    for i, game in tqdm(enumerate(game_batch), desc='Steering Batch', total=batch_size):\n",
    "        V_square_idx = square_idx\n",
    "        V_row = V_square_idx // 8\n",
    "        V_col = V_square_idx % 8\n",
    "        V_rotated_row = 7 - V_row\n",
    "        V_rotated_col = 7 - V_col\n",
    "        V_rotated_square_idx = V_rotated_row * 8 + V_rotated_col\n",
    "        C_square_idx = game[-2]\n",
    "\n",
    "        # Clean forward pass\n",
    "        with t.no_grad(), model.trace(game, **tracer_kwargs):\n",
    "            x = submodule.output\n",
    "            f = ae.encode(x).save() # shape: [batch_size, seq_len, n_features]\n",
    "            logits_clean = model.unembed.output.save() # batch_size x seq_len x vocab_size\n",
    "\n",
    "        steering_value = f[:, -1, feat_idx] # Activation value where valid_move is present\n",
    "        \n",
    "        # Steering forward pass\n",
    "        with t.no_grad(), model.trace(game, **tracer_kwargs):\n",
    "            x = submodule.output\n",
    "            f = ae.encode(x).save() # shape: [batch_size, seq_len, n_features]\n",
    "            f[:, :, feat_idx] = steering_value * steering_factor\n",
    "            submodule.output = ae.decode(f)\n",
    "            logits_steer = model.unembed.output.save() # batch_size x seq_len x vocab_size\n",
    "\n",
    "        # Logit diffs for t-2\n",
    "        logit_diff_clean = logits_clean[:, timestep, to_int(V_square_idx)] - logits_clean[:, timestep, to_int(C_square_idx)]\n",
    "        logit_diff_steer = logits_steer[:, timestep, to_int(V_square_idx)] - logits_steer[:, timestep, to_int(C_square_idx)]\n",
    "\n",
    "        rotated_logit_diff_clean = logits_clean[:, timestep, to_int(V_rotated_square_idx)] - logits_clean[:, timestep, to_int(C_square_idx)]\n",
    "        rotated_logit_diff_steer = logits_steer[:, timestep, to_int(V_rotated_square_idx)] - logits_steer[:, timestep, to_int(C_square_idx)]\n",
    "    \n",
    "        steer_clean_diffs[i] = logit_diff_steer - logit_diff_clean\n",
    "        rotated_steer_clean_diffs[i] = rotated_logit_diff_steer - rotated_logit_diff_clean\n",
    "        boards_clean[i][stoi_indices] = logits_clean[0, timestep, 1:]\n",
    "        boards_steer[i][stoi_indices] = logits_steer[0, timestep, 1:]\n",
    "\n",
    "    return steer_clean_diffs, rotated_steer_clean_diffs, boards_clean, boards_steer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "square_idx, config_idxs_per_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_batch_example = game_state_where_line_present(train_data_example, square_idx, config_idxs_per_feat)\n",
    "print('Number of games where line is present:', len(game_batch_example))\n",
    "for steering_factor in [0.1, 0.5, 1.0, 2.0, 5.0, 10.0]:\n",
    "    steer_clean_diff, rotated_steer_clean_diff, boards_clean, boards_steer = steering(\n",
    "        model, \n",
    "        game_batch_example, \n",
    "        submodule, \n",
    "        ae, \n",
    "        feat_idx, \n",
    "        square_idx, \n",
    "        steering_factor,\n",
    "        timestep=-1,\n",
    "        device=device\n",
    "        )\n",
    "    steer_clean_diff = steer_clean_diff.mean()\n",
    "    rotated_steer_clean_diff = rotated_steer_clean_diff.mean()\n",
    "    print(f\"Steering factor: {steering_factor}, steer_clean_diff: {steer_clean_diff.item() - rotated_steer_clean_diff.item()}, rotated_steer_clean_diff: {rotated_steer_clean_diff.item()}\")\n",
    "    # plt.imshow(boards_clean[3].view(8, 8).cpu().numpy() - boards_steer[0].view(8, 8).cpu().numpy())\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ablation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "single batch of size 100\n",
    "track number of times the condition is present\n",
    "\n",
    "do clean forward pass\n",
    "do mean ablation / zero ablation forward pass.\n",
    "\n",
    "compute difference in IEs. \n",
    "Do Activation patching for HRC and HPC neurons first\n",
    "Test wheter activation patching is also feasible for..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_patching(model, train_data, submodule, ae, feat_idx, square_idx, config_idxs_per_feat, ablation_method='zero', device='cpu'):\n",
    "    assert ablation_method in ['mean', 'zero'], \"Invalid ablation method. Must be one of ['mean', 'zero']\"\n",
    "    game_batch = t.tensor(train_data['encoded_inputs'])\n",
    "\n",
    "    # Get clean logits and mean submodule activations\n",
    "    with t.no_grad(), model.trace(game_batch, **tracer_kwargs):\n",
    "        x = submodule.output\n",
    "        if ablation_method == 'mean':\n",
    "            f = ae.encode(x) # shape: [batch_size, seq_len, n_features]\n",
    "            f_mean_clean = f.mean(dim=(0, 1))\n",
    "        logits_clean = model.unembed.output.save() # batch_size x seq_len x vocab_size\n",
    "\n",
    "    # Get patch logits\n",
    "    with t.no_grad(), model.trace(game_batch, **tracer_kwargs):\n",
    "        x = submodule.output\n",
    "        f = ae.encode(x)\n",
    "        if ablation_method == 'mean':\n",
    "            f[:, :, feat_idx] = f_mean_clean[feat_idx]\n",
    "        else:\n",
    "            f[:, :, feat_idx] = 0\n",
    "        submodule.output = ae.decode(f)\n",
    "        logits_patch = model.unembed.output.save()\n",
    "\n",
    "    logit_diff = logits_patch - logits_clean\n",
    "    logit_diff = logit_diff[:, :, to_int(square_idx)]\n",
    "    \n",
    "    line_mask = check_any_present(train_data, square_idx, config_idxs_per_feat)\n",
    "    logit_diff_line_present = logit_diff[line_mask]\n",
    "    logit_diff_line_absent = logit_diff[~line_mask]\n",
    "    return logit_diff_line_present, logit_diff_line_absent, line_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_idx, square_idx, config_idxs_per_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ablation_data.keys())\n",
    "print(ablation_data['encoded_inputs'])\n",
    "print(ablation_data['games_batch_to_state_stack_length_lines_mine_BLRCC'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=10\n",
    "n_batches=40\n",
    "\n",
    "\n",
    "def extract_batch(data, batch_idx, batch_size):\n",
    "    start_idx = batch_idx * batch_size\n",
    "    end_idx = start_idx + batch_size\n",
    "    batch_data = {\n",
    "        key: value[start_idx:end_idx] if isinstance(value, list) else value[start_idx:end_idx].clone()\n",
    "        for key, value in data.items()\n",
    "    }\n",
    "    return batch_data\n",
    "\n",
    "logit_diffs_line_present = []\n",
    "logit_diffs_line_absent = []\n",
    "line_masks = []\n",
    "\n",
    "# Iterate over the number of batches\n",
    "for batch_idx in range(n_batches):\n",
    "    train_data = extract_batch(ablation_data, batch_idx, batch_size)\n",
    "\n",
    "    logit_diff_line_present, logit_diff_line_absent, line_mask = activation_patching(\n",
    "        model,\n",
    "        train_data,\n",
    "        submodule,\n",
    "        ae,\n",
    "        feat_idx,\n",
    "        square_idx,\n",
    "        config_idxs_per_feat,\n",
    "        ablation_method='zero',\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    if logit_diff_line_present.numel() > 0:\n",
    "        logit_diffs_line_present.append(logit_diff_line_present.mean())\n",
    "    logit_diffs_line_absent.append(logit_diff_line_absent.mean())\n",
    "    line_masks.append(line_mask.float().mean())\n",
    "\n",
    "    # print(f'batch {batch_idx}')\n",
    "    # print(f'mean logit diff for line present: {logit_diff_line_present.mean()}')\n",
    "    # print(f'mean logit diff for line absent: {logit_diff_line_absent.mean()}')\n",
    "    # print(f'fraction of line present: {line_mask.float().mean()}')\n",
    "\n",
    "print(f'mean logit diff for line present: {t.stack(logit_diffs_line_present).mean()}')\n",
    "print(f'mean logit diff for line absent: {t.stack(logit_diffs_line_absent).mean()}')\n",
    "print(f'fraction of line present: {t.stack(line_masks).mean()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
