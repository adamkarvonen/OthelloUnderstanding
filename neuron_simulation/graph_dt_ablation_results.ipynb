{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import einops\n",
    "from collections import defaultdict\n",
    "from typing import Callable, Optional\n",
    "import os\n",
    "import importlib\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Tuple, Optional, Union\n",
    "\n",
    "import circuits.utils as utils\n",
    "import circuits.othello_utils as othello_utils\n",
    "import neuron_simulation.simulation_config as sim_config\n",
    "import neuron_simulation.simulate_activations_with_dts as sim_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_config = sim_config.selected_config\n",
    "device = \"cpu\"\n",
    "\n",
    "# If desired, you can run simulate_activations_with_dts.py from this cell\n",
    "# Values from default config will also be used later on in the notebook to filter out saved pickle files\n",
    "\n",
    "# Example of filtering out certain configurations\n",
    "\n",
    "# default_config.n_batches = 2\n",
    "\n",
    "# for combination in default_config.combinations:\n",
    "#     combination.ablate_not_selected = [True]\n",
    "\n",
    "# sim_activations.run_simulations(default_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ablation_pickle_files(\n",
    "    directory: str,\n",
    "    dataset_size: int,\n",
    "    ablation_method: str,\n",
    "    ablate_not_selected: bool,\n",
    "    add_error: bool,\n",
    "):\n",
    "    data = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".pkl\") and \"ablation\" in filename:\n",
    "            with open(os.path.join(directory, filename), \"rb\") as f:\n",
    "                single_data = pickle.load(f)\n",
    "\n",
    "            hyperparams = single_data[\"hyperparameters\"]\n",
    "            if hyperparams[\"ablate_not_selected\"] != ablate_not_selected:\n",
    "                continue\n",
    "\n",
    "            if hyperparams[\"input_location\"] != \"mlp_neuron\":\n",
    "                if hyperparams[\"add_error\"] != add_error:\n",
    "                    continue\n",
    "\n",
    "            if hyperparams[\"ablation_method\"] != ablation_method:\n",
    "                continue\n",
    "\n",
    "            if hyperparams[\"dataset_size\"] != dataset_size:\n",
    "                continue\n",
    "            data.append(single_data)\n",
    "    return data\n",
    "\n",
    "\n",
    "def extract_mean_ablation_results(\n",
    "    data: List[Dict],\n",
    "    desired_metric: str,\n",
    "    desired_layer_tuples: Optional[List[Tuple[int]]] = None,\n",
    ") -> Dict:\n",
    "    allowed_metrics = [\"kl\", \"patch_accuracy\"]\n",
    "    if desired_metric not in allowed_metrics:\n",
    "        raise ValueError(f\"desired_metric must be one of {allowed_metrics}\")\n",
    "\n",
    "    nested_results = {}\n",
    "    for run in data:\n",
    "        hyperparams = run[\"hyperparameters\"]\n",
    "        input_location = hyperparams[\"input_location\"] + \"_mean_ablate\"\n",
    "        trainer_id = hyperparams[\"trainer_id\"]\n",
    "\n",
    "        nested_results[input_location] = {trainer_id: {}}\n",
    "\n",
    "        for layer_tuple, func_results in run[\"results\"].items():\n",
    "            if desired_layer_tuples is not None and layer_tuple not in desired_layer_tuples:\n",
    "                continue\n",
    "            prev_result = None\n",
    "            for idx, func_name in enumerate(func_results):\n",
    "\n",
    "                result = func_results[func_name][desired_metric]\n",
    "                nested_results[input_location][trainer_id][layer_tuple] = result\n",
    "\n",
    "                if idx > 0:\n",
    "                    assert prev_result == result\n",
    "\n",
    "                prev_result = result\n",
    "\n",
    "    return nested_results\n",
    "\n",
    "\n",
    "def extract_ablation_results(\n",
    "    data: List[Dict],\n",
    "    custom_function_names: list[str],\n",
    "    desired_metric: str,\n",
    "    group_by: str,\n",
    "    input_location_filter: Optional[str] = None,\n",
    "    func_name_filter: Optional[str] = None,\n",
    "    desired_layer_tuples: Optional[List[Tuple[int]]] = None,\n",
    ") -> Dict:\n",
    "    allowed_metrics = [\"kl\", \"patch_accuracy\"]\n",
    "    if desired_metric not in allowed_metrics:\n",
    "        raise ValueError(f\"desired_metric must be one of {allowed_metrics}\")\n",
    "\n",
    "    group_by_options = [\"input_location\", \"custom_function\"]\n",
    "    if group_by not in group_by_options:\n",
    "        raise ValueError(f\"group_by must be one of {group_by_options}\")\n",
    "\n",
    "    nested_results = {}\n",
    "    for run in data:\n",
    "        hyperparams = run[\"hyperparameters\"]\n",
    "        input_location = hyperparams[\"input_location\"]\n",
    "        trainer_id = hyperparams[\"trainer_id\"]\n",
    "\n",
    "        if input_location_filter is not None and input_location != input_location_filter:\n",
    "            continue\n",
    "\n",
    "        for custom_function_name in custom_function_names:\n",
    "\n",
    "            if func_name_filter is not None and custom_function_name != func_name_filter:\n",
    "                continue\n",
    "\n",
    "            primary_key = input_location if group_by == \"input_location\" else custom_function_name\n",
    "\n",
    "            if primary_key not in nested_results:\n",
    "                nested_results[primary_key] = {}\n",
    "            if trainer_id not in nested_results[primary_key]:\n",
    "                nested_results[primary_key][trainer_id] = {}\n",
    "\n",
    "            for layer_tuple, func_results in run[\"results\"].items():\n",
    "                print(layer_tuple, desired_layer_tuples)\n",
    "                if desired_layer_tuples is not None and layer_tuple not in desired_layer_tuples:\n",
    "                    continue\n",
    "                if custom_function_name in func_results:\n",
    "                    result = func_results[custom_function_name][desired_metric]\n",
    "                    nested_results[primary_key][trainer_id][layer_tuple] = result\n",
    "\n",
    "    return nested_results\n",
    "\n",
    "\n",
    "def min_or_max_metric_per_layer(nested_data, min_or_max: str):\n",
    "    allowed_options = [\"min\", \"max\"]\n",
    "    if min_or_max not in allowed_options:\n",
    "        raise ValueError(f\"min_or_max must be one of {allowed_options}\")\n",
    "\n",
    "    sorted_metrics = {}\n",
    "    for input_location, trainer_ids in nested_data.items():\n",
    "        sorted_metrics[input_location] = {}\n",
    "        all_layers = set()\n",
    "        for layer_results in trainer_ids.values():\n",
    "            all_layers.update(layer_results.keys())\n",
    "\n",
    "        for layer in all_layers:\n",
    "            if min_or_max == \"min\":\n",
    "                min_kl = float(\"inf\")\n",
    "            else:\n",
    "                min_kl = float(\"-inf\")\n",
    "\n",
    "            for trainer_id, layer_results in trainer_ids.items():\n",
    "                if layer in layer_results:\n",
    "                    if min_or_max == \"min\":\n",
    "                        min_kl = min(min_kl, layer_results[layer])\n",
    "                    else:\n",
    "                        min_kl = max(min_kl, layer_results[layer])\n",
    "            sorted_metrics[input_location][layer] = min_kl\n",
    "\n",
    "    return sorted_metrics\n",
    "\n",
    "\n",
    "def plot_grouped_metrics(nested_metrics, ablation_method):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    all_layers = set()\n",
    "    for input_location, trainer_ids in nested_metrics.items():\n",
    "        for trainer_id, layer_results in trainer_ids.items():\n",
    "            all_layers.update(layer_results.keys())\n",
    "\n",
    "    all_layers = sorted(all_layers)\n",
    "    layer_labels = [str(layer) for layer in all_layers]\n",
    "\n",
    "    for input_location, trainer_ids in nested_metrics.items():\n",
    "        for trainer_id, layer_results in trainer_ids.items():\n",
    "            values = [layer_results.get(layer, np.nan) for layer in all_layers]\n",
    "            plt.plot(\n",
    "                range(len(all_layers)),\n",
    "                values,\n",
    "                \"o-\",\n",
    "                label=f\"{input_location} - Trainer {trainer_id}\",\n",
    "            )\n",
    "\n",
    "    plt.xlabel(\"Layer\")\n",
    "    plt.ylabel(\"Ablation Metric\")\n",
    "    plt.title(f\"Ablation Results per Layer ({ablation_method} ablation)\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.xticks(range(len(all_layers)), layer_labels, rotation=45, ha=\"right\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "directory = \"decision_trees\"\n",
    "custom_function_names = [\n",
    "    # othello_utils.games_batch_to_input_tokens_flipped_bs_classifier_input_BLC.__name__,\n",
    "    othello_utils.games_batch_to_board_state_classifier_input_BLC.__name__,\n",
    "    othello_utils.games_batch_to_input_tokens_classifier_input_BLC.__name__,\n",
    "    othello_utils.games_batch_to_input_tokens_flipped_classifier_input_BLC.__name__,\n",
    "]\n",
    "\n",
    "default_config = sim_config.selected_config\n",
    "dataset_size = default_config.batch_size * default_config.n_batches\n",
    "dataset_size = 100\n",
    "dataset_size = 80\n",
    "\n",
    "ablation_method = \"dt\"\n",
    "ablate_not_selected = True\n",
    "add_error = False\n",
    "\n",
    "desired_layer_tuples = [\n",
    "    (0, 1, 2),\n",
    "    (1, 2, 3),\n",
    "    (2, 3, 4),\n",
    "    (3, 4, 5),\n",
    "    (4, 5, 6),\n",
    "    (5, 6, 7),\n",
    "    # (0, 1, 2, 3, 4, 5, 6, 7),\n",
    "]\n",
    "\n",
    "desired_layer_tuples = []\n",
    "\n",
    "for i in range(8):\n",
    "    desired_layer_tuples.append((i,))\n",
    "\n",
    "desired_ablation_metric = \"patch_accuracy\"\n",
    "group_by = \"input_location\"\n",
    "# group_by = \"custom_function\"\n",
    "\n",
    "func_name_filter = custom_function_names[0]\n",
    "# func_name_filter = None\n",
    "# input_location_filter = \"mlp_neuron\"\n",
    "input_location_filter = None\n",
    "\n",
    "data = load_ablation_pickle_files(\n",
    "    directory, dataset_size, ablation_method, ablate_not_selected, add_error\n",
    ")\n",
    "\n",
    "mean_ablate_data = load_ablation_pickle_files(directory, dataset_size, \"mean\", True, True)\n",
    "\n",
    "example_dict = data[0]\n",
    "\n",
    "print(example_dict[\"hyperparameters\"].keys())\n",
    "print(example_dict)\n",
    "\n",
    "# Current dict structure: input_location / custom function -> trainer_id -> layer_tuple -> kl_divergence or other metric\n",
    "\n",
    "metric_per_layers = extract_ablation_results(\n",
    "    data,\n",
    "    custom_function_names,\n",
    "    desired_ablation_metric,\n",
    "    group_by,\n",
    "    input_location_filter=input_location_filter,\n",
    "    func_name_filter=func_name_filter,\n",
    "    desired_layer_tuples=desired_layer_tuples,\n",
    ")\n",
    "mean_ablate_per_layers = extract_mean_ablation_results(\n",
    "    mean_ablate_data, desired_ablation_metric, desired_layer_tuples\n",
    ")\n",
    "\n",
    "for key in mean_ablate_per_layers.keys():\n",
    "    metric_per_layers[key] = mean_ablate_per_layers[key]\n",
    "\n",
    "print(metric_per_layers.keys())\n",
    "print(metric_per_layers)\n",
    "plot_grouped_metrics(metric_per_layers, ablation_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_3var_results(metric_per_layer: dict, eval_df: pd.DataFrame, layer: int, input_location: str):\n",
    "    # Filter the evaluation dataframe\n",
    "    eval_df_filtered = eval_df[eval_df['layer_idx'] == layer]\n",
    "    \n",
    "    # Prepare data for plotting\n",
    "    l0 = []\n",
    "    frac_recovered = []\n",
    "    kl_values = []\n",
    "    tuple_layer = (layer,)\n",
    "    \n",
    "    for trainer_id, layer_results in metric_per_layer[input_location].items():\n",
    "\n",
    "        if tuple_layer in layer_results:\n",
    "            # Find matching row in eval_df_filtered\n",
    "            eval_row = eval_df_filtered[eval_df_filtered['trainer_idx'] == int(trainer_id)]\n",
    "            if not eval_row.empty:\n",
    "                l0.append(eval_row['l0'].values[0])\n",
    "                frac_recovered.append(eval_row['frac_recovered'].values[0])\n",
    "                kl_values.append(layer_results[tuple_layer].item())\n",
    "    \n",
    "    # Create the 2D plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Plot the points\n",
    "    scatter = plt.scatter(l0, frac_recovered, c=kl_values, cmap='viridis', s=50)\n",
    "    \n",
    "    # Set labels and title\n",
    "    plt.xlabel('L0', fontsize=12)\n",
    "    plt.ylabel('Fraction Recovered', fontsize=12)\n",
    "    plt.title(f'L0 vs Fraction Recovered vs Neuron Simulation KL Divergence\\n'\n",
    "              f'Layer {layer}, {input_location}', fontsize=14)\n",
    "    \n",
    "    # Add a color bar\n",
    "    cbar = plt.colorbar(scatter)\n",
    "    cbar.set_label(f'Layer {layer} KL Divergence', fontsize=12)\n",
    "    \n",
    "    # Add grid lines\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Improve layout\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(f\"l0_vs_frac_recovered_vs_kl_divergence_{input_location}_layer_{layer}.png\")\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "# Load evaluation data\n",
    "sae_mlp_out_eval = pd.read_csv(\"sae_eval_csvs/sae_mlp_out_feature_evaluations.csv\")\n",
    "transcoder_eval = pd.read_csv(\"sae_eval_csvs/transcoder_evaluations.csv\")\n",
    "\n",
    "\n",
    "for layer in range(4, 8):\n",
    "\n",
    "    # Plot for sae_mlp_out_feature\n",
    "    graph_3var_results(metric_per_layers, sae_mlp_out_eval, layer, 'sae_mlp_out_feature')\n",
    "\n",
    "    # Plot for transcoder\n",
    "    graph_3var_results(metric_per_layers, transcoder_eval, layer, 'transcoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Currently broken, just need to get the dictionary keys right\n",
    "# This can be used to plot multiple thresholds for a single location / custom function combination\n",
    "\n",
    "# colors = ['b', 'g', 'r']  # Colors for different thresholds\n",
    "# markers = ['o', 's', '^']  # Markers for different thresholds\n",
    "\n",
    "# for custom_function in custom_functions:\n",
    "#     function_name = custom_function.__name__\n",
    "    \n",
    "#     plt.figure(figsize=(12, 7))\n",
    "    \n",
    "#     for i, threshold in enumerate(thresholds):\n",
    "#         f1_counts = accuracy_by_layer[function_name][threshold]\n",
    "        \n",
    "#         plt.plot(intervention_layers, f1_counts, color=colors[i], marker=markers[i], \n",
    "#                  label=f'Threshold: {threshold}', linewidth=2, markersize=8)\n",
    "        \n",
    "#         # Optionally, add value labels on each point\n",
    "#         for j, count in enumerate(f1_counts):\n",
    "#             plt.annotate(str(count), (intervention_layers[j], count), \n",
    "#                          textcoords=\"offset points\", xytext=(0,5), ha='center', \n",
    "#                          fontsize=8, color=colors[i])\n",
    "    \n",
    "#     plt.title(f'Number of Neurons with F1 > Threshold for \\n{function_labels[custom_function]} by Layer \\n{dataset_size} datapoints Input location: {input_location} depth: {max_depth}', fontsize=14)\n",
    "#     plt.xlabel('Layer Number', fontsize=12)\n",
    "#     plt.ylabel('F1 Count', fontsize=12)\n",
    "#     plt.legend(loc='best', fontsize=10)\n",
    "#     plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "#     output_filename = f\"images/{input_location}_{function_name}_inputs_{dataset_size}_depth_{max_depth}_f1_count_by_layer_all_thresholds.png\"\n",
    "#     plt.savefig(output_filename, dpi=300, bbox_inches='tight')\n",
    "#     plt.show()\n",
    "\n",
    "# print(\"All graphs have been created and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Currently broken, just need to get the dictionary keys right\n",
    "\n",
    "# layer = 1\n",
    "# neuron_idx = 421\n",
    "# custom_function_name = custom_functions[0].__name__\n",
    "\n",
    "# decision_tree_filename = f\"decision_trees/decision_trees_{input_location}_{dataset_size}.pkl\"\n",
    "\n",
    "# with open(decision_tree_filename, \"rb\") as f:\n",
    "#     decision_tree = pickle.load(f)\n",
    "\n",
    "# layer_dt = decision_tree[layer][custom_function_name]['decision_tree']['model']\n",
    "# layer_dt = decision_tree[layer][custom_function_name]['binary_decision_tree']['model']\n",
    "\n",
    "# games_BLC = train_data[custom_function_name]\n",
    "\n",
    "# feature_names = []\n",
    "\n",
    "# X_binary_train, X_binary_test, y_binary_train, y_binary_test = prepare_data(\n",
    "#     games_BLC, binary_acts[layer]\n",
    "# )\n",
    "# accuracy, precision, recall, f1 = calculate_binary_metrics(\n",
    "#     decision_tree[layer][custom_function_name][\"binary_decision_tree\"][\"model\"], X_binary_test, y_binary_test\n",
    "# )\n",
    "\n",
    "# print(f\"Neuron 421 F1: {f1[neuron_idx]}\")\n",
    "\n",
    "# for i in range(games_BLC.shape[2]):\n",
    "#     if i < 64:\n",
    "#         square = idx_to_square_notation(i)\n",
    "#         feature_names.append(f\"Input_{square}\")\n",
    "#     elif i < 128:\n",
    "#         j = i - 64\n",
    "#         square = idx_to_square_notation(j)\n",
    "#         feature_names.append(f\"Occupied_{square}\")\n",
    "#     else:\n",
    "#         feature_names.append(f\"Output_{i}\")\n",
    "\n",
    "# print_decision_tree_rules(layer_dt, feature_names=feature_names, neuron_index=neuron_idx, max_depth=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "othello",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
