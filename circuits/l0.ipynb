{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnsight import LanguageModel\n",
    "import torch\n",
    "\n",
    "from dictionary_learning import ActivationBuffer\n",
    "from dictionary_learning.training import trainSAE\n",
    "from circuits.nanogpt_to_hf_transformers import NanogptTokenizer, convert_nanogpt_model\n",
    "from dictionary_learning.utils import hf_dataset_to_generator\n",
    "from dictionary_learning.trainers.standard import StandardTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\")\n",
    "\n",
    "tokenizer = NanogptTokenizer()\n",
    "model = convert_nanogpt_model(\"lichess_8layers_ckpt_no_optimizer.pt\", torch.device(DEVICE))\n",
    "model = LanguageModel(model, device_map=DEVICE, tokenizer=tokenizer)\n",
    "\n",
    "submodule = model.transformer.h[5].mlp  # layer 1 MLP\n",
    "activation_dim = 512  # output dimension of the MLP\n",
    "dictionary_size = 8 * activation_dim\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "data = hf_dataset_to_generator(\"adamkarvonen/chess_sae_test\", streaming=False)\n",
    "buffer = ActivationBuffer(\n",
    "    data,\n",
    "    model,\n",
    "    submodule,\n",
    "    n_ctxs=512,\n",
    "    ctx_len=256,\n",
    "    refresh_batch_size=4,\n",
    "    io=\"out\",\n",
    "    d_submodule=512,\n",
    "    device=DEVICE,\n",
    "    out_batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dictionary_learning import AutoEncoder\n",
    "\n",
    "ae = AutoEncoder.from_pretrained(\"t1_ae.pt\", device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_feature(\n",
    "    activations,\n",
    "    ae: AutoEncoder,\n",
    "    device,\n",
    "):\n",
    "    try:\n",
    "        x = next(activations).to(device)\n",
    "    except StopIteration:\n",
    "        raise StopIteration(\n",
    "            \"Not enough activations in buffer. Pass a buffer with a smaller batch size or more data.\"\n",
    "        )\n",
    "    # print(x.shape)\n",
    "\n",
    "    print(x.shape)\n",
    "\n",
    "    x_hat, f = ae(x, output_features=True)\n",
    "\n",
    "    # print(x_hat.shape, f.shape)\n",
    "\n",
    "    return f\n",
    "\n",
    "    return f.mean(0)\n",
    "    # batch_size, seq_len = tokens.shape\n",
    "\n",
    "    # logits, cache = model.run_with_cache(tokens, names_filter = [\"blocks.0.mlp.hook_post\"])\n",
    "    # post = cache[\"blocks.0.mlp.hook_post\"]\n",
    "    # assert post.shape == (batch_size, seq_len, model.cfg.d_mlp)\n",
    "\n",
    "    # post_reshaped = einops.repeat(post, \"batch seq d_mlp -> (batch seq) instances d_mlp\", instances=2)\n",
    "    # assert post_reshaped.shape == (batch_size * seq_len, 2, model.cfg.d_mlp)\n",
    "\n",
    "    # acts = autoencoder.forward(post_reshaped)[3]\n",
    "    # assert acts.shape == (batch_size * seq_len, 2, autoencoder.cfg.n_hidden_ae)\n",
    "\n",
    "    # return acts.mean(0)\n",
    "num_iters = 1024\n",
    "seq_len = 4096\n",
    "\n",
    "features = torch.zeros((batch_size*num_iters, seq_len), device=DEVICE)\n",
    "probs = []\n",
    "for i in range(num_iters):\n",
    "    feature = get_feature(buffer, ae, DEVICE)\n",
    "    prob = feature.mean(0)\n",
    "    features[i*batch_size:(i+1)*batch_size, :] = feature\n",
    "    probs.append(prob)\n",
    "    # print(i)\n",
    "\n",
    "# l0 = (f != 0).float().sum(dim=-1).mean()\n",
    "feat_prob = sum(probs) / len(probs)\n",
    "print(feat_prob.shape)\n",
    "log_freq = (feat_prob + 1e-10).log10()\n",
    "print(log_freq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features.shape)\n",
    "l0 = (features != 0).float().sum(dim=-1)#.mean()\n",
    "print(l0.mean())\n",
    "l0 /= num_iters * batch_size\n",
    "print(l0.shape)\n",
    "print(l0)\n",
    "\n",
    "print(l0.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (l0 > 0) & (l0 < 0.5)\n",
    "idx = torch.nonzero(mask, as_tuple=False).squeeze()\n",
    "print(idx.shape)\n",
    "print(idx[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l0_log = l0\n",
    "import matplotlib.pyplot as plt\n",
    "lo_log_np = l0_log.cpu().numpy()\n",
    "\n",
    "# Creating the histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(lo_log_np, bins=50, alpha=0.75, color='blue')\n",
    "plt.title('Histogram of log10 of Feature Probabilities')\n",
    "plt.xlabel('log10(Probability)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "log_freq_np = log_freq.cpu().numpy()\n",
    "# log_freq_np = feat_prob.cpu()\n",
    "\n",
    "# Creating the histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(log_freq_np, bins=50, alpha=0.75, color='blue')\n",
    "plt.title('Histogram of log10 of Feature Probabilities')\n",
    "plt.xlabel('log10(Probability)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(idx[:20])\n",
    "interest = 19\n",
    "print(l0[interest])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from circuitsvis.activations import text_neuron_activations\n",
    "from einops import rearrange\n",
    "import torch as t\n",
    "from collections import namedtuple\n",
    "import umap\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dictionary_learning.interp import examine_dimension\n",
    "\n",
    "top_contexts, top_tokens = examine_dimension(model, submodule, buffer, dictionary=ae, dim_idx=interest, n_inputs=500, k=30, batch_size=4, device=DEVICE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "circuits",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
