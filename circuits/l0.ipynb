{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnsight import LanguageModel\n",
    "import torch\n",
    "\n",
    "from dictionary_learning import ActivationBuffer\n",
    "from dictionary_learning.training import trainSAE\n",
    "from circuits.nanogpt_to_hf_transformers import NanogptTokenizer, convert_nanogpt_model\n",
    "from dictionary_learning.utils import hf_dataset_to_generator\n",
    "from dictionary_learning.trainers.standard import StandardTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\")\n",
    "\n",
    "tokenizer = NanogptTokenizer()\n",
    "model = convert_nanogpt_model(\"lichess_8layers_ckpt_no_optimizer.pt\", torch.device(DEVICE))\n",
    "model = LanguageModel(model, device_map=DEVICE, tokenizer=tokenizer)\n",
    "\n",
    "submodule = model.transformer.h[5].mlp  # layer 1 MLP\n",
    "activation_dim = 512  # output dimension of the MLP\n",
    "dictionary_size = 8 * activation_dim\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "data = hf_dataset_to_generator(\"adamkarvonen/chess_sae_test\", streaming=False)\n",
    "buffer = ActivationBuffer(\n",
    "    data,\n",
    "    model,\n",
    "    submodule,\n",
    "    n_ctxs=512,\n",
    "    ctx_len=256,\n",
    "    refresh_batch_size=4,\n",
    "    io=\"out\",\n",
    "    d_submodule=512,\n",
    "    device=DEVICE,\n",
    "    out_batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dictionary_learning import AutoEncoder\n",
    "\n",
    "ae = AutoEncoder.from_pretrained(\"t1_ae.pt\", device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_feature(\n",
    "    activations,\n",
    "    ae: AutoEncoder,\n",
    "    device,\n",
    "):\n",
    "    try:\n",
    "        x = next(activations).to(device)\n",
    "    except StopIteration:\n",
    "        raise StopIteration(\n",
    "            \"Not enough activations in buffer. Pass a buffer with a smaller batch size or more data.\"\n",
    "        )\n",
    "    # print(x.shape)\n",
    "\n",
    "    print(x.shape)\n",
    "\n",
    "    x_hat, f = ae(x, output_features=True)\n",
    "\n",
    "    # print(x_hat.shape, f.shape)\n",
    "\n",
    "    return f\n",
    "\n",
    "    return f.mean(0)\n",
    "    # batch_size, seq_len = tokens.shape\n",
    "\n",
    "    # logits, cache = model.run_with_cache(tokens, names_filter = [\"blocks.0.mlp.hook_post\"])\n",
    "    # post = cache[\"blocks.0.mlp.hook_post\"]\n",
    "    # assert post.shape == (batch_size, seq_len, model.cfg.d_mlp)\n",
    "\n",
    "    # post_reshaped = einops.repeat(post, \"batch seq d_mlp -> (batch seq) instances d_mlp\", instances=2)\n",
    "    # assert post_reshaped.shape == (batch_size * seq_len, 2, model.cfg.d_mlp)\n",
    "\n",
    "    # acts = autoencoder.forward(post_reshaped)[3]\n",
    "    # assert acts.shape == (batch_size * seq_len, 2, autoencoder.cfg.n_hidden_ae)\n",
    "\n",
    "    # return acts.mean(0)\n",
    "num_iters = 1024\n",
    "seq_len = 4096\n",
    "\n",
    "features = torch.zeros((batch_size*num_iters, seq_len), device=DEVICE)\n",
    "probs = []\n",
    "for i in range(num_iters):\n",
    "    feature = get_feature(buffer, ae, DEVICE)\n",
    "    prob = feature.mean(0)\n",
    "    features[i*batch_size:(i+1)*batch_size, :] = feature\n",
    "    probs.append(prob)\n",
    "    # print(i)\n",
    "\n",
    "# l0 = (f != 0).float().sum(dim=-1).mean()\n",
    "feat_prob = sum(probs) / len(probs)\n",
    "print(feat_prob.shape)\n",
    "log_freq = (feat_prob + 1e-10).log10()\n",
    "print(log_freq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features.shape)\n",
    "l0 = (features != 0).float().sum(dim=0)#.mean()\n",
    "print(l0.mean())\n",
    "l0 /= num_iters * batch_size\n",
    "print(l0.shape)\n",
    "print(l0)\n",
    "\n",
    "print(l0.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (l0 > 0) & (l0 < 0.5)\n",
    "idx = torch.nonzero(mask, as_tuple=False).squeeze()\n",
    "print(idx.shape)\n",
    "print(idx[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l0_log = l0\n",
    "import matplotlib.pyplot as plt\n",
    "lo_log_np = l0_log.cpu().numpy()\n",
    "\n",
    "# Creating the histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(lo_log_np, bins=50, alpha=0.75, color='blue')\n",
    "plt.title('Histogram of firing rates for features')\n",
    "plt.xlabel('(Probability)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from circuitsvis.activations import text_neuron_activations\n",
    "from einops import rearrange\n",
    "import torch as t\n",
    "from collections import namedtuple\n",
    "import umap\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "log_freq_np = log_freq.cpu().numpy()\n",
    "# log_freq_np = feat_prob.cpu()\n",
    "\n",
    "# Creating the histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(log_freq_np, bins=50, alpha=0.75, color='blue')\n",
    "plt.title('Histogram of log10 of Feature Probabilities')\n",
    "plt.xlabel('log10(Probability)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(idx[:20])\n",
    "interest = 6\n",
    "print(l0[interest])\n",
    "print(idx.shape)\n",
    "print(l0[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from circuitsvis.activations import text_neuron_activations\n",
    "from einops import rearrange\n",
    "import torch as t\n",
    "from collections import namedtuple\n",
    "import umap\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "\n",
    "def feature_effect(\n",
    "    model,\n",
    "    submodule,\n",
    "    dictionary,\n",
    "    feature,\n",
    "    inputs,\n",
    "    add_residual=True,  # whether to compensate for dictionary reconstruction error by adding residual\n",
    "    k=10,\n",
    "    largest=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Effect of ablating the feature on top k predictions for next token.\n",
    "    \"\"\"\n",
    "    # clean run\n",
    "    with model.trace(inputs):\n",
    "        if dictionary is None:\n",
    "            pass\n",
    "        elif not add_residual:  # run hidden state through autoencoder\n",
    "            if type(submodule.output.shape) == tuple:\n",
    "                submodule.output[0][:] = dictionary(submodule.output[0])\n",
    "            else:\n",
    "                submodule.output = dictionary(submodule.output)\n",
    "        clean_output = model.output.save()\n",
    "    try:\n",
    "        clean_logits = clean_output.value.logits[:, -1, :]\n",
    "    except:\n",
    "        clean_logits = clean_output.value[:, -1, :]\n",
    "    clean_logprobs = t.nn.functional.log_softmax(clean_logits, dim=-1)\n",
    "\n",
    "    # ablated run\n",
    "    with model.trace(inputs):\n",
    "        if dictionary is None:\n",
    "            if type(submodule.output.shape) == tuple:\n",
    "                submodule.output[0][:, -1, feature] = 0\n",
    "            else:\n",
    "                submodule.output[:, -1, feature] = 0\n",
    "        else:\n",
    "            x = submodule.output\n",
    "            if type(x.shape) == tuple:\n",
    "                x = x[0]\n",
    "            x_hat, f = dictionary(x, output_features=True)\n",
    "            residual = x - x_hat\n",
    "\n",
    "            f[:, -1, feature] = 0\n",
    "            if add_residual:\n",
    "                x_hat = dictionary.decode(f) + residual\n",
    "            else:\n",
    "                x_hat = dictionary.decode(f)\n",
    "\n",
    "            if type(submodule.output.shape) == tuple:\n",
    "                submodule.output[0][:] = x_hat\n",
    "            else:\n",
    "                submodule.output = x_hat\n",
    "        ablated_output = model.output.save()\n",
    "    try:\n",
    "        ablated_logits = ablated_output.value.logits[:, -1, :]\n",
    "    except:\n",
    "        ablated_logits = ablated_output.value[:, -1, :]\n",
    "    ablated_logprobs = t.nn.functional.log_softmax(ablated_logits, dim=-1)\n",
    "\n",
    "    diff = clean_logprobs - ablated_logprobs\n",
    "    top_probs, top_tokens = t.topk(diff.mean(dim=0), k=k, largest=largest)\n",
    "    return top_tokens, top_probs\n",
    "\n",
    "\n",
    "@t.no_grad()\n",
    "def examine_dimension_old(\n",
    "    model,\n",
    "    submodule,\n",
    "    buffer,\n",
    "    dictionary=None,\n",
    "    max_length=128,\n",
    "    n_inputs=512,\n",
    "    dim_idx=None,\n",
    "    k=30,\n",
    "    batch_size=4,\n",
    "    device=t.device(\"cuda\"),\n",
    "):\n",
    "\n",
    "    def _list_decode(x):\n",
    "        if isinstance(x, int):\n",
    "            return model.tokenizer.decode(x)\n",
    "        else:\n",
    "            return [_list_decode(y) for y in x]\n",
    "\n",
    "    if dim_idx is None:\n",
    "        dim_idx = random.randint(0, activations.shape[-1] - 1)\n",
    "\n",
    "    n_iters = n_inputs // batch_size\n",
    "\n",
    "    activations = t.zeros((n_iters * batch_size, max_length), device=device, dtype=t.float32)\n",
    "    tokens = t.zeros((n_iters * batch_size, max_length), device=device, dtype=t.int64)\n",
    "\n",
    "    for i in range(n_iters):\n",
    "        inputs = buffer.text_batch(batch_size=batch_size)\n",
    "        with model.trace(inputs, invoker_args=dict(max_length=max_length, truncation=True)):\n",
    "            cur_tokens = model.input[1][\n",
    "                \"input_ids\"\n",
    "            ].save()  # if you're getting errors, check here; might only work for pythia models\n",
    "            cur_activations = submodule.output\n",
    "            if type(cur_activations.shape) == tuple:\n",
    "                cur_activations = cur_activations[0]\n",
    "            if dictionary is not None:\n",
    "                cur_activations = dictionary.encode(cur_activations)\n",
    "            cur_activations = cur_activations[:, :, dim_idx].save()\n",
    "        activations[i * batch_size : (i + 1) * batch_size, :] = cur_activations.value\n",
    "        tokens[i * batch_size : (i + 1) * batch_size, :] = cur_tokens.value\n",
    "\n",
    "    top_affected = feature_effect(model, submodule, dictionary, dim_idx, tokens, k=k)\n",
    "    top_affected = [(model.tokenizer.decode(tok), prob.item()) for tok, prob in zip(*top_affected)]\n",
    "\n",
    "    # get top k tokens by mean activation\n",
    "    token_mean_acts = {}\n",
    "    # tokens = tokens.value\n",
    "    for ctx in tokens:\n",
    "        for tok in ctx:\n",
    "            if tok.item() in token_mean_acts:\n",
    "                continue\n",
    "            idxs = (tokens == tok).nonzero(as_tuple=True)\n",
    "            token_mean_acts[tok.item()] = activations[idxs].mean().item()\n",
    "    top_tokens = sorted(token_mean_acts.items(), key=lambda x: x[1], reverse=True)[:k]\n",
    "    top_tokens = [(model.tokenizer.decode(tok), act) for tok, act in top_tokens]\n",
    "\n",
    "    flattened_acts = rearrange(activations, \"b n -> (b n)\")\n",
    "    topk_indices = t.argsort(flattened_acts, dim=0, descending=True)[:k]\n",
    "    batch_indices = topk_indices // activations.shape[1]\n",
    "    token_indices = topk_indices % activations.shape[1]\n",
    "    tokens = [\n",
    "        tokens[batch_idx, : token_idx + 1].tolist()\n",
    "        for batch_idx, token_idx in zip(batch_indices, token_indices)\n",
    "    ]\n",
    "    activations = [\n",
    "        activations[batch_idx, : token_id + 1, None, None]\n",
    "        for batch_idx, token_id in zip(batch_indices, token_indices)\n",
    "    ]\n",
    "    decoded_tokens = _list_decode(tokens)\n",
    "    top_contexts = text_neuron_activations(decoded_tokens, activations)\n",
    "\n",
    "    return namedtuple(\n",
    "        \"featureProfile\",\n",
    "        [\"top_contexts\", \"top_tokens\", \"top_affected\", \"decoded_tokens\", \"activations\"],\n",
    "    )(top_contexts, top_tokens, top_affected, decoded_tokens, activations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from circuitsvis.activations import text_neuron_activations\n",
    "from einops import rearrange\n",
    "import torch as t\n",
    "from collections import namedtuple\n",
    "import umap\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "\n",
    "def feature_effect(\n",
    "    model,\n",
    "    submodule,\n",
    "    dictionary,\n",
    "    feature,\n",
    "    inputs,\n",
    "    add_residual=True,  # whether to compensate for dictionary reconstruction error by adding residual\n",
    "    k=10,\n",
    "    largest=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Effect of ablating the feature on top k predictions for next token.\n",
    "    \"\"\"\n",
    "    # clean run\n",
    "    with model.trace(inputs):\n",
    "        if dictionary is None:\n",
    "            pass\n",
    "        elif not add_residual:  # run hidden state through autoencoder\n",
    "            if type(submodule.output.shape) == tuple:\n",
    "                submodule.output[0][:] = dictionary(submodule.output[0])\n",
    "            else:\n",
    "                submodule.output = dictionary(submodule.output)\n",
    "        clean_output = model.output.save()\n",
    "    try:\n",
    "        clean_logits = clean_output.value.logits[:, -1, :]\n",
    "    except:\n",
    "        clean_logits = clean_output.value[:, -1, :]\n",
    "    clean_logprobs = t.nn.functional.log_softmax(clean_logits, dim=-1)\n",
    "\n",
    "    # ablated run\n",
    "    with model.trace(inputs):\n",
    "        if dictionary is None:\n",
    "            if type(submodule.output.shape) == tuple:\n",
    "                submodule.output[0][:, -1, feature] = 0\n",
    "            else:\n",
    "                submodule.output[:, -1, feature] = 0\n",
    "        else:\n",
    "            x = submodule.output\n",
    "            if type(x.shape) == tuple:\n",
    "                x = x[0]\n",
    "            x_hat, f = dictionary(x, output_features=True)\n",
    "            residual = x - x_hat\n",
    "\n",
    "            f[:, -1, feature] = 0\n",
    "            if add_residual:\n",
    "                x_hat = dictionary.decode(f) + residual\n",
    "            else:\n",
    "                x_hat = dictionary.decode(f)\n",
    "\n",
    "            if type(submodule.output.shape) == tuple:\n",
    "                submodule.output[0][:] = x_hat\n",
    "            else:\n",
    "                submodule.output = x_hat\n",
    "        ablated_output = model.output.save()\n",
    "    try:\n",
    "        ablated_logits = ablated_output.value.logits[:, -1, :]\n",
    "    except:\n",
    "        ablated_logits = ablated_output.value[:, -1, :]\n",
    "    ablated_logprobs = t.nn.functional.log_softmax(ablated_logits, dim=-1)\n",
    "\n",
    "    diff = clean_logprobs - ablated_logprobs\n",
    "    top_probs, top_tokens = t.topk(diff.mean(dim=0), k=k, largest=largest)\n",
    "    return top_tokens, top_probs\n",
    "\n",
    "\n",
    "@t.no_grad()\n",
    "def examine_dimension(\n",
    "    model,\n",
    "    submodule,\n",
    "    buffer,\n",
    "    dictionary=None,\n",
    "    max_length=128,\n",
    "    n_inputs=512,\n",
    "    dims=torch.tensor([0]),\n",
    "    k=30,\n",
    "    batch_size=4,\n",
    "    device=t.device(\"cuda\"),\n",
    "):\n",
    "\n",
    "    def _list_decode(x):\n",
    "        if isinstance(x, int):\n",
    "            return model.tokenizer.decode(x)\n",
    "        else:\n",
    "            return [_list_decode(y) for y in x]\n",
    "\n",
    "\n",
    "    n_iters = n_inputs // batch_size\n",
    "\n",
    "    dim_count = dims.shape[0]\n",
    "\n",
    "    activations = t.zeros((dim_count, n_iters * batch_size, max_length), device=device, dtype=t.float32)\n",
    "    tokens = t.zeros((n_iters * batch_size, max_length), device=device, dtype=t.int64)\n",
    "\n",
    "    for i in range(n_iters):\n",
    "        inputs = buffer.text_batch(batch_size=batch_size)\n",
    "        with model.trace(inputs, invoker_args=dict(max_length=max_length, truncation=True)):\n",
    "            cur_tokens = model.input[1][\n",
    "                \"input_ids\"\n",
    "            ].save()  # if you're getting errors, check here; might only work for pythia models\n",
    "            cur_activations = submodule.output\n",
    "            if type(cur_activations.shape) == tuple:\n",
    "                cur_activations = cur_activations[0]\n",
    "            if dictionary is not None:\n",
    "                cur_activations = dictionary.encode(cur_activations)\n",
    "            cur_activations = cur_activations[:, :, dims].save() # Shape: (batch_size, max_length, dim_count)\n",
    "        cur_activations = rearrange(cur_activations.value, \"b n d -> d b n\") # Shape: (dim_count, batch_size, max_length)\n",
    "        activations[:, i * batch_size : (i + 1) * batch_size, :] = cur_activations\n",
    "        tokens[i * batch_size : (i + 1) * batch_size, :] = cur_tokens.value\n",
    "\n",
    "\n",
    "    per_dim_stats = {}\n",
    "    for i, dim in enumerate(dims):\n",
    "        individual_acts = activations[i]\n",
    "        \n",
    "        # top_affected = feature_effect(model, submodule, dictionary, dim_idx, tokens, k=k)\n",
    "        # top_affected = [(model.tokenizer.decode(tok), prob.item()) for tok, prob in zip(*top_affected)]\n",
    "        top_affected = None\n",
    "\n",
    "        # get top k tokens by mean activation\n",
    "        token_mean_acts = {}\n",
    "        # tokens = tokens.value\n",
    "        for ctx in tokens:\n",
    "            for tok in ctx:\n",
    "                if tok.item() in token_mean_acts:\n",
    "                    continue\n",
    "                idxs = (tokens == tok).nonzero(as_tuple=True)\n",
    "                token_mean_acts[tok.item()] = individual_acts[idxs].mean().item()\n",
    "        top_tokens = sorted(token_mean_acts.items(), key=lambda x: x[1], reverse=True)[:k]\n",
    "        top_tokens = [(model.tokenizer.decode(tok), act) for tok, act in top_tokens]\n",
    "\n",
    "        flattened_acts = rearrange(individual_acts, \"b n -> (b n)\")\n",
    "        topk_indices = t.argsort(flattened_acts, dim=0, descending=True)[:k]\n",
    "        batch_indices = topk_indices // individual_acts.shape[1]\n",
    "        token_indices = topk_indices % individual_acts.shape[1]\n",
    "        individual_tokens = [\n",
    "            tokens[batch_idx, : token_idx + 1].tolist()\n",
    "            for batch_idx, token_idx in zip(batch_indices, token_indices)\n",
    "        ]\n",
    "        individual_acts = [\n",
    "            individual_acts[batch_idx, : token_id + 1, None, None]\n",
    "            for batch_idx, token_id in zip(batch_indices, token_indices)\n",
    "        ]\n",
    "        decoded_tokens = _list_decode(individual_tokens)\n",
    "\n",
    "        if dim_count == 1:\n",
    "            top_contexts = text_neuron_activations(decoded_tokens, activations)\n",
    "        else:\n",
    "            top_contexts = None\n",
    "        per_dim_stats[dim.item()] = namedtuple(\n",
    "            \"featureProfile\",\n",
    "            [\"top_contexts\", \"top_tokens\", \"top_affected\", \"decoded_tokens\", \"activations\"],\n",
    "        )(top_contexts, top_tokens, top_affected, decoded_tokens, individual_acts)\n",
    "\n",
    "    return per_dim_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dictionary_learning.interp import examine_dimension\n",
    "top_contexts, top_tokens, top_affected, decoded_tokens, activations = examine_dimension_old(model, submodule, buffer, dictionary=ae, dim_idx=interest, n_inputs=20, k=30, batch_size=4, device=DEVICE)\n",
    "print(top_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(idx)\n",
    "print(torch.tensor([interest], device=DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_dim_stats = examine_dimension(model, submodule, buffer, dictionary=ae, dims=idx[:500], n_inputs=200, k=30, batch_size=4, device=DEVICE)\n",
    "print(per_dim_stats[interest].top_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(per_dim_stats.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess_utils\n",
    "# for string in decoded_tokens:\n",
    "#     print(string)\n",
    "# for act, string in zip(activations, decoded_tokens):\n",
    "    # print(act.shape, len(string))\n",
    "    # print(act[-1:], string[-1:])\n",
    "    # print(\"\".join(string))\n",
    "# top_contexts\n",
    "\n",
    "nonzero_count = 0\n",
    "num_idx_count = 0\n",
    "stop = 0\n",
    "for dim in per_dim_stats:\n",
    "    stop += 1\n",
    "    if stop > 100:\n",
    "        break\n",
    "    decoded_tokens = per_dim_stats[dim].decoded_tokens\n",
    "    activations = per_dim_stats[dim].activations\n",
    "    if activations[10][-1].item() == 0:\n",
    "        continue\n",
    "    nonzero_count += 1\n",
    "    inputs = [\"\".join(string) for string in decoded_tokens]\n",
    "    # print(inputs)\n",
    "    num_indices = []\n",
    "    count = 0\n",
    "    top_k = 10\n",
    "    for i, pgn in enumerate(inputs[:top_k]):\n",
    "        # print(f\"dim: {dim} pgn: {pgn}, activation: {activations[i][-1].item()}\")\n",
    "        nums = chess_utils.find_num_indices(pgn)\n",
    "        num_indices.append(nums)\n",
    "        if (len(pgn) - 1) in nums:\n",
    "            count += 1\n",
    "    if count == top_k:\n",
    "        # print(f\"dim: {dim} all have num indices\")\n",
    "        num_idx_count += 1\n",
    "print(num_idx_count, nonzero_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess_utils\n",
    "import chess\n",
    "# for string in decoded_tokens:\n",
    "#     print(string)\n",
    "# for act, string in zip(activations, decoded_tokens):\n",
    "    # print(act.shape, len(string))\n",
    "    # print(act[-1:], string[-1:])\n",
    "    # print(\"\".join(string))\n",
    "# top_contexts\n",
    "\n",
    "nonzero_count = 0\n",
    "num_idx_count = 0\n",
    "stop = 0\n",
    "for dim in per_dim_stats:\n",
    "    stop += 1\n",
    "    if stop > 500:\n",
    "        break\n",
    "    decoded_tokens = per_dim_stats[dim].decoded_tokens\n",
    "    activations = per_dim_stats[dim].activations\n",
    "    if activations[10][-1].item() == 0:\n",
    "        continue\n",
    "    nonzero_count += 1\n",
    "    inputs = [\"\".join(string) for string in decoded_tokens]\n",
    "\n",
    "    chess_boards = [chess_utils.pgn_string_to_board(pgn, allow_exception=True) for pgn in inputs]\n",
    "\n",
    "    num_indices = []\n",
    "    count = 0\n",
    "    top_k = 10\n",
    "\n",
    "    if dim == 1082:\n",
    "        print(inputs)\n",
    "    \n",
    "    for i, board in enumerate(chess_boards[:top_k]):\n",
    "        # board = chess.Board()\n",
    "        if board.is_check():\n",
    "            print(\"Check\")\n",
    "            count += 1\n",
    "    # print(count)\n",
    "    if count > top_k - 2:\n",
    "        print(f\"dim: {dim} all have num indices\")\n",
    "        print(count)\n",
    "        num_idx_count += 1\n",
    "print(num_idx_count, nonzero_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "circuits",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
