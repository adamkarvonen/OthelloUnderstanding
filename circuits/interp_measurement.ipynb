{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnsight import LanguageModel\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import chess\n",
    "import json\n",
    "\n",
    "from dictionary_learning import ActivationBuffer\n",
    "from circuits.nanogpt_to_hf_transformers import NanogptTokenizer, convert_nanogpt_model\n",
    "from dictionary_learning.utils import hf_dataset_to_generator\n",
    "from dictionary_learning import AutoEncoder\n",
    "\n",
    "import chess_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Load the model, dictionary, data, and activation buffers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "autoencoder_path = \"../autoencoders/ef=8_lr=1e-04_l1=1e-03_layer=5/\"\n",
    "autoencoder_model_path = f\"{autoencoder_path}ae.pt\"\n",
    "autoencoder_config_path = f\"{autoencoder_path}config.json\"\n",
    "\n",
    "DEVICE = torch.device(\"cuda\")\n",
    "ae = AutoEncoder.from_pretrained(autoencoder_model_path, device=DEVICE)\n",
    "\n",
    "with open(autoencoder_config_path, \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "print(config)\n",
    "\n",
    "context_length = config['buffer']['ctx_len']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = NanogptTokenizer()\n",
    "model = convert_nanogpt_model(\"../models/lichess_8layers_ckpt_no_optimizer.pt\", torch.device(DEVICE))\n",
    "model = LanguageModel(model, device_map=DEVICE, tokenizer=tokenizer).to(DEVICE)\n",
    "\n",
    "submodule = model.transformer.h[5].mlp  # layer 1 MLP\n",
    "activation_dim = 512  # output dimension of the MLP\n",
    "dictionary_size = 8 * activation_dim\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "# chess_sae_test is 100MB of data, so no big deal to download it\n",
    "data = hf_dataset_to_generator(\"adamkarvonen/chess_sae_test\", streaming=False)\n",
    "buffer = ActivationBuffer(\n",
    "    data,\n",
    "    model,\n",
    "    submodule,\n",
    "    n_ctxs=512,\n",
    "    ctx_len=256,\n",
    "    refresh_batch_size=4,\n",
    "    io=\"out\",\n",
    "    d_submodule=512,\n",
    "    device=DEVICE,\n",
    "    out_batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect feature activations on total_inputs inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_feature(\n",
    "    activations,\n",
    "    ae: AutoEncoder,\n",
    "    device,\n",
    "):\n",
    "    try:\n",
    "        x = next(activations).to(device)\n",
    "    except StopIteration:\n",
    "        raise StopIteration(\n",
    "            \"Not enough activations in buffer. Pass a buffer with a smaller batch size or more data.\"\n",
    "        )\n",
    "\n",
    "    x_hat, f = ae(x, output_features=True)\n",
    "\n",
    "    return f\n",
    "\n",
    "total_inputs = 8192\n",
    "assert total_inputs % batch_size == 0\n",
    "num_iters = total_inputs // batch_size\n",
    "\n",
    "features = torch.zeros((total_inputs, dictionary_size), device=DEVICE)\n",
    "for i in range(num_iters):\n",
    "    feature = get_feature(buffer, ae, DEVICE) # (batch_size, dictionary_size)\n",
    "    features[i*batch_size:(i+1)*batch_size, :] = feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few plots about various statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firing_rate_per_feature = (features != 0).float().sum(dim=0).cpu() / total_inputs\n",
    "\n",
    "# Creating the histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(firing_rate_per_feature, bins=50, alpha=0.75, color='blue')\n",
    "plt.title('Histogram of firing rates for features')\n",
    "plt.xlabel('Probability')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "firing_rate_per_input = (features != 0).float().sum(dim=-1).cpu() / total_inputs\n",
    "\n",
    "# Creating the histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(firing_rate_per_input, bins=50, alpha=0.75, color='blue')\n",
    "plt.title('Percentage of features firing per input')\n",
    "plt.xlabel('Probability')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I got this from: https://colab.research.google.com/drive/19Qo9wj5rGLjb6KsB9NkKNJkMiHcQhLqo?usp=sharing#scrollTo=WZMhAzLTvw-u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_prob = features.mean(0)\n",
    "print(feat_prob.shape)\n",
    "log_freq = (feat_prob + 1e-10).log10()\n",
    "print(log_freq.shape)\n",
    "\n",
    "log_freq_np = log_freq.cpu().numpy()\n",
    "\n",
    "# Creating the histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(log_freq_np, bins=50, alpha=0.75, color='blue')\n",
    "plt.title('Histogram of log10 of Feature Probabilities')\n",
    "plt.xlabel('log10(Probability)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the L0 statistic. Then, get a list of indices for features that fire between 0 and 50% of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features.shape)\n",
    "l0 = (features != 0).float().sum(dim=-1).mean()\n",
    "print(f\"l0: {l0}\")\n",
    "\n",
    "firing_rate_per_feature = (features != 0).float().sum(dim=0) / total_inputs\n",
    "\n",
    "assert firing_rate_per_feature.shape[0] == dictionary_size\n",
    "\n",
    "mask = (firing_rate_per_feature > 0) & (firing_rate_per_feature < 0.5)\n",
    "idx = torch.nonzero(mask, as_tuple=False).squeeze()\n",
    "print(idx.shape)\n",
    "print(f\"\\n\\nWe have {idx.shape[0]} features that fire between 0 and 50% of the time.\")\n",
    "print(idx[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we collect per dim stats, which include the top tokens it fires on, and the top k inputs and activations per input token.\n",
    "To speed this up, we can reduce the amount of processing by reducing the number of dims with idx[:10]. idx will probably contain thousands\n",
    "of dimensions, and we do n_inputs work for each dimension.\n",
    "At low numbers of dims, runtime is dominated by forwarding the GPT model through n_inputs.\n",
    "At high numbers of dims, runtime is dominated by finding the top inputs per dimension, work of len(dims) * n_inputs.\n",
    "Note that we could probably make the processing much faster, I haven't optimized it at all.\n",
    "\n",
    "Rough ballpark times on my RTX 3050: \n",
    "\n",
    "10 dims, 2400 inputs, batch size 48 = 25 seconds\n",
    "\n",
    "1500 dims, 192 inputs, batch_size 48 = 4 minutes 20 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "from dictionary_learning import interp\n",
    "importlib.reload(interp)\n",
    "\n",
    "from dictionary_learning.interp import examine_dimension\n",
    "\n",
    "\n",
    "per_dim_stats = examine_dimension(model, submodule, buffer, dictionary=ae, dims=idx[:], n_inputs=192, k=30, batch_size=48, device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell looks at syntax related features. Specifically, it looks for features that always fire on a PGN \"counting number\". In this PGN, I've wrapped the \"counting numbers\" in brackets.\n",
    "\n",
    ";<1.>e4 e5 <2.>Nf3 ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(chess_utils)\n",
    "\n",
    "minimum_number_of_activations = 10\n",
    "top_k = 10\n",
    "\n",
    "nonzero_count = 0\n",
    "num_idx_count = 0\n",
    "dim_count = 0\n",
    "max_dims = 10000\n",
    "for dim in per_dim_stats:\n",
    "    dim_count += 1\n",
    "    if dim_count > max_dims:\n",
    "        break\n",
    "\n",
    "    decoded_tokens = per_dim_stats[dim].decoded_tokens\n",
    "    activations = per_dim_stats[dim].activations\n",
    "    # If the dim doesn't have at least 10 firing activations, skip it\n",
    "    if activations[minimum_number_of_activations][-1].item() == 0:\n",
    "        continue\n",
    "    nonzero_count += 1\n",
    "\n",
    "    inputs = [\"\".join(string) for string in decoded_tokens]\n",
    "    num_indices = []\n",
    "    count = 0\n",
    "    for i, pgn in enumerate(inputs[:top_k]):\n",
    "        print(f\"dim: {dim} pgn: {pgn}, activation: {activations[i][-1].item()}\")\n",
    "        nums = chess_utils.find_num_indices(pgn)\n",
    "        num_indices.append(nums)\n",
    "\n",
    "        # If the last token (which contains the max activation for that context) is a number\n",
    "        # Then we count this firing as a \"number index firing\"\n",
    "        if (len(pgn) - 1) in nums:\n",
    "            count += 1\n",
    "\n",
    "    if count == top_k:\n",
    "        print(f\"All top {top_k} activations in dim: {dim} are on num indices\")\n",
    "        num_idx_count += 1\n",
    "print(num_idx_count, nonzero_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonzero_count = 0\n",
    "nonzero_dim_count = 0\n",
    "dim_count = 0\n",
    "top_k = 20\n",
    "\n",
    "max_dims = 10000\n",
    "\n",
    "average_input_length = 0\n",
    "\n",
    "board_tracker = torch.zeros(8, 8, device=\"cpu\")\n",
    "result_dict = {key: 0 for key in range(0, 13)}\n",
    "length_tracker = []\n",
    "board_to_state_fn = chess_utils.board_to_piece_state\n",
    "\n",
    "for dim in per_dim_stats:\n",
    "    dim_count += 1\n",
    "    if dim_count > max_dims:\n",
    "        break\n",
    "    decoded_tokens = per_dim_stats[dim].decoded_tokens\n",
    "    activations = per_dim_stats[dim].activations\n",
    "    if activations[10][-1].item() == 0:\n",
    "        continue\n",
    "    nonzero_count += 1\n",
    "    inputs = [\"\".join(string) for string in decoded_tokens]\n",
    "    inputs = inputs[:top_k]\n",
    "\n",
    "    chess_boards = [chess_utils.pgn_string_to_board(pgn, allow_exception=True) for pgn in inputs]\n",
    "    \n",
    "    one_hot_list = chess_utils.chess_boards_to_state_stack(chess_boards, DEVICE, board_to_state_fn)\n",
    "    one_hot_list = chess_utils.mask_initial_board_states(one_hot_list, DEVICE, board_to_state_fn)\n",
    "    averaged_one_hot = chess_utils.get_averaged_states(one_hot_list)\n",
    "    common_indices = chess_utils.find_common_states(averaged_one_hot, 0.9)\n",
    "\n",
    "    if any(len(idx) > 0 for idx in common_indices):\n",
    "        nonzero_dim_count += 1  # Increment if there are nonzero indices\n",
    "        average_input_length = sum(len(pgn) for pgn in inputs) / len(inputs)\n",
    "        length_tracker.append(average_input_length)\n",
    "\n",
    "    for idx in zip(*common_indices):\n",
    "        value = averaged_one_hot[idx].item()\n",
    "        # print(f\"Dim: {dim}, Average input length: {int(average_input_length):04}, Value: {value:.2f} at Index: {idx}\")\n",
    "        board_tracker[idx[0], idx[1]] += 1\n",
    "        result_dict[idx[2].item()] += 1\n",
    "print(nonzero_dim_count, nonzero_count)\n",
    "\n",
    "for key, count in result_dict.items():\n",
    "    print(f\"Index: {key}, Count: {count}\")\n",
    "print(board_tracker.flip(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonzero_count = 0\n",
    "nonzero_dim_count = 0\n",
    "dim_count = 0\n",
    "top_k = 20\n",
    "\n",
    "max_dims = 100\n",
    "\n",
    "average_input_length = 0\n",
    "\n",
    "board_tracker = torch.zeros(8, 8, device=\"cpu\")\n",
    "result_dict = {key: 0 for key in range(0, 13)}\n",
    "length_tracker = []\n",
    "board_to_state_fn = chess_utils.board_to_threat_state\n",
    "\n",
    "for dim in per_dim_stats:\n",
    "    dim_count += 1\n",
    "    if dim_count > max_dims:\n",
    "        break\n",
    "    decoded_tokens = per_dim_stats[dim].decoded_tokens\n",
    "    activations = per_dim_stats[dim].activations\n",
    "    if activations[10][-1].item() == 0:\n",
    "        continue\n",
    "    nonzero_count += 1\n",
    "    inputs = [\"\".join(string) for string in decoded_tokens]\n",
    "    inputs = inputs[:top_k]\n",
    "\n",
    "    chess_boards = [chess_utils.pgn_string_to_board(pgn, allow_exception=True) for pgn in inputs]\n",
    "    \n",
    "    one_hot_list = chess_utils.chess_boards_to_state_stack(chess_boards, DEVICE, board_to_state_fn)\n",
    "    one_hot_list = chess_utils.mask_initial_board_states(one_hot_list, DEVICE, board_to_state_fn)\n",
    "    averaged_one_hot = chess_utils.get_averaged_states(one_hot_list)\n",
    "    common_indices = chess_utils.find_common_states(averaged_one_hot, 0.9)\n",
    "\n",
    "    if any(len(idx) > 0 for idx in common_indices):\n",
    "        nonzero_dim_count += 1  # Increment if there are nonzero indices\n",
    "        average_input_length = sum(len(pgn) for pgn in inputs) / len(inputs)\n",
    "        length_tracker.append(average_input_length)\n",
    "\n",
    "    for idx in zip(*common_indices):\n",
    "        value = averaged_one_hot[idx].item()\n",
    "        board_tracker[idx[0], idx[1]] += 1\n",
    "        result_dict[idx[2].item()] += 1\n",
    "print(nonzero_dim_count, nonzero_count)\n",
    "\n",
    "for key, count in result_dict.items():\n",
    "    print(f\"Index: {key}, Count: {count}\")\n",
    "print(board_tracker.flip(0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "circuits",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
