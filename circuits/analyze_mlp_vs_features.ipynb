{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import einops\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from circuits.dictionary_learning.dictionary import AutoEncoder, AutoEncoderNew, GatedAutoEncoder\n",
    "import circuits.utils as utils\n",
    "import circuits.analysis as analysis\n",
    "import circuits.eval_sae_as_classifier as eval_sae\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results_filename(othello: bool) -> str:\n",
    "    if othello:\n",
    "        return 'indexing_None_n_inputs_1000_results.pkl'\n",
    "    else:\n",
    "        return 'indexing_find_dots_indices_n_inputs_1000_results.pkl'\n",
    "\n",
    "def get_layer_data(base_path: str, othello: bool):\n",
    "\n",
    "    results_filename = get_results_filename(othello)\n",
    "\n",
    "    with open(os.path.join(base_path, results_filename), \"rb\") as f:\n",
    "        results = pickle.load(f)\n",
    "    results = utils.to_device(results, device)\n",
    "    feature_labels, misc_stats = analysis.analyze_results_dict(\n",
    "        results,\n",
    "        \"\",\n",
    "        device,\n",
    "        save_results=False,\n",
    "        verbose=False,\n",
    "        print_results=False,\n",
    "        significance_threshold=10,\n",
    "    )\n",
    "\n",
    "    with open(os.path.join(base_path, \"n_inputs_1000_evals.pkl\"), \"rb\") as f:\n",
    "        eval_results = pickle.load(f)\n",
    "\n",
    "    return feature_labels\n",
    "\n",
    "\n",
    "def get_dataset(othello: bool, context_length: int):\n",
    "    if not othello:\n",
    "        with open(\"models/meta.pkl\", \"rb\") as f:\n",
    "            meta = pickle.load(f)\n",
    "\n",
    "        dataset_name = \"adamkarvonen/chess_sae_text\"\n",
    "        data = utils.chess_hf_dataset_to_generator(\n",
    "            dataset_name, meta, context_length=context_length, split=\"train\", streaming=True\n",
    "        )\n",
    "    else:\n",
    "        dataset_name = \"adamkarvonen/othello_45MB_games\"\n",
    "        data = utils.othello_hf_dataset_to_generator(\n",
    "            dataset_name, context_length=context_length, split=\"train\", streaming=False\n",
    "        )\n",
    "    return data\n",
    "\n",
    "\n",
    "def rc_to_square_notation(row, col):\n",
    "    letters = \"ABCDEFGH\"\n",
    "    number = 8 - row\n",
    "    letter = letters[col]\n",
    "    return f\"{letter}{number}\"\n",
    "\n",
    "\n",
    "def collect_activations(\n",
    "    games_bL: torch.Tensor,\n",
    "    context_length: int,\n",
    "    ae_bundle: utils.AutoEncoderBundle,\n",
    "    feature_labels: dict,\n",
    "    device,\n",
    "    total_games: int,\n",
    "    batch_size: int,\n",
    ") -> torch.Tensor:\n",
    "    alive_features_F = feature_labels['alive_features']\n",
    "    activations_bLF = torch.zeros(\n",
    "        (total_games, context_length, alive_features_F.shape[0]), dtype=torch.float32, device=device\n",
    "    )\n",
    "    for i in tqdm(range(0, total_games, batch_size)):\n",
    "        games_batch_BL = games_bL[i:i + batch_size]\n",
    "        activations_FBL, tokens = eval_sae.collect_activations_batch(ae_bundle, games_batch_BL, alive_features_F)\n",
    "        activations_BLF = einops.rearrange(activations_FBL, \"F B L -> B L F\")\n",
    "        activations_bLF[i:i + batch_size] = activations_BLF\n",
    "\n",
    "\n",
    "    feature_activations_Fb = einops.rearrange(activations_bLF, \"B L F -> F (B L)\")\n",
    "    return feature_activations_Fb\n",
    "\n",
    "\n",
    "func_name = \"games_batch_to_valid_moves_BLRRC\"\n",
    "othello = True\n",
    "device = \"cuda\"\n",
    "# device = 'cpu'\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "layer1 = 5\n",
    "layer2 = 5\n",
    "\n",
    "ae_path1 = f\"../autoencoders/othello_mlp_acts_identity_aes/layer_{layer1}/\"\n",
    "# ae_path2 = f\"../autoencoders/othello_mlp_acts_identity_aes/layer_{layer2}/\"\n",
    "ae_path2 = f\"../autoencoders/all_layers_othello_p_anneal_0530/layer_{layer2}/trainer0/\"\n",
    "\n",
    "assert(f\"layer_{layer1}\" in ae_path1)\n",
    "assert(f\"layer_{layer2}\" in ae_path2)\n",
    "\n",
    "\n",
    "ae_bundle1 = utils.get_ae_bundle(\n",
    "    ae_path1, device, data=[], batch_size=1\n",
    ")\n",
    "ae_bundle2 = utils.get_ae_bundle(\n",
    "    ae_path2, device, data=[], batch_size=1\n",
    ")\n",
    "\n",
    "ae_bundle1.buffer = None\n",
    "ae_bundle2.buffer = None\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "context_length = ae_bundle1.context_length\n",
    "ae1 = ae_bundle1.ae\n",
    "ae2 = ae_bundle2.ae\n",
    "\n",
    "data = get_dataset(othello, context_length)\n",
    "\n",
    "threshold1 = 3\n",
    "threshold2 = 3\n",
    "\n",
    "feature_labels1 = get_layer_data(ae_path1, othello)\n",
    "feature_labels2 = get_layer_data(ae_path2, othello)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_filename = \"hpc_hrc_same_square_indexes_dict.json\"\n",
    "\n",
    "with open(json_filename, \"r\") as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "print(json_data.keys())\n",
    "\n",
    "hpr_indices = json_data['high_precision_and_recall']\n",
    "hpr_indices_tensor = torch.tensor(hpr_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_cos_sims_for_single_feature(x_vector_D: torch.Tensor, y_vectors_FD: torch.Tensor) -> list[float]:\n",
    "    return F.cosine_similarity(x_vector_D, y_vectors_FD)\n",
    "\n",
    "def get_all_max_cos_sims(x_vectors_FD: torch.Tensor, y_vectors_FD: torch.Tensor) -> torch.Tensor:\n",
    "    max_cos_sims = []\n",
    "\n",
    "    for x_vector in x_vectors_FD:\n",
    "        max_cos_sims.append(get_all_cos_sims_for_single_feature(x_vector, y_vectors_FD).max())\n",
    "    return torch.stack(max_cos_sims)\n",
    "\n",
    "x_vectors_FD = ae_bundle2.model.blocks[layer1].mlp.W_out\n",
    "filtered_x_vectors_FD = x_vectors_FD[hpr_indices_tensor]\n",
    "y_vectors_DF = ae_bundle2.ae.decoder.weight\n",
    "y_vectors_FD = einops.rearrange(y_vectors_DF, 'D F -> F D')\n",
    "y_vectors_FD = ae_bundle2.ae.encoder.weight\n",
    "print(x_vectors_FD.shape)\n",
    "print(y_vectors_DF.shape)\n",
    "print(y_vectors_FD.shape)\n",
    "\n",
    "all_mlp_sims = get_all_max_cos_sims(x_vectors_FD, y_vectors_FD)\n",
    "\n",
    "hpr_cos_sims = get_all_max_cos_sims(filtered_x_vectors_FD, y_vectors_FD)\n",
    "\n",
    "plt.title(\"MLP Decoder vs SAE Encoder\")\n",
    "plt.xlabel(\"Cosine Similarity\")\n",
    "plt.ylabel(\"Density\")\n",
    "\n",
    "# Plot the first histogram (all_mlp_sims) with some transparency (alpha)\n",
    "plt.hist(all_mlp_sims.cpu().numpy(), bins=20, alpha=0.5, label='All 2048 MLP Neurons', density=True)\n",
    "\n",
    "# Plot the second histogram (hpr_cos_sims) on top of the first one\n",
    "plt.hist(hpr_cos_sims.cpu().numpy(), bins=20, alpha=0.5, label='150 Legal Move MLP Neurons', density=True)\n",
    "\n",
    "plt.legend()\n",
    "plt.savefig(\"mlp_ae_encoder_cos_sims.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "total_games = batch_size * 10\n",
    "\n",
    "games_bL = torch.zeros((total_games, context_length), dtype=torch.long, device=device)\n",
    "\n",
    "for i in range(0, total_games, batch_size):\n",
    "    game_batch_BL = [next(data) for _ in range(batch_size)]\n",
    "    game_batch_BL = torch.tensor(game_batch_BL, device=device)\n",
    "    games_bL[i:i + batch_size] = game_batch_BL\n",
    "    \n",
    "activations1 = collect_activations(\n",
    "    games_bL, context_length, ae_bundle1, feature_labels1, device, total_games, batch_size\n",
    ")\n",
    "activations2 = collect_activations(\n",
    "    games_bL, context_length, ae_bundle2, feature_labels2, device, total_games, batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearson_corr(x, y):\n",
    "    mean_x = x.mean(dim=-1, keepdim=True)\n",
    "    mean_y = y.mean(dim=-1, keepdim=True)\n",
    "    xm = x - mean_x\n",
    "    ym = y - mean_y\n",
    "    r_num = torch.sum(xm * ym, dim=-1)\n",
    "    r_den = torch.sqrt(torch.sum(xm * xm, dim=-1) * torch.sum(ym * ym, dim=-1))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        zero_variance = r_den == 0\n",
    "    r = torch.where(zero_variance, torch.zeros_like(r_num), r_num / r_den)\n",
    "\n",
    "    return r\n",
    "\n",
    "# def get_correlation_for_activation(x_activations_Fb: torch.Tensor, x_activation_index: int, y_activations_Fb: torch.Tensor) -> torch.Tensor:\n",
    "#     x_activation_b = x_activations_Fb[x_activation_index]\n",
    "#     correlations = pearson_corr(x_activation_b.unsqueeze(0), y_activations_Fb)\n",
    "#     return correlations\n",
    "\n",
    "# def get_correlation_per_feature(activations1: torch.Tensor, activations2: torch.Tensor) -> list[float]:\n",
    "#     correlations = pearson_corr(activations1.unsqueeze(1), activations2.unsqueeze(0))\n",
    "#     best_correlations, _ = torch.max(correlations, dim=1)\n",
    "#     best_correlations = best_correlations[best_correlations != 0]\n",
    "#     return best_correlations.tolist()\n",
    "\n",
    "def get_correlation_for_activation(x_activations_Fb: torch.Tensor, x_activation_index: int, y_activations_Fb: torch.Tensor) -> torch.Tensor:\n",
    "    x_activation_b = x_activations_Fb[x_activation_index]\n",
    "    correlations = pearson_corr(x_activation_b.unsqueeze(0), y_activations_Fb)\n",
    "    return correlations\n",
    "\n",
    "def get_correlation_per_feature(activations1: torch.Tensor, activations2: torch.Tensor, batch_size: int = 1000) -> list[float]:\n",
    "    num_activations1 = activations1.shape[0]\n",
    "    num_activations2 = activations2.shape[0]\n",
    "    \n",
    "    best_correlations = []\n",
    "    \n",
    "    for i in tqdm(range(0, num_activations1, batch_size)):\n",
    "        start_idx = i\n",
    "        end_idx = min(i + batch_size, num_activations1)\n",
    "        \n",
    "        correlations = pearson_corr(activations1[start_idx:end_idx].unsqueeze(1), activations2.unsqueeze(0))\n",
    "        batch_best_correlations, _ = torch.max(correlations, dim=1)\n",
    "        batch_best_correlations = batch_best_correlations[batch_best_correlations != 0]\n",
    "        best_correlations.extend(batch_best_correlations.tolist())\n",
    "    \n",
    "    return best_correlations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "hpr_mlp_activations1 = activations1[hpr_indices]\n",
    "print(hpr_mlp_activations1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_correlations_hpr2 = get_correlation_per_feature(hpr_mlp_activations1, activations2, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_correlations_hpr = get_correlation_per_feature(hpr_mlp_activations1, activations2, batch_size=4)\n",
    "\n",
    "best_correlations_all = get_correlation_per_feature(activations1, activations2, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.title(\"Max Activation Correlation between Layer 5 MLP Neurons and SAE Features\")\n",
    "plt.xlabel(\"Pearson Correlation\")\n",
    "plt.ylabel(\"Density\")\n",
    "\n",
    "# Plot the second histogram (hpr_cos_sims) on top of the first one\n",
    "plt.hist(best_correlations_all, bins=20, alpha=0.5, label='All 2048 MLP Neurons', density=True)\n",
    "\n",
    "# Plot the first histogram (all_mlp_sims) with some transparency (alpha)\n",
    "plt.hist(best_correlations_hpr, bins=20, alpha=0.5, label='150 Legal Move MLP Neurons', density=True)\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig(\"mlp_ae_activation_correlation.png\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_correlations_per_labeled_feature(\n",
    "    activations1: torch.Tensor,\n",
    "    activations2: torch.Tensor,\n",
    "    features1_F: torch.Tensor,\n",
    "    features2_F: torch.Tensor,\n",
    "    verbose: bool = False,\n",
    ") -> list[float]:\n",
    "\n",
    "    best_correlations_same_label = []\n",
    "    best_correlations = []\n",
    "    for f in features1_F[0]:\n",
    "        if verbose:\n",
    "            print(f\"\\n\\n\\n\\nFeature {f.item()}\")\n",
    "        correlations = get_correlation_for_activation(activations1, f, activations2)\n",
    "        k = 5\n",
    "        values, indices = torch.topk(correlations, k)\n",
    "\n",
    "        # Printing the top n values and their corresponding indices\n",
    "        best_same_label_correlation = 0.0\n",
    "        best_correlation = values[0].item()\n",
    "        if best_correlation != 0.0:\n",
    "            best_correlations.append(best_correlation)\n",
    "        for index, value in zip(indices, values):\n",
    "            if verbose:\n",
    "                print(f\"Feature Index: {index}, Value: {value.item():.2f}\")\n",
    "            if index in features2_F[0] and best_same_label_correlation == 0.0:\n",
    "                best_same_label_correlation = value.item()\n",
    "        if best_correlation != 0.0:\n",
    "            best_correlations_same_label.append(best_same_label_correlation)\n",
    "    return best_correlations_same_label, best_correlations\n",
    "\n",
    "\n",
    "def compare_feature_labels(\n",
    "    feature_labels1: dict,\n",
    "    feature_labels2: dict,\n",
    "    activations1: torch.Tensor,\n",
    "    activations2: torch.Tensor,\n",
    "    threshold1: int,\n",
    "    threshold2: int,\n",
    "    func_name: str,\n",
    "    verbose: bool = False,\n",
    "):\n",
    "    labels1_TFRRC = feature_labels1[func_name]\n",
    "    labels2_TFRRC = feature_labels2[func_name]\n",
    "\n",
    "    counts1_T = einops.reduce(labels1_TFRRC, \"T F R1 R2 C -> T\", \"sum\")\n",
    "    counts2_T = einops.reduce(labels2_TFRRC, \"T F R1 R2 C -> T\", \"sum\")\n",
    "\n",
    "    print(f\"Counts1: {counts1_T}\")\n",
    "    print(f\"Counts2: {counts2_T}\")\n",
    "\n",
    "    labels1_FRRC = labels1_TFRRC[threshold1]\n",
    "    labels2_FRRC = labels2_TFRRC[threshold2]\n",
    "\n",
    "    best_correlations1 = []\n",
    "    best_correlation_same_label1 = []\n",
    "    best_correlations2 = []\n",
    "    best_correlation_same_label2 = []\n",
    "\n",
    "    for square in tqdm(range(64)):\n",
    "        r = square // 8\n",
    "        c = square % 8\n",
    "\n",
    "        if r != 0:\n",
    "            continue\n",
    "        if c != 4:\n",
    "            continue\n",
    "\n",
    "        index = (r, c, 0)\n",
    "        features1_F = torch.where(labels1_FRRC[:, index[0], index[1], index[2]] == 1)\n",
    "        features2_F = torch.where(labels2_FRRC[:, index[0], index[1], index[2]] == 1)\n",
    "\n",
    "        \n",
    "\n",
    "        correlations_same_label1, correlations1 = get_correlations_per_labeled_feature(activations1, activations2, features1_F, features2_F, verbose=verbose)\n",
    "        best_correlation_same_label1.extend(correlations_same_label1)\n",
    "        best_correlations1.extend(correlations1)\n",
    "\n",
    "        correlations_same_label2, correlations2 = get_correlations_per_labeled_feature(activations2, activations1, features2_F, features1_F, verbose=verbose)\n",
    "        best_correlation_same_label2.extend(correlations_same_label2)\n",
    "        best_correlations2.extend(correlations2)\n",
    "\n",
    "        if verbose:\n",
    "            print(r, c)\n",
    "            print(f\"Features1: {features1_F}\")\n",
    "            print(f\"Features2: {features2_F}\")\n",
    "            print(f\"Correlations1: {correlations1}\")\n",
    "            print(f\"Correlations2: {correlations2}\")\n",
    "\n",
    "    return best_correlations1, best_correlations2, best_correlation_same_label1, best_correlation_same_label2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_correlations1, best_correlations2, best_correlations_same_label1, best_correlations_same_label2 = compare_feature_labels(\n",
    "    feature_labels1, feature_labels2, activations1, activations2, threshold1, threshold2, func_name, verbose=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
