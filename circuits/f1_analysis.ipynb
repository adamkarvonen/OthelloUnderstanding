{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import einops\n",
    "import importlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "import circuits.analysis as analysis\n",
    "import circuits.eval_sae_as_classifier as eval_sae\n",
    "import circuits.chess_utils as chess_utils\n",
    "import circuits.utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# # for testing purposes\n",
    "\n",
    "# # Define a sample 3D tensor with random values\n",
    "# # Dimensions: T x F x C (let's use 2 x 3 x 4 for simplicity)\n",
    "# f1_TFC = torch.randn(2, 3, 4)\n",
    "# print(\"Original Tensor (T x F x C):\")\n",
    "# print(f1_TFC)\n",
    "\n",
    "# def best_f1_average(f1_TFC: torch.Tensor) -> torch.Tensor:\n",
    "#     # Apply torch.max along the last dimension (dimension 2)\n",
    "#     # Select only the values, ignoring the indices\n",
    "#     f1_TF, _ = torch.max(f1_TFC, dim=1)\n",
    "#     return f1_TF\n",
    "\n",
    "# # Compute the maximum along the 'C' dimension and reduce to a 2D tensor\n",
    "# f1_TF = best_f1_average(f1_TFC)\n",
    "# print(\"\\nReduced Tensor (T x F) with max values from 'C':\")\n",
    "# print(f1_TF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_all_blanks(results: dict, device) -> dict:\n",
    "    custom_functions = analysis.get_all_custom_functions(results)\n",
    "    for function in custom_functions:\n",
    "        function_name = function.__name__\n",
    "\n",
    "        if function == chess_utils.board_to_piece_state or function == chess_utils.board_to_piece_color_state:\n",
    "            on_TFRRC = results[function_name]['on']\n",
    "            off_TFRRC = results[function_name]['off']\n",
    "            results[function_name]['on'] = analysis.mask_initial_board_state(on_TFRRC, function, device)\n",
    "            results[function_name]['off'] = analysis.mask_initial_board_state(off_TFRRC, function, device)\n",
    "\n",
    "    return results\n",
    "\n",
    "def best_f1_average(f1_TFRRC: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"For every threshold, for every square, find the best F1 score across all features. Then average across all squares.\n",
    "    NOTE: If the function is binary, num_squares == 1. If it is board to piece state, num_squares == 8 * 8 * 12\"\"\"\n",
    "    f1_TRRC, _ = torch.max(f1_TFRRC, dim=1)\n",
    "\n",
    "    T, R1, R2, C = f1_TRRC.shape\n",
    "\n",
    "    max_possible = R1 * R2 * C\n",
    "\n",
    "    f1_T = einops.reduce(f1_TRRC, 'T R1 R2 C -> T', 'sum') / max_possible\n",
    "\n",
    "    return f1_T\n",
    "    \n",
    "\n",
    "def f1s_above_threshold(f1_TFRRC: torch.Tensor, threshold: float) -> torch.Tensor:\n",
    "    \"\"\"For every threshold, for every square, find the best F1 score across all features. Then, find the number of squares that have a F1 score above the threshold.\n",
    "    If the function is binary, num_squares == 1. If it is board to piece state, num_squares == 8 * 8 * 12\n",
    "    NOTE: This will probably be most useful for features with 8x8xn options.\"\"\"\n",
    "    f1_TRRC, _ = torch.max(f1_TFRRC, dim=1)\n",
    "\n",
    "    f1s_above_threshold_TRCC = f1_TRRC > threshold\n",
    "\n",
    "    T, R1, R2, C = f1_TRRC.shape\n",
    "\n",
    "    max_possible = R1 * R2 * C\n",
    "\n",
    "    f1_T = einops.reduce(f1s_above_threshold_TRCC, 'T R1 R2 C -> T', 'sum')\n",
    "\n",
    "    return f1_T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define more custom functions above. At the bottom of this cell, by the NOTE, use the custom function you are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(analysis)\n",
    "\n",
    "device = \"cpu\"\n",
    "# device = \"cuda\"\n",
    "mask = False\n",
    "\n",
    "autoencoder_group_paths = [\"../autoencoders/chess_layer5_large_sweep/\"]\n",
    "autoencoder_group_paths = [\"../autoencoders/group-2024-05-14_chess/\"]\n",
    "# autoencoder_group_paths = [\"../autoencoders/chess_layer0/\"]\n",
    "\n",
    "custom_functions = []\n",
    "custom_function_names = []\n",
    "\n",
    "csv_results_file = \"../autoencoders/chess_layer5_large_sweep/results.csv\"\n",
    "# csv_results_file = \"../autoencoders/chess_layer0/results.csv\"\n",
    "csv_results_file = \"../autoencoders/group-2024-05-14_chess/results.csv\"\n",
    "\n",
    "df = pd.read_csv(csv_results_file)\n",
    "\n",
    "all_sae_results = {}\n",
    "\n",
    "results_filename_filter = \"1000\" # This is only necessary if you have multiple files with multiple n_inputs\n",
    "# e.g. indexing_find_dots_indices_n_inputs_1000_results.pkl and indexing_find_dots_indices_n_inputs_5000_results.pkl\n",
    "# In this case, if you want to view the results for n_inputs = 1000, you would set filter = \"1000\"\n",
    "\n",
    "for autoencoder_group_path in autoencoder_group_paths:\n",
    "\n",
    "    folders = eval_sae.get_nested_folders(autoencoder_group_path)\n",
    "    sae_results = {}\n",
    "\n",
    "    for autoencoder_path in folders:\n",
    "\n",
    "        print(f\"Processing {autoencoder_path}\")\n",
    "\n",
    "        assert autoencoder_path in df[\"autoencoder_path\"].values, f\"{autoencoder_path} not in csv file\"\n",
    "\n",
    "        sae_results[autoencoder_path] = {}\n",
    "\n",
    "        results_filenames = analysis.get_all_results_file_names(autoencoder_path, results_filename_filter)\n",
    "        if len(results_filenames) > 1 or len(results_filenames) == 0:\n",
    "            print(f\"Skipping {autoencoder_path} because it has {len(results_filenames)} results files\")\n",
    "            print(\"This is most likely because there are results files from different n_inputs\")\n",
    "            continue\n",
    "        results_filename = results_filenames[0]\n",
    "\n",
    "        with open(autoencoder_path + results_filename, \"rb\") as f:\n",
    "            results = pickle.load(f)\n",
    "\n",
    "        results = utils.to_device(results, device)\n",
    "\n",
    "        custom_functions = analysis.get_all_custom_functions(results)\n",
    "        for function in custom_functions:\n",
    "            function_name = function.__name__\n",
    "            custom_function_names.append(function_name)\n",
    "        \n",
    "        results = analysis.add_off_tracker(results, custom_functions, device)\n",
    "        f1_dict_TFRRC = analysis.get_all_f1s(results, device)\n",
    "\n",
    "        feature_labels = analysis.analyze_results_dict(results, output_path=\"\", device=device, high_threshold=0.95, low_threshold=0.1, significance_threshold=10, save_results=False, mask=mask, verbose=False, print_results=False)\n",
    "\n",
    "        correct_row = df[\"autoencoder_path\"] == autoencoder_path\n",
    "        sae_results[autoencoder_path][\"l0\"] = df[correct_row][\"l0\"].values[0]\n",
    "        sae_results[autoencoder_path][\"frac_variance_explained\"] = df[correct_row][\"frac_variance_explained\"].values[0]\n",
    "\n",
    "        for func_name in custom_function_names:\n",
    "            if func_name in all_sae_results:\n",
    "                continue\n",
    "\n",
    "            T = f1_dict_TFRRC[func_name].shape[0]\n",
    "            f1_counter_T = torch.zeros(T, device=device)\n",
    "            all_sae_results[func_name] = {\"f1_counter\": f1_counter_T}\n",
    "\n",
    "        for func_name in f1_dict_TFRRC:\n",
    "            config = chess_utils.config_lookup[func_name]\n",
    "            custom_function = config.custom_board_state_function\n",
    "            assert custom_function in custom_functions, f\"Key {custom_function} not in custom_functions\"\n",
    "            f1_TFRRC = f1_dict_TFRRC[func_name]\n",
    "\n",
    "\n",
    "            # NOTE: Set your function of interest here\n",
    "            f1_T = best_f1_average(f1_TFRRC)\n",
    "            # f1_T = f1s_above_threshold(f1_TFRRC, 0.5)\n",
    "            all_sae_results[func_name][\"f1_counter\"] += f1_T\n",
    "\n",
    "            sae_results[autoencoder_path][func_name] = f1_T\n",
    "\n",
    "        # torch.cuda.empty_cache()\n",
    "    all_sae_results[autoencoder_group_path] = sae_results\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, this looks at the best_idx. If you want to look at a particular threshold, set `best_idx = 3` or whatever you are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for func_name in custom_function_names:\n",
    "\n",
    "    new_column_name = f\"{func_name}_best_custom_metric\"\n",
    "    if new_column_name not in df.columns:\n",
    "        df[new_column_name] = np.nan\n",
    "    \n",
    "    second_column_name = f\"{func_name}_best_custom_metric_idx\"\n",
    "\n",
    "    f1_counter_T = all_sae_results[func_name][\"f1_counter\"]\n",
    "    best_idx = torch.argmax(f1_counter_T)\n",
    "\n",
    "    for autoencoder_group_path in autoencoder_group_paths:\n",
    "        folders = eval_sae.get_nested_folders(autoencoder_group_path)\n",
    "        \n",
    "        for autoencoder_path in folders:\n",
    "            f1_T = all_sae_results[autoencoder_group_path][autoencoder_path][func_name]\n",
    "            best_f1 = f1_T[best_idx]\n",
    "            df.loc[df[\"autoencoder_path\"] == autoencoder_path, new_column_name] = best_f1.item()\n",
    "            df.loc[df[\"autoencoder_path\"] == autoencoder_path, second_column_name] = best_idx.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_metric_columns = []\n",
    "custom_metric_idx_columns = []\n",
    "for col in df.columns:\n",
    "    if \"custom_metric\" in col and \"idx\" not in col:\n",
    "        custom_metric_columns.append(col)\n",
    "        print(col)\n",
    "    \n",
    "        idx_column_name = col + \"_idx\"\n",
    "        custom_metric_idx_columns.append(idx_column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_f1_columns = []\n",
    "best_f1_idx_columns = []\n",
    "for col in df.columns:\n",
    "    if \"best_f1_score_per_square\" in col:\n",
    "        best_f1_columns.append(col)\n",
    "        print(col)\n",
    "        f1_idx = col.replace(\"best_f1_score_per_square\", \"best_idx\")\n",
    "        best_f1_idx_columns.append(f1_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next 2 cells find the average f1 score and custom metric score for all functions, all 8x8 board state functions, and all binary functions, then store it in the df and `average_metric_columns` and `average_metric_idx_columns`. It's pretty verbose, but it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "board_state_8x8_columns = [\"board_to_piece_state\", \"board_to_piece_color_state\", \"board_to_threat_state\", \"board_to_legal_moves_state\", \"board_to_pseudo_legal_moves_state\"]\n",
    "\n",
    "def get_board_state_columns(board_state_columns: list[str], columns: list[str], include: bool = True) -> list[str]:\n",
    "    result_columns = []\n",
    "    for col in columns:\n",
    "        if any([board_state in col for board_state in board_state_columns]):\n",
    "            if include:\n",
    "                result_columns.append(col)\n",
    "        else:\n",
    "            if not include:\n",
    "                result_columns.append(col)\n",
    "    return result_columns\n",
    "\n",
    "best_f1_board_state_columns = get_board_state_columns(board_state_8x8_columns, best_f1_columns, include=True)\n",
    "best_f1_board_state_idx_columns = get_board_state_columns(board_state_8x8_columns, best_f1_idx_columns, include=True)\n",
    "\n",
    "best_custom_metric_board_state_columns = get_board_state_columns(board_state_8x8_columns, custom_metric_columns, include=True)\n",
    "best_custom_metric_board_state_idx_columns = get_board_state_columns(board_state_8x8_columns, custom_metric_idx_columns, include=True)\n",
    "\n",
    "best_f1_binary_columns = get_board_state_columns(board_state_8x8_columns, best_f1_columns, include=False)\n",
    "best_f1_binary_idx_columns = get_board_state_columns(board_state_8x8_columns, best_f1_idx_columns, include=False)\n",
    "\n",
    "best_custom_metric_binary_columns = get_board_state_columns(board_state_8x8_columns, custom_metric_columns, include=False)\n",
    "best_custom_metric_binary_idx_columns = get_board_state_columns(board_state_8x8_columns, custom_metric_idx_columns, include=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_average_metric_over_functions(\n",
    "    df: pd.DataFrame,\n",
    "    metric_type: str,\n",
    "    average_metric_columns: list[str],\n",
    "    average_metric_idx_columns: list[str],\n",
    "    custom_metric_columns: list[str],\n",
    "    custom_metric_idx_columns: list[str],\n",
    ") -> tuple[pd.DataFrame, list[str], list[str]]:\n",
    "\n",
    "    average_metric_column = f\"{metric_type}_average\"\n",
    "    average_metric_idx_column = f\"{metric_type}_average_idx\"\n",
    "\n",
    "    average_metric_columns.append(average_metric_column)\n",
    "    average_metric_idx_columns.append(average_metric_idx_column)\n",
    "\n",
    "    df[average_metric_column] = np.nan\n",
    "    df[average_metric_idx_column] = np.nan\n",
    "\n",
    "    df[average_metric_column] = df[custom_metric_columns].mean(axis=1)\n",
    "    df[average_metric_idx_column] = df[custom_metric_idx_columns].mean(axis=1)\n",
    "\n",
    "    return df, average_metric_columns, average_metric_idx_columns\n",
    "\n",
    "\n",
    "average_metric_columns = []\n",
    "average_metric_idx_columns = []\n",
    "\n",
    "df, average_metric_columns, average_metric_idx_columns = add_average_metric_over_functions(\n",
    "    df,\n",
    "    \"best_f1_score_per_square\",\n",
    "    average_metric_columns,\n",
    "    average_metric_idx_columns,\n",
    "    best_f1_board_state_columns,\n",
    "    best_f1_board_state_idx_columns,\n",
    ")\n",
    "df, average_metric_columns, average_metric_idx_columns = add_average_metric_over_functions(\n",
    "    df,\n",
    "    \"best_custom_metric\",\n",
    "    average_metric_columns,\n",
    "    average_metric_idx_columns,\n",
    "    best_custom_metric_board_state_columns,\n",
    "    best_custom_metric_board_state_idx_columns,\n",
    ")\n",
    "df, average_metric_columns, average_metric_idx_columns = add_average_metric_over_functions(\n",
    "    df,\n",
    "    \"best_f1_score_per_square_only_board_state\",\n",
    "    average_metric_columns,\n",
    "    average_metric_idx_columns,\n",
    "    best_f1_board_state_columns,\n",
    "    best_f1_board_state_idx_columns,\n",
    ")\n",
    "df, average_metric_columns, average_metric_idx_columns = add_average_metric_over_functions(\n",
    "    df,\n",
    "    \"best_custom_metric_only_board_state\",\n",
    "    average_metric_columns,\n",
    "    average_metric_idx_columns,\n",
    "    best_custom_metric_board_state_columns,\n",
    "    best_custom_metric_board_state_idx_columns,\n",
    ")\n",
    "df, average_metric_columns, average_metric_idx_columns = add_average_metric_over_functions(\n",
    "    df,\n",
    "    \"best_f1_score_per_square_only_binary\",\n",
    "    average_metric_columns,\n",
    "    average_metric_idx_columns,\n",
    "    best_f1_binary_columns,\n",
    "    best_f1_binary_idx_columns,\n",
    ")\n",
    "df, average_metric_columns, average_metric_idx_columns = add_average_metric_over_functions(\n",
    "    df,\n",
    "    \"best_custom_metric_only_binary\",\n",
    "    average_metric_columns,\n",
    "    average_metric_idx_columns,\n",
    "    best_custom_metric_binary_columns,\n",
    "    best_custom_metric_binary_idx_columns,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df\n",
    "df.to_csv(\"processed_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only the numerical columns\n",
    "numerical_columns = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "numerical_data = df[numerical_columns]\n",
    "\n",
    "# calculate the correlation matrix\n",
    "correlation_matrix = numerical_data.corr()\n",
    "\n",
    "# create a heatmap using plotly\n",
    "fig = px.imshow(correlation_matrix, \n",
    "                labels=dict(x=\"Columns\", y=\"Columns\", color=\"Correlation\"),\n",
    "                x=correlation_matrix.columns,\n",
    "                y=correlation_matrix.columns,\n",
    "                color_continuous_scale='RdBu_r',\n",
    "                zmin=-1, zmax=1)\n",
    "\n",
    "# update the layout\n",
    "fig.update_layout(\n",
    "    title='Correlation Matrix',\n",
    "    width=2000,\n",
    "    height=2000\n",
    ")\n",
    "\n",
    "# display the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are all the new custom metric columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in custom_metric_columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in best_f1_columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in average_metric_columns:\n",
    "    print(col)\n",
    "for col in average_metric_idx_columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get unique trainer types\n",
    "unique_trainers = df['trainer_class'].unique()\n",
    "\n",
    "# create a dictionary mapping trainer types to marker shapes\n",
    "trainer_markers = dict(zip(unique_trainers, ['o', 's', '^', 'D']))\n",
    "\n",
    "def plot_custom_metric(color_column: str, idx_column_name: str):\n",
    "    # create the scatter plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    # create a normalize object for color scaling\n",
    "    # color_column = 'board_to_can_capture_queen_best_custom_metric'\n",
    "    # color_column = 'board_to_piece_state_best_custom_metric'\n",
    "    # color_column = 'board_to_has_legal_en_passant_best_custom_metric'\n",
    "    # color_column = 'board_to_pin_state_best_custom_metric'\n",
    "    # color_column = custom_metric_columns[6]\n",
    "    norm = Normalize(vmin=df[color_column].min(), vmax=df[color_column].max())\n",
    "\n",
    "    metric_1 = \"l0\"\n",
    "    metric_2 = \"frac_variance_explained\"\n",
    "    metric_2 = \"frac_recovered\"\n",
    "\n",
    "    idx = df[idx_column_name].values[0]\n",
    "\n",
    "    # plot data points for each trainer type separately\n",
    "    for trainer, marker in trainer_markers.items():\n",
    "        trainer_data = df[df['trainer_class'].str.contains(trainer)]\n",
    "        ax.scatter(trainer_data[metric_1], trainer_data[metric_2], c=trainer_data[color_column], cmap='viridis', marker=marker, s=100, label=trainer, norm=norm)\n",
    "\n",
    "    # add colorbar\n",
    "    cbar = fig.colorbar(ax.collections[0], ax=ax)\n",
    "    cbar.set_label(color_column)\n",
    "\n",
    "    # set labels and title\n",
    "    ax.set_xlabel(metric_1)\n",
    "    ax.set_ylabel(metric_2)\n",
    "    ax.set_title(f'{metric_1} vs. {metric_2} at threshold {idx} for {color_column}')\n",
    "\n",
    "    # addnd\n",
    "    ax.legend(title='Trainer Type', loc='upper right')\n",
    "\n",
    "    # # set x range\n",
    "    ax.set_xlim(0, 600)\n",
    "    ax.set_ylim(0.9825, 1.001)\n",
    "\n",
    "    # display the plot\n",
    "    plt.show()\n",
    "\n",
    "for i, column_name in enumerate(custom_metric_columns):\n",
    "    idx_column_name = custom_metric_idx_columns[i]\n",
    "    plot_custom_metric(column_name, idx_column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, column_name in enumerate(average_metric_columns):\n",
    "    idx_column_name = average_metric_idx_columns[i]\n",
    "    plot_custom_metric(column_name, idx_column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, column_name in enumerate(best_f1_columns):\n",
    "    idx_column_name = best_f1_idx_columns[i]\n",
    "    plot_custom_metric(column_name, idx_column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for func_name in custom_function_names:\n",
    "\n",
    "#     new_column_name = f\"{func_name}_best_custom_metric\"\n",
    "#     if new_column_name not in df.columns:\n",
    "#         df[new_column_name] = np.nan\n",
    "    \n",
    "#     second_column_name = f\"{func_name}_best_custom_metric_idx\"\n",
    "\n",
    "#     f1_counter_T = all_sae_results[func_name][\"f1_counter\"]\n",
    "#     best_idx = torch.argmax(f1_counter_T)\n",
    "\n",
    "#     for autoencoder_group_path in autoencoder_group_paths:\n",
    "#         folders = eval_sae.get_nested_folders(autoencoder_group_path)\n",
    "        \n",
    "#         for autoencoder_path in folders:\n",
    "#             f1_T = all_sae_results[autoencoder_group_path][autoencoder_path][func_name]\n",
    "#             best_f1 = f1_T[best_idx]\n",
    "#             df.loc[df[\"autoencoder_path\"] == autoencoder_path, new_column_name] = best_f1.item()\n",
    "#             df.loc[df[\"autoencoder_path\"] == autoencoder_path, second_column_name] = best_idx.item()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "circuits",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
