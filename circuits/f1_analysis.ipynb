{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import einops\n",
    "import importlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "import circuits.analysis as analysis\n",
    "import circuits.eval_sae_as_classifier as eval_sae\n",
    "import circuits.chess_utils as chess_utils\n",
    "import circuits.utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# # for testing purposes\n",
    "\n",
    "# # Define a sample 3D tensor with random values\n",
    "# # Dimensions: T x F x C (let's use 2 x 3 x 4 for simplicity)\n",
    "# f1_TFC = torch.randn(2, 3, 4)\n",
    "# print(\"Original Tensor (T x F x C):\")\n",
    "# print(f1_TFC)\n",
    "\n",
    "# def best_f1_average(f1_TFC: torch.Tensor) -> torch.Tensor:\n",
    "#     # Apply torch.max along the last dimension (dimension 2)\n",
    "#     # Select only the values, ignoring the indices\n",
    "#     f1_TF, _ = torch.max(f1_TFC, dim=1)\n",
    "#     return f1_TF\n",
    "\n",
    "# # Compute the maximum along the 'C' dimension and reduce to a 2D tensor\n",
    "# f1_TF = best_f1_average(f1_TFC)\n",
    "# print(\"\\nReduced Tensor (T x F) with max values from 'C':\")\n",
    "# print(f1_TF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_all_blanks(results: dict, device) -> dict:\n",
    "    custom_functions = analysis.get_all_custom_functions(results)\n",
    "    for function in custom_functions:\n",
    "        function_name = function.__name__\n",
    "\n",
    "        if function == chess_utils.board_to_piece_state or function == chess_utils.board_to_piece_color_state:\n",
    "            on_TFRRC = results[function_name]['on']\n",
    "            off_TFRRC = results[function_name]['off']\n",
    "            results[function_name]['on'] = analysis.mask_initial_board_state(on_TFRRC, function, device)\n",
    "            results[function_name]['off'] = analysis.mask_initial_board_state(off_TFRRC, function, device)\n",
    "\n",
    "    return results\n",
    "\n",
    "def best_f1_average(f1_TFRRC: torch.Tensor) -> torch.Tensor:\n",
    "    f1_TRRC, _ = torch.max(f1_TFRRC, dim=1)\n",
    "\n",
    "    T, R1, R2, C = f1_TRRC.shape\n",
    "\n",
    "    max_possible = R1 * R2 * C\n",
    "\n",
    "    f1_T = einops.reduce(f1_TRRC, 'T R1 R2 C -> T', 'sum') / max_possible\n",
    "\n",
    "    return f1_T\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(analysis)\n",
    "\n",
    "device = \"cuda\"\n",
    "autoencoder_group_paths = [\"../autoencoders/chess_layer5_large_sweep/\"]\n",
    "autoencoder_group_path = autoencoder_group_paths[0]\n",
    "folders = eval_sae.get_nested_folders(autoencoder_group_path)\n",
    "\n",
    "custom_functions = []\n",
    "custom_function_names = []\n",
    "\n",
    "sae_results = {}\n",
    "\n",
    "csv_results_file = \"results.csv\"\n",
    "df = pd.read_csv(autoencoder_group_path + csv_results_file)\n",
    "\n",
    "for autoencoder_path in folders:\n",
    "\n",
    "    print(f\"Processing {autoencoder_path}\")\n",
    "\n",
    "    assert autoencoder_path in df[\"autoencoder_path\"].values, f\"{autoencoder_path} not in csv file\"\n",
    "\n",
    "    sae_results[autoencoder_path] = {}\n",
    "\n",
    "\n",
    "    filter = \"1000\" # This is only necessary if you have multiple files with multiple n_inputs\n",
    "    # e.g. indexing_find_dots_indices_n_inputs_1000_results.pkl and indexing_find_dots_indices_n_inputs_5000_results.pkl\n",
    "    # In this case, if you want to view the results for n_inputs = 1000, you would set filter = \"1000\"\n",
    "    eval_filter = None\n",
    "\n",
    "    results_filenames = analysis.get_all_results_file_names(autoencoder_path, filter)\n",
    "    if len(results_filenames) > 1 or len(results_filenames) == 0:\n",
    "        print(f\"Skipping {autoencoder_path} because it has {len(results_filenames)} results files\")\n",
    "        print(\"This is most likely because there are results files from different n_inputs\")\n",
    "        continue\n",
    "    results_filename = results_filenames[0]\n",
    "\n",
    "    evals_filename = analysis.get_all_evals_file_names(autoencoder_path, eval_filter)\n",
    "\n",
    "    if len(evals_filename) > 1 or len(evals_filename) == 0:\n",
    "        print(f\"Skipping {autoencoder_path} because it has {len(evals_filename)} evals files\")\n",
    "        print(\"This is most likely because there are evals files from different n_inputs\")\n",
    "        continue\n",
    "\n",
    "    evals_filename = evals_filename[0]\n",
    "\n",
    "    with open(autoencoder_path + results_filename, \"rb\") as f:\n",
    "        results = pickle.load(f)\n",
    "\n",
    "    with open(autoencoder_path + evals_filename, \"rb\") as f:\n",
    "        evals = pickle.load(f)\n",
    "\n",
    "    results = utils.to_device(results, device)\n",
    "    evals = utils.to_device(evals, device)\n",
    "\n",
    "    custom_functions = analysis.get_all_custom_functions(results)\n",
    "    for function in custom_functions:\n",
    "        function_name = function.__name__\n",
    "        custom_function_names.append(function_name)\n",
    "    \n",
    "    results = analysis.add_off_tracker(results, custom_functions, device)\n",
    "    results = mask_all_blanks(results, device)\n",
    "    f1_dict_TFRRC = analysis.get_all_f1s(results, device)\n",
    "\n",
    "    sae_results[autoencoder_path][\"l0\"] = evals[\"eval_results\"][\"l0\"]\n",
    "    sae_results[autoencoder_path][\"frac_variance_explained\"] = evals[\"eval_results\"][\"frac_variance_explained\"]\n",
    "\n",
    "    for func_name in custom_function_names:\n",
    "        if func_name in sae_results:\n",
    "            continue\n",
    "\n",
    "        T = f1_dict_TFRRC[func_name].shape[0]\n",
    "        f1_counter_T = torch.zeros(T, device=device)\n",
    "        sae_results[func_name] = {\"f1_counter\": f1_counter_T}\n",
    "\n",
    "    for func_name in f1_dict_TFRRC:\n",
    "        config = chess_utils.config_lookup[func_name]\n",
    "        custom_function = config.custom_board_state_function\n",
    "        assert custom_function in custom_functions, f\"Key {custom_function} not in custom_functions\"\n",
    "        f1_TFRRC = f1_dict_TFRRC[func_name]\n",
    "        f1_T = best_f1_average(f1_TFRRC)\n",
    "        sae_results[func_name][\"f1_counter\"] += f1_T\n",
    "\n",
    "        sae_results[autoencoder_path][func_name] = f1_T\n",
    "\n",
    "    # torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for func_name in custom_function_names:\n",
    "\n",
    "    new_column_name = f\"{func_name}_best_custom_metric\"\n",
    "    if new_column_name not in df.columns:\n",
    "        df[new_column_name] = np.nan\n",
    "\n",
    "    f1_counter_T = sae_results[func_name][\"f1_counter\"]\n",
    "    best_idx = torch.argmax(f1_counter_T)\n",
    "\n",
    "    for autoencoder_path in folders:\n",
    "        f1_T = sae_results[autoencoder_path][func_name]\n",
    "        best_f1 = f1_T[best_idx]\n",
    "        df.loc[df[\"autoencoder_path\"] == autoencoder_path, new_column_name] = best_f1.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only the numerical columns\n",
    "numerical_columns = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "numerical_data = df[numerical_columns]\n",
    "\n",
    "# calculate the correlation matrix\n",
    "correlation_matrix = numerical_data.corr()\n",
    "\n",
    "# create a heatmap using plotly\n",
    "fig = px.imshow(correlation_matrix, \n",
    "                labels=dict(x=\"Columns\", y=\"Columns\", color=\"Correlation\"),\n",
    "                x=correlation_matrix.columns,\n",
    "                y=correlation_matrix.columns,\n",
    "                color_continuous_scale='RdBu_r',\n",
    "                zmin=-1, zmax=1)\n",
    "\n",
    "# update the layout\n",
    "fig.update_layout(\n",
    "    title='Correlation Matrix',\n",
    "    width=2000,\n",
    "    height=2000\n",
    ")\n",
    "\n",
    "# display the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get unique trainer types\n",
    "unique_trainers = df['trainer_class'].unique()\n",
    "\n",
    "# create a dictionary mapping trainer types to marker shapes\n",
    "trainer_markers = dict(zip(unique_trainers, ['o', 's', '^', 'D']))\n",
    "\n",
    "# create the scatter plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# create a normalize object for color scaling\n",
    "color_column = 'board_to_can_capture_queen_best_custom_metric'\n",
    "norm = Normalize(vmin=df[color_column].min(), vmax=df[color_column].max())\n",
    "\n",
    "metric_1 = \"l0\"\n",
    "metric_2 = \"frac_variance_explained\"\n",
    "\n",
    "# plot data points for each trainer type separately\n",
    "for trainer, marker in trainer_markers.items():\n",
    "    trainer_data = df[df['trainer_class'].str.contains(trainer)]\n",
    "    ax.scatter(trainer_data[metric_1], trainer_data[metric_2], c=trainer_data[color_column], cmap='viridis', marker=marker, s=100, label=trainer, norm=norm)\n",
    "\n",
    "# add colorbar\n",
    "cbar = fig.colorbar(ax.collections[0], ax=ax)\n",
    "cbar.set_label(color_column)\n",
    "\n",
    "# set labels and title\n",
    "ax.set_xlabel(metric_1)\n",
    "ax.set_ylabel(metric_2)\n",
    "ax.set_title(f'{metric_1} vs. {metric_2}')\n",
    "\n",
    "# addnd\n",
    "ax.legend(title='Trainer Type', loc='upper right')\n",
    "\n",
    "# # set x range\n",
    "ax.set_xlim(0, 1000)\n",
    "# ax.set_ylim(0.8, 1)\n",
    "\n",
    "# display the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "circuits",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
