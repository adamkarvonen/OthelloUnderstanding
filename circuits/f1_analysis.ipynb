{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import einops\n",
    "import importlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from matplotlib.colors import Normalize\n",
    "from typing import Callable\n",
    "import json\n",
    "\n",
    "import circuits.analysis as analysis\n",
    "import circuits.eval_sae_as_classifier as eval_sae\n",
    "import circuits.chess_utils as chess_utils\n",
    "import circuits.utils as utils\n",
    "import circuits.f1_analysis as f1_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've been trying to balance: Having a notebook that's easy to play around with, but also modularizing code into files, and not having duplicated functions between notebooks and python files that goes stale.\n",
    "\n",
    "Here's what I'm trying: All of the following functions in the next few cells are also in `f1_analysis.py`. By default, this notebook just loads a `f1_results.csv` from the autoencoder group path, automatically generated during `full_pipeline.ipynb`. If you want to play around with new analysis functions, feel free to uncomment the lines that call these functions, modify the below functions, and experiment to your heart's content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_all_blanks(results: dict, device) -> dict:\n",
    "    custom_functions = analysis.get_all_custom_functions(results)\n",
    "    for function in custom_functions:\n",
    "        function_name = function.__name__\n",
    "\n",
    "        if function == chess_utils.board_to_piece_state or function == chess_utils.board_to_piece_color_state:\n",
    "            on_TFRRC = results[function_name]['on']\n",
    "            off_TFRRC = results[function_name]['off']\n",
    "            results[function_name]['on'] = analysis.mask_initial_board_state(on_TFRRC, function, device)\n",
    "            results[function_name]['off'] = analysis.mask_initial_board_state(off_TFRRC, function, device)\n",
    "\n",
    "    return results\n",
    "\n",
    "def best_f1_average(f1_TFRRC: torch.Tensor, config: chess_utils.Config) -> torch.Tensor:\n",
    "    \"\"\"For every threshold, for every square, find the best F1 score across all features. Then average across all squares.\n",
    "    NOTE: If the function is binary, num_squares == 1. If it is board to piece state, num_squares == 8 * 8 * 12\"\"\"\n",
    "    f1_TRRC, _ = torch.max(f1_TFRRC, dim=1)\n",
    "\n",
    "    T, R1, R2, C = f1_TRRC.shape\n",
    "\n",
    "    if config.one_hot_mask_idx is not None:\n",
    "        C -= 1\n",
    "\n",
    "    max_possible = R1 * R2 * C\n",
    "\n",
    "    f1_T = einops.reduce(f1_TRRC, 'T R1 R2 C -> T', 'sum') / max_possible\n",
    "\n",
    "    return f1_T\n",
    "    \n",
    "\n",
    "def f1s_above_threshold(f1_TFRRC: torch.Tensor, threshold: float, config: chess_utils.Config) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"For every threshold, for every square, find the best F1 score across all features. Then, find the number of squares that have a F1 score above the threshold.\n",
    "    If the function is binary, num_squares == 1. If it is board to piece state, num_squares == 8 * 8 * 12\n",
    "    NOTE: This will probably be most useful for features with 8x8xn options.\"\"\"\n",
    "    f1_TRRC, _ = torch.max(f1_TFRRC, dim=1)\n",
    "\n",
    "    f1s_above_threshold_TRCC = f1_TRRC > threshold\n",
    "\n",
    "    T, R1, R2, C = f1_TRRC.shape\n",
    "    if config.one_hot_mask_idx is not None:\n",
    "        C -= 1\n",
    "\n",
    "    max_possible = R1 * R2 * C\n",
    "\n",
    "    f1_T = einops.reduce(f1s_above_threshold_TRCC, 'T R1 R2 C -> T', 'sum')\n",
    "\n",
    "    f1_T_normalized = f1_T / max_possible\n",
    "\n",
    "    return f1_T, f1_T_normalized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(analysis)\n",
    "\n",
    "\n",
    "def get_custom_functions(\n",
    "    autoencoder_group_path: str, results_filename_filter: str, device: str\n",
    ") -> list[Callable]:\n",
    "    folders = eval_sae.get_nested_folders(autoencoder_group_path)\n",
    "    first_autoencoder_path = folders[0]\n",
    "    results_filenames = analysis.get_all_results_file_names(\n",
    "        first_autoencoder_path, results_filename_filter\n",
    "    )\n",
    "\n",
    "    if len(results_filenames) > 1 or len(results_filenames) == 0:\n",
    "        raise ValueError(\"There are multiple results files\")\n",
    "    results_filename = results_filenames[0]\n",
    "\n",
    "    with open(first_autoencoder_path + results_filename, \"rb\") as f:\n",
    "        results = pickle.load(f)\n",
    "\n",
    "    results = utils.to_device(results, device)\n",
    "\n",
    "    custom_functions = analysis.get_all_custom_functions(results)\n",
    "    return custom_functions\n",
    "\n",
    "\n",
    "def get_custom_function_names(custom_functions: list[Callable]) -> list[str]:\n",
    "    custom_function_names = [function.__name__ for function in custom_functions]\n",
    "    return custom_function_names\n",
    "\n",
    "def get_threshold_column_names(func_name: str, threshold: float) -> tuple[str, str]:\n",
    "    return (f\"{func_name}_f1_threshold_{threshold}\", f\"{func_name}_f1_threshold_{threshold}_normalized\")\n",
    "\n",
    "\n",
    "def get_all_sae_f1_results(\n",
    "    autoencoder_group_paths: list[str],\n",
    "    df: pd.DataFrame,\n",
    "    results_filename_filter: str,\n",
    "    custom_functions: list[Callable],\n",
    "    custom_function_names: list[str],\n",
    "    device: str,\n",
    "    thresholds: list[float],\n",
    "    mask: bool,\n",
    ") -> dict:\n",
    "    all_sae_results = {}\n",
    "\n",
    "    for autoencoder_group_path in autoencoder_group_paths:\n",
    "\n",
    "        folders = eval_sae.get_nested_folders(autoencoder_group_path)\n",
    "        sae_results = {}\n",
    "\n",
    "        for autoencoder_path in folders:\n",
    "\n",
    "            print(f\"Processing {autoencoder_path}\")\n",
    "\n",
    "            assert (\n",
    "                autoencoder_path in df[\"autoencoder_path\"].values\n",
    "            ), f\"{autoencoder_path} not in csv file\"\n",
    "\n",
    "            sae_results[autoencoder_path] = {}\n",
    "\n",
    "            results_filenames = analysis.get_all_results_file_names(\n",
    "                autoencoder_path, results_filename_filter\n",
    "            )\n",
    "            if len(results_filenames) > 1 or len(results_filenames) == 0:\n",
    "                print(\n",
    "                    f\"Skipping {autoencoder_path} because it has {len(results_filenames)} results files\"\n",
    "                )\n",
    "                print(\"This is most likely because there are results files from different n_inputs\")\n",
    "                continue\n",
    "            results_filename = results_filenames[0]\n",
    "\n",
    "            with open(autoencoder_path + results_filename, \"rb\") as f:\n",
    "                results = pickle.load(f)\n",
    "\n",
    "            results = utils.to_device(results, device)\n",
    "\n",
    "            results = analysis.add_off_tracker(results, custom_functions, device)\n",
    "            f1_dict_TFRRC = analysis.get_all_f1s(results, device)\n",
    "\n",
    "            # feature_labels = analysis.analyze_results_dict(\n",
    "            #     results,\n",
    "            #     output_path=\"\",\n",
    "            #     device=device,\n",
    "            #     high_threshold=0.95,\n",
    "            #     low_threshold=0.1,\n",
    "            #     significance_threshold=10,\n",
    "            #     save_results=False,\n",
    "            #     mask=mask,\n",
    "            #     verbose=False,\n",
    "            #     print_results=False,\n",
    "            # )\n",
    "\n",
    "            correct_row = df[\"autoencoder_path\"] == autoencoder_path\n",
    "            sae_results[autoencoder_path][\"l0\"] = df[correct_row][\"l0\"].values[0]\n",
    "            sae_results[autoencoder_path][\"frac_variance_explained\"] = df[correct_row][\n",
    "                \"frac_variance_explained\"\n",
    "            ].values[0]\n",
    "\n",
    "            for func_name in f1_dict_TFRRC:\n",
    "                config = chess_utils.config_lookup[func_name]\n",
    "                custom_function = config.custom_board_state_function\n",
    "                assert (\n",
    "                    custom_function in custom_functions\n",
    "                ), f\"Key {custom_function} not in custom_functions\"\n",
    "                f1_TFRRC = f1_dict_TFRRC[func_name]\n",
    "\n",
    "                average_f1_T = best_f1_average(f1_TFRRC, config)\n",
    "                sae_results[autoencoder_path][f\"{func_name}_average_f1\"] = average_f1_T\n",
    "\n",
    "                for threshold in thresholds:\n",
    "                    threshold_column, threshold_column_normalized = get_threshold_column_names(\n",
    "                        func_name, threshold\n",
    "                    )\n",
    "                    f1_T, f1_T_normalized = f1s_above_threshold(f1_TFRRC, threshold, config)\n",
    "                    sae_results[autoencoder_path][threshold_column] = f1_T\n",
    "                    sae_results[autoencoder_path][threshold_column_normalized] = f1_T_normalized\n",
    "\n",
    "            # torch.cuda.empty_cache()\n",
    "        all_sae_results[autoencoder_group_path] = sae_results\n",
    "    return all_sae_results\n",
    "\n",
    "def update_dataframe_with_results(df, all_sae_results, custom_function_names, autoencoder_group_paths, thresholds):\n",
    "    assert df['autoencoder_path'].is_unique\n",
    "    updates = []\n",
    "    for autoencoder_group_path in autoencoder_group_paths:\n",
    "        folders = eval_sae.get_nested_folders(autoencoder_group_path)\n",
    "        for autoencoder_path in folders:\n",
    "            results = {'autoencoder_path': autoencoder_path}\n",
    "            for func_name in custom_function_names:\n",
    "        \n",
    "                f1_T = all_sae_results[autoencoder_group_path][autoencoder_path][f\"{func_name}_average_f1\"]\n",
    "                best_idx = torch.argmax(f1_T)\n",
    "                best_f1 = f1_T[best_idx]\n",
    "                \n",
    "                results[f\"{func_name}_best_average_f1\"] = best_f1.item()\n",
    "                results[f\"{func_name}_best_average_f1_idx\"] = best_idx.item()\n",
    "                results[f\"{func_name}_all_average_f1s\"] = json.dumps(f1_T.tolist())\n",
    "\n",
    "                for threshold in thresholds:\n",
    "                    threshold_column, threshold_normalized_column = get_threshold_column_names(func_name, threshold)\n",
    "                    f1_T = all_sae_results[autoencoder_group_path][autoencoder_path][threshold_column]\n",
    "                    f1_T_normalized = all_sae_results[autoencoder_group_path][autoencoder_path][threshold_normalized_column]\n",
    "                    best_idx = torch.argmax(f1_T)\n",
    "                    best_f1_at_threshold = f1_T[best_idx]\n",
    "                    best_f1_normalized = f1_T_normalized[best_idx]\n",
    "                    \n",
    "                    results[f\"{func_name}_f1_threshold_{threshold}_best\"] = best_f1_at_threshold.item()\n",
    "                    results[f\"{func_name}_f1_threshold_{threshold}_best_normalized\"] = best_f1_normalized.item()\n",
    "                    results[f\"{func_name}_f1_threshold_{threshold}_best_idx\"] = best_idx.item()\n",
    "                    results[f\"{func_name}_f1_threshold_{threshold}_best_normalized_idx\"] = best_idx.item()\n",
    "                    results[f\"{func_name}_f1_threshold_{threshold}_all\"] = json.dumps(f1_T.tolist())\n",
    "                    results[f\"{func_name}_f1_threshold_{threshold}_all_normalized\"] = json.dumps(f1_T_normalized.tolist())\n",
    "\n",
    "            \n",
    "            updates.append(results)\n",
    "\n",
    "    update_df = pd.DataFrame(updates)\n",
    "    df = pd.merge(df, update_df, on='autoencoder_path', how='outer')\n",
    "    assert df['autoencoder_path'].is_unique\n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set autoencoder group paths and thresholds here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "device = \"cpu\"\n",
    "# device = \"cuda\"\n",
    "mask = False\n",
    "thresholds = torch.arange(0.1, 1, 0.1)\n",
    "\n",
    "autoencoder_group_paths = [\"../autoencoders/chess_layer5_large_sweep/\"]\n",
    "autoencoder_group_paths = [\"../autoencoders/group-2024-05-14_chess/\"]\n",
    "autoencoder_group_paths = [\"../autoencoders/group-2024-05-14_chess/\"]\n",
    "# autoencoder_group_paths = [\"../autoencoders/chess_layer0/\"]\n",
    "\n",
    "csv_results_file = \"../autoencoders/chess_layer5_large_sweep/results.csv\"\n",
    "# csv_results_file = \"../autoencoders/chess_layer0/results.csv\"\n",
    "csv_results_file = \"../autoencoders/group-2024-05-14_chess/results.csv\"\n",
    "output_path = csv_results_file.replace(\".csv\", \"_f1_results.csv\")\n",
    "\n",
    "\n",
    "results_filename_filter = (\n",
    "    \"1000\"  # This is only necessary if you have multiple files with multiple n_inputs\n",
    ")\n",
    "# e.g. indexing_find_dots_indices_n_inputs_1000_results.pkl and indexing_find_dots_indices_n_inputs_5000_results.pkl\n",
    "# In this case, if you want to view the results for n_inputs = 1000, you would set filter = \"1000\"\n",
    "\n",
    "# custom_functions = get_custom_functions(autoencoder_group_paths[0], results_filename_filter, device)\n",
    "# custom_function_names = get_custom_function_names(custom_functions)\n",
    "# all_sae_results = get_all_sae_f1_results(autoencoder_group_paths, df, results_filename_filter, custom_functions, custom_function_names, device, thresholds, mask)\n",
    "\n",
    "# df = pd.read_csv(csv_results_file)\n",
    "# df = update_dataframe_with_results(df, all_sae_results, custom_function_names, autoencoder_group_paths, thresholds)\n",
    "# df.to_csv(output_path, index=False)\n",
    "\n",
    "# output_path = f1_analysis.complete_analysis_pipeline(autoencoder_group_paths, csv_results_file, results_filename_filter, device, thresholds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"../autoencoders/chess_layer0_subset/results_f1_results.csv\"\n",
    "df = pd.read_csv(output_path)\n",
    "def convert_json(x):\n",
    "    try:\n",
    "        return json.loads(x)  # Attempt to parse JSON\n",
    "    except (ValueError, TypeError):\n",
    "        return x  # Return the original value if it's not a JSON string\n",
    "\n",
    "def convert_dataframe(df):\n",
    "    # Apply the conversion function to each element in the DataFrame\n",
    "    return df.map(convert_json)\n",
    "df = convert_dataframe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_metric_columns = []\n",
    "custom_metric_idx_columns = []\n",
    "desired_threshold = 0.5\n",
    "for col in df.columns:\n",
    "    if f\"f1_threshold_{desired_threshold}_best_normalized\" in col and \"idx\" not in col:\n",
    "        custom_metric_columns.append(col)\n",
    "        print(col)\n",
    "    if f\"f1_threshold_{desired_threshold}_best_normalized_idx\" in col:\n",
    "        custom_metric_idx_columns.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_f1_columns = []\n",
    "best_f1_idx_columns = []\n",
    "for col in df.columns:\n",
    "    if \"best_f1_score_per_square\" in col:\n",
    "        best_f1_columns.append(col)\n",
    "        print(col)\n",
    "        f1_idx = col.replace(\"best_f1_score_per_square\", \"best_idx\")\n",
    "        best_f1_idx_columns.append(f1_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next 2 cells find the average f1 score and custom metric score for all functions, all 8x8 board state functions, and all binary functions, then store it in the df and `average_metric_columns` and `average_metric_idx_columns`. It's pretty verbose, but it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# board_state_8x8_columns = [\"board_to_piece_state\", \"board_to_piece_color_state\", \"board_to_threat_state\", \"board_to_legal_moves_state\", \"board_to_pseudo_legal_moves_state\"]\n",
    "\n",
    "# def get_board_state_columns(board_state_columns: list[str], columns: list[str], include: bool = True) -> list[str]:\n",
    "#     result_columns = []\n",
    "#     for col in columns:\n",
    "#         if any([board_state in col for board_state in board_state_columns]):\n",
    "#             if include:\n",
    "#                 result_columns.append(col)\n",
    "#         else:\n",
    "#             if not include:\n",
    "#                 result_columns.append(col)\n",
    "#     return result_columns\n",
    "\n",
    "# best_f1_board_state_columns = get_board_state_columns(board_state_8x8_columns, best_f1_columns, include=True)\n",
    "# best_f1_board_state_idx_columns = get_board_state_columns(board_state_8x8_columns, best_f1_idx_columns, include=True)\n",
    "\n",
    "# best_custom_metric_board_state_columns = get_board_state_columns(board_state_8x8_columns, custom_metric_columns, include=True)\n",
    "# best_custom_metric_board_state_idx_columns = get_board_state_columns(board_state_8x8_columns, custom_metric_idx_columns, include=True)\n",
    "\n",
    "# best_f1_binary_columns = get_board_state_columns(board_state_8x8_columns, best_f1_columns, include=False)\n",
    "# best_f1_binary_idx_columns = get_board_state_columns(board_state_8x8_columns, best_f1_idx_columns, include=False)\n",
    "\n",
    "# best_custom_metric_binary_columns = get_board_state_columns(board_state_8x8_columns, custom_metric_columns, include=False)\n",
    "# best_custom_metric_binary_idx_columns = get_board_state_columns(board_state_8x8_columns, custom_metric_idx_columns, include=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def add_average_metric_over_functions(\n",
    "#     df: pd.DataFrame,\n",
    "#     metric_type: str,\n",
    "#     average_metric_columns: list[str],\n",
    "#     average_metric_idx_columns: list[str],\n",
    "#     custom_metric_columns: list[str],\n",
    "#     custom_metric_idx_columns: list[str],\n",
    "# ) -> tuple[pd.DataFrame, list[str], list[str]]:\n",
    "\n",
    "#     average_metric_column = f\"{metric_type}_average\"\n",
    "#     average_metric_idx_column = f\"{metric_type}_average_idx\"\n",
    "\n",
    "#     average_metric_columns.append(average_metric_column)\n",
    "#     average_metric_idx_columns.append(average_metric_idx_column)\n",
    "\n",
    "#     df[average_metric_column] = np.nan\n",
    "#     df[average_metric_idx_column] = np.nan\n",
    "\n",
    "#     df[average_metric_column] = df[custom_metric_columns].mean(axis=1)\n",
    "#     df[average_metric_idx_column] = df[custom_metric_idx_columns].mean(axis=1)\n",
    "\n",
    "#     return df, average_metric_columns, average_metric_idx_columns\n",
    "\n",
    "\n",
    "# average_metric_columns = []\n",
    "# average_metric_idx_columns = []\n",
    "\n",
    "# df, average_metric_columns, average_metric_idx_columns = add_average_metric_over_functions(\n",
    "#     df,\n",
    "#     \"best_f1_score_per_square\",\n",
    "#     average_metric_columns,\n",
    "#     average_metric_idx_columns,\n",
    "#     best_f1_board_state_columns,\n",
    "#     best_f1_board_state_idx_columns,\n",
    "# )\n",
    "# df, average_metric_columns, average_metric_idx_columns = add_average_metric_over_functions(\n",
    "#     df,\n",
    "#     \"best_custom_metric\",\n",
    "#     average_metric_columns,\n",
    "#     average_metric_idx_columns,\n",
    "#     best_custom_metric_board_state_columns,\n",
    "#     best_custom_metric_board_state_idx_columns,\n",
    "# )\n",
    "# df, average_metric_columns, average_metric_idx_columns = add_average_metric_over_functions(\n",
    "#     df,\n",
    "#     \"best_f1_score_per_square_only_board_state\",\n",
    "#     average_metric_columns,\n",
    "#     average_metric_idx_columns,\n",
    "#     best_f1_board_state_columns,\n",
    "#     best_f1_board_state_idx_columns,\n",
    "# )\n",
    "# df, average_metric_columns, average_metric_idx_columns = add_average_metric_over_functions(\n",
    "#     df,\n",
    "#     \"best_custom_metric_only_board_state\",\n",
    "#     average_metric_columns,\n",
    "#     average_metric_idx_columns,\n",
    "#     best_custom_metric_board_state_columns,\n",
    "#     best_custom_metric_board_state_idx_columns,\n",
    "# )\n",
    "# df, average_metric_columns, average_metric_idx_columns = add_average_metric_over_functions(\n",
    "#     df,\n",
    "#     \"best_f1_score_per_square_only_binary\",\n",
    "#     average_metric_columns,\n",
    "#     average_metric_idx_columns,\n",
    "#     best_f1_binary_columns,\n",
    "#     best_f1_binary_idx_columns,\n",
    "# )\n",
    "# df, average_metric_columns, average_metric_idx_columns = add_average_metric_over_functions(\n",
    "#     df,\n",
    "#     \"best_custom_metric_only_binary\",\n",
    "#     average_metric_columns,\n",
    "#     average_metric_idx_columns,\n",
    "#     best_custom_metric_binary_columns,\n",
    "#     best_custom_metric_binary_idx_columns,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df\n",
    "# df.to_csv(\"processed_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # select only the numerical columns\n",
    "# numerical_columns = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "# numerical_data = df[numerical_columns]\n",
    "\n",
    "# # calculate the correlation matrix\n",
    "# correlation_matrix = numerical_data.corr()\n",
    "\n",
    "# # create a heatmap using plotly\n",
    "# fig = px.imshow(correlation_matrix, \n",
    "#                 labels=dict(x=\"Columns\", y=\"Columns\", color=\"Correlation\"),\n",
    "#                 x=correlation_matrix.columns,\n",
    "#                 y=correlation_matrix.columns,\n",
    "#                 color_continuous_scale='RdBu_r',\n",
    "#                 zmin=-1, zmax=1)\n",
    "\n",
    "# # update the layout\n",
    "# fig.update_layout(\n",
    "#     title='Correlation Matrix',\n",
    "#     width=2000,\n",
    "#     height=2000\n",
    "# )\n",
    "\n",
    "# # display the plot\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are all the new custom metric columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in custom_metric_columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in best_f1_columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in average_metric_columns:\n",
    "#     print(col)\n",
    "# for col in average_metric_idx_columns:\n",
    "#     print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get unique trainer types\n",
    "unique_trainers = df['trainer_class'].unique()\n",
    "\n",
    "# create a dictionary mapping trainer types to marker shapes\n",
    "trainer_markers = dict(zip(unique_trainers, ['o', 's', '^', 'D']))\n",
    "\n",
    "def plot_custom_metric(color_column: str, idx_column_name: str):\n",
    "    # create the scatter plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    # create a normalize object for color scaling\n",
    "    # color_column = 'board_to_can_capture_queen_best_custom_metric'\n",
    "    # color_column = 'board_to_piece_state_best_custom_metric'\n",
    "    # color_column = 'board_to_has_legal_en_passant_best_custom_metric'\n",
    "    # color_column = 'board_to_pin_state_best_custom_metric'\n",
    "    # color_column = custom_metric_columns[6]\n",
    "    norm = Normalize(vmin=df[color_column].min(), vmax=df[color_column].max())\n",
    "\n",
    "    metric_1 = \"l0\"\n",
    "    metric_2 = \"frac_recovered\"\n",
    "\n",
    "    idx = df[idx_column_name].values[0]\n",
    "\n",
    "    # plot data points for each trainer type separately\n",
    "    for trainer, marker in trainer_markers.items():\n",
    "        trainer_data = df[df['trainer_class'].str.contains(trainer)]\n",
    "        ax.scatter(trainer_data[metric_1], trainer_data[metric_2], c=trainer_data[color_column], cmap='viridis', marker=marker, s=100, label=trainer, norm=norm)\n",
    "\n",
    "    # add colorbar\n",
    "    cbar = fig.colorbar(ax.collections[0], ax=ax)\n",
    "    cbar.set_label(color_column)\n",
    "\n",
    "    # set labels and title\n",
    "    ax.set_xlabel(metric_1)\n",
    "    ax.set_ylabel(metric_2)\n",
    "    ax.set_title(f'{metric_1} vs. {metric_2} at threshold {idx} for {color_column}')\n",
    "\n",
    "    # addnd\n",
    "    ax.legend(title='Trainer Type', loc='upper right')\n",
    "\n",
    "    # # set x range\n",
    "    # ax.set_xlim(0, 600)\n",
    "    # ax.set_ylim(0.0, 1.001)\n",
    "\n",
    "    # display the plot\n",
    "    plt.show()\n",
    "\n",
    "# column_name = \"board_to_piece_state_best_f1_score_per_square\"\n",
    "# idx_column_name = \"board_to_piece_state_best_idx\"\n",
    "# plot_custom_metric(column_name, idx_column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, column_name in enumerate(custom_metric_columns):\n",
    "    idx_column_name = custom_metric_idx_columns[i]\n",
    "    plot_custom_metric(column_name, idx_column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, column_name in enumerate(average_metric_columns):\n",
    "#     idx_column_name = average_metric_idx_columns[i]\n",
    "#     plot_custom_metric(column_name, idx_column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, column_name in enumerate(best_f1_columns):\n",
    "    idx_column_name = best_f1_idx_columns[i]\n",
    "    plot_custom_metric(column_name, idx_column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for func_name in custom_function_names:\n",
    "\n",
    "#     new_column_name = f\"{func_name}_best_custom_metric\"\n",
    "#     if new_column_name not in df.columns:\n",
    "#         df[new_column_name] = np.nan\n",
    "    \n",
    "#     second_column_name = f\"{func_name}_best_custom_metric_idx\"\n",
    "\n",
    "#     f1_counter_T = all_sae_results[func_name][\"f1_counter\"]\n",
    "#     best_idx = torch.argmax(f1_counter_T)\n",
    "\n",
    "#     for autoencoder_group_path in autoencoder_group_paths:\n",
    "#         folders = eval_sae.get_nested_folders(autoencoder_group_path)\n",
    "        \n",
    "#         for autoencoder_path in folders:\n",
    "#             f1_T = all_sae_results[autoencoder_group_path][autoencoder_path][func_name]\n",
    "#             best_f1 = f1_T[best_idx]\n",
    "#             df.loc[df[\"autoencoder_path\"] == autoencoder_path, new_column_name] = best_f1.item()\n",
    "#             df.loc[df[\"autoencoder_path\"] == autoencoder_path, second_column_name] = best_idx.item()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "circuits",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
