{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import einops\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.tree import export_text\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "import sklearn\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import concurrent.futures\n",
    "import multiprocessing\n",
    "from typing import Callable, Optional\n",
    "import os\n",
    "import importlib\n",
    "import pickle\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "\n",
    "import circuits.utils as utils\n",
    "import circuits.othello_utils as othello_utils\n",
    "from circuits.eval_sae_as_classifier import construct_othello_dataset\n",
    "\n",
    "# Setup\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = \"cpu\"\n",
    "torch.set_grad_enabled(False)\n",
    "tracer_kwargs = {'validate' : False, 'scan' : False}\n",
    "\n",
    "# Model and data loading\n",
    "def load_model_and_data(model_name: str, dataset_size: int, custom_functions: list[Callable]):\n",
    "    model = utils.get_model(model_name, device)\n",
    "    data = construct_othello_dataset(\n",
    "        custom_functions=custom_functions,\n",
    "        n_inputs=dataset_size,\n",
    "        split=\"train\",\n",
    "        device=device,\n",
    "    )\n",
    "    return model, data\n",
    "\n",
    "# Cache Neuron Activations\n",
    "def cache_neuron_activations(model, data: dict, layers: list, batch_size: int, n_batches: int) -> dict:\n",
    "    \"\"\"Deprecated in favor of using identity autoencoders\"\"\"\n",
    "    neuron_acts = defaultdict(list)\n",
    "\n",
    "    for batch_idx in range(n_batches):\n",
    "        batch_start = batch_idx * batch_size\n",
    "        batch_end = (batch_idx + 1) * batch_size\n",
    "        data_batch = data[\"encoded_inputs\"][batch_start:batch_end]\n",
    "        data_batch = torch.tensor(data_batch, device=device)\n",
    "\n",
    "        with torch.no_grad(), model.trace(data_batch, scan=False, validate=False):\n",
    "            for layer in layers:\n",
    "                neuron_activations_BLD = model.blocks[layer].mlp.hook_post.output.save()\n",
    "                neuron_acts[layer].append(neuron_activations_BLD)\n",
    "\n",
    "    for layer in neuron_acts:\n",
    "        neuron_acts[layer] = torch.stack(neuron_acts[layer])\n",
    "        neuron_acts[layer] = einops.rearrange(neuron_acts[layer], \"n b l c -> (n b) l c\")\n",
    "    \n",
    "    return neuron_acts\n",
    "\n",
    "def get_submodule_dict(model, model_name: str, layers: list, input_location: str) -> dict:\n",
    "    submodule_dict = {}\n",
    "\n",
    "    for layer in layers:\n",
    "        if input_location == \"sae_feature\":\n",
    "            submodule = utils.get_resid_post_submodule(model_name, layer, model)\n",
    "        elif input_location == \"sae_mlp_feature\":\n",
    "            submodule = utils.get_mlp_activations_submodule(model_name, layer, model)\n",
    "        elif input_location == \"mlp_neuron\":\n",
    "            submodule = utils.get_mlp_activations_submodule(model_name, layer, model)\n",
    "        elif input_location == \"attention_out\":\n",
    "            submodule = model.blocks[layer].hook_attn_out\n",
    "        elif input_location == \"mlp_out\":\n",
    "            submodule = model.blocks[layer].hook_mlp_out\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid input location: {input_location}\")\n",
    "        submodule_dict[layer] = submodule\n",
    "\n",
    "    return submodule_dict\n",
    "\n",
    "def cache_sae_activations(model, model_name: str, data: dict, layers: list, batch_size: int, n_batches: int, repo_dir: str, input_location: str) -> dict:\n",
    "    sae_acts = defaultdict(list)\n",
    "\n",
    "    ae_dict = utils.get_aes(node_type=input_location, repo_dir = repo_dir)\n",
    "    submodule_dict = get_submodule_dict(model, model_name, layers, input_location)\n",
    "\n",
    "    for batch_idx in range(n_batches):\n",
    "        batch_start = batch_idx * batch_size\n",
    "        batch_end = (batch_idx + 1) * batch_size\n",
    "        data_batch = data[\"encoded_inputs\"][batch_start:batch_end]\n",
    "        data_batch = torch.tensor(data_batch, device=device)\n",
    "\n",
    "        with torch.no_grad(), model.trace(data_batch, scan=False, validate=False):\n",
    "            for layer in layers:\n",
    "                ae = ae_dict[layer]\n",
    "                submodule = submodule_dict[layer]\n",
    "                x = submodule.output\n",
    "                f = ae.encode(x)\n",
    "                sae_acts[layer].append(f.save())\n",
    "                \n",
    "    for layer in sae_acts:\n",
    "        sae_acts[layer] = torch.stack(sae_acts[layer])\n",
    "        sae_acts[layer] = einops.rearrange(sae_acts[layer], \"n b l c -> (n b) l c\")\n",
    "\n",
    "    return sae_acts\n",
    "\n",
    "def get_max_activations(neuron_acts: dict, layer: int) -> torch.Tensor:\n",
    "    D = neuron_acts[layer].shape[-1]\n",
    "    max_activations_D = torch.full((D,), float(\"-inf\"), device=device)\n",
    "\n",
    "    neuron_acts_BLD = neuron_acts[layer]\n",
    "    neuron_acts_BD = einops.rearrange(neuron_acts_BLD, \"b l d -> (b l) d\")\n",
    "\n",
    "    max_activations_D = torch.max(max_activations_D, neuron_acts_BD.max(dim=0).values)\n",
    "    return max_activations_D\n",
    "\n",
    "def calculate_binary_activations(neuron_acts: dict, threshold: float=0.1):\n",
    "    binary_acts = {}\n",
    "\n",
    "    for layer in neuron_acts:\n",
    "        max_activations_D = get_max_activations(neuron_acts, layer)\n",
    "\n",
    "        binary_acts[layer] = (neuron_acts[layer] > (threshold * max_activations_D)).int()\n",
    "    return binary_acts\n",
    "\n",
    "# Prepare data for modeling\n",
    "def prepare_data(games_BLC: torch.Tensor, mlp_acts_BLD: torch.Tensor):\n",
    "    X = einops.rearrange(games_BLC, \"b l c -> (b l) c\").cpu().numpy()\n",
    "    y = einops.rearrange(mlp_acts_BLD, \"b l d -> (b l) d\").cpu().numpy()\n",
    "    return train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train and evaluate models\n",
    "def train_and_evaluate(model, X_train, X_test, y_train, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    return model, mse, r2\n",
    "\n",
    "def train_and_evaluate_xgb(X_train, X_test, y_train, y_test, is_binary=False):\n",
    "    if is_binary:\n",
    "        model = MultiOutputClassifier(XGBClassifier(\n",
    "            n_estimators=50,  # limit number of trees\n",
    "            max_depth=6,      # limit depth\n",
    "            learning_rate=0.1,\n",
    "            random_state=42\n",
    "        ))\n",
    "    else:\n",
    "        model = MultiOutputRegressor(XGBRegressor(\n",
    "            n_estimators=50,\n",
    "            max_depth=6,\n",
    "            learning_rate=0.1,\n",
    "            random_state=42\n",
    "        ))\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    if is_binary:\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "        return model, accuracy, f1\n",
    "    else:\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        return model, mse, r2\n",
    "\n",
    "def calculate_neuron_metrics(model, X, y):\n",
    "    y_pred = model.predict(X)\n",
    "    \n",
    "    # Calculate MSE for all neurons at once\n",
    "    mse_list = np.mean((y - y_pred)**2, axis=0)\n",
    "    \n",
    "    # Calculate R2 for all neurons at once\n",
    "    ss_res = np.sum((y - y_pred)**2, axis=0)\n",
    "    ss_tot = np.sum((y - np.mean(y, axis=0))**2, axis=0)\n",
    "    r2_list = 1 - (ss_res / ss_tot)\n",
    "    \n",
    "    return mse_list, r2_list\n",
    "\n",
    "def calculate_binary_metrics(model, X, y):\n",
    "    y_pred = model.predict(X)\n",
    "    \n",
    "    # Compute true positives, false positives, true negatives, false negatives\n",
    "    tp = np.sum((y_pred == 1) & (y == 1), axis=0)\n",
    "    fp = np.sum((y_pred == 1) & (y == 0), axis=0)\n",
    "    tn = np.sum((y_pred == 0) & (y == 0), axis=0)\n",
    "    fn = np.sum((y_pred == 0) & (y == 1), axis=0)\n",
    "    \n",
    "    # Compute metrics\n",
    "    accuracy = (tp + tn) / (tp + fp + tn + fn)\n",
    "    precision = np.divide(tp, tp + fp, out=np.zeros_like(tp, dtype=float), where=(tp + fp) != 0)\n",
    "    recall = np.divide(tp, tp + fn, out=np.zeros_like(tp, dtype=float), where=(tp + fn) != 0)\n",
    "    \n",
    "    # Compute F1 score\n",
    "    f1 = np.divide(2 * precision * recall, precision + recall, \n",
    "                   out=np.zeros_like(precision, dtype=float), \n",
    "                   where=(precision + recall) != 0)\n",
    "    \n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "# Print decision tree rules\n",
    "def print_decision_tree_rules(model, feature_names, neuron_index, max_depth=None):\n",
    "    tree = model.estimators_[neuron_index]\n",
    "    if feature_names is None:\n",
    "        feature_names = [f\"feature_{i}\" for i in range(tree.n_features_in_)]\n",
    "    tree_rules = export_text(tree, feature_names=feature_names, max_depth=max_depth)\n",
    "    print(f\"Decision Tree Rules for Neuron {neuron_index}:\")\n",
    "    print(tree_rules)\n",
    "\n",
    "def rc_to_square_notation(row, col):\n",
    "    letters = \"ABCDEFGH\"\n",
    "    letter = letters[row]\n",
    "    number = 8 - col\n",
    "    # letter = letters[col]\n",
    "    return f\"{letter}{number}\"\n",
    "\n",
    "def idx_to_square_notation(idx):\n",
    "    row = idx // 8\n",
    "    col = idx % 8\n",
    "    square = rc_to_square_notation(row, col)\n",
    "    return square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_DEPTH = 8\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "def process_layer(\n",
    "    layer: int,\n",
    "    games_BLC: torch.Tensor,\n",
    "    neuron_acts: dict,\n",
    "    binary_acts: dict,\n",
    "    linear_reg: bool = False,\n",
    "    regular_dt: bool = True,\n",
    "    binary_dt: bool = True,\n",
    ") -> dict:\n",
    "\n",
    "    print(f\"\\nLayer {layer}\")\n",
    "\n",
    "    if regular_dt:\n",
    "        X_train, X_test, y_train, y_test = prepare_data(games_BLC, neuron_acts[layer])\n",
    "\n",
    "        # Decision Tree\n",
    "        dt_model, dt_mse, dt_r2 = train_and_evaluate(\n",
    "            MultiOutputRegressor(\n",
    "                DecisionTreeRegressor(\n",
    "                    random_state=RANDOM_STATE, max_depth=MAX_DEPTH, #min_samples_leaf=5, min_samples_split=5\n",
    "                )\n",
    "            ),\n",
    "            X_train,\n",
    "            X_test,\n",
    "            y_train,\n",
    "            y_test,\n",
    "        )\n",
    "\n",
    "        dt_mse, dt_r2 = calculate_neuron_metrics(dt_model, X_test, y_test)\n",
    "\n",
    "    if binary_dt:\n",
    "        # Binary Decision Tree\n",
    "        X_binary_train, X_binary_test, y_binary_train, y_binary_test = prepare_data(\n",
    "            games_BLC, binary_acts[layer]\n",
    "        )\n",
    "        dt_binary_model = MultiOutputClassifier(\n",
    "            DecisionTreeClassifier(\n",
    "                random_state=RANDOM_STATE, max_depth=MAX_DEPTH, #min_samples_leaf=5, min_samples_split=5\n",
    "            )\n",
    "        )\n",
    "        dt_binary_model.fit(X_binary_train, y_binary_train)\n",
    "\n",
    "        accuracy, precision, recall, f1 = calculate_binary_metrics(dt_binary_model, X_binary_test, y_binary_test)\n",
    "\n",
    "    layer_results = {\n",
    "        \"layer\": layer,\n",
    "        \"regular_dt\": {\"model\": dt_model, \"mse\": dt_mse, \"r2\": dt_r2},\n",
    "        \"binary_dt\": {\n",
    "            \"model\": dt_binary_model,\n",
    "            \"accuracy\": accuracy,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    if linear_reg:\n",
    "        lasso_model, lasso_mse, lasso_r2 = train_and_evaluate(\n",
    "            Lasso(alpha=0.005), X_train, X_test, y_train, y_test\n",
    "        )\n",
    "        layer_results[\"lasso\"] = {\"model\": lasso_model, \"mse\": lasso_mse, \"r2\": lasso_r2}\n",
    "\n",
    "    print(f\"Finished Layer {layer}\")\n",
    "\n",
    "    return layer_results\n",
    "\n",
    "def process_layer_xgb(\n",
    "    layer: int,\n",
    "    games_BLC: torch.Tensor,\n",
    "    neuron_acts: dict,\n",
    "    binary_acts: dict,\n",
    "    linear_reg: bool = False,\n",
    ") -> dict:\n",
    "    \n",
    "\n",
    "    print(f\"\\nLayer {layer}\")\n",
    "    X_train, X_test, y_train, y_test = prepare_data(games_BLC, neuron_acts[layer])\n",
    "\n",
    "    # Decision Tree\n",
    "    xgb_model, xgb_mse, xgb_r2 = train_and_evaluate_xgb(X_train, X_test, y_train, y_test)\n",
    "\n",
    "    # Binary Decision Tree\n",
    "    X_binary_train, X_binary_test, y_binary_train, y_binary_test = prepare_data(\n",
    "        games_BLC, binary_acts[layer]\n",
    "    )\n",
    "    xgb_binary_model, xgb_accuracy, xgb_f1 = train_and_evaluate_xgb(\n",
    "        X_binary_train, X_binary_test, y_binary_train, y_binary_test, is_binary=True\n",
    "    )\n",
    "\n",
    "    layer_results = {\n",
    "        \"layer\": layer,\n",
    "        \"regular_dt\": {\"model\": xgb_model, \"mse\": xgb_mse, \"r2\": xgb_r2},\n",
    "        \"binary_dt\": {\n",
    "            \"model\": xgb_binary_model,\n",
    "            \"accuracy\": xgb_accuracy,\n",
    "            \"f1\": xgb_f1,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    return layer_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interventions(\n",
    "    model,\n",
    "    model_name: str,\n",
    "    train_data: dict,\n",
    "    selected_features: dict[int, torch.Tensor],\n",
    "    ae_dict: dict,\n",
    "    submodule_dict: dict,\n",
    "    layers: list[int],\n",
    "    ablation_method: str=\"zero\",\n",
    "    decision_trees: Optional[dict]=None,\n",
    "    custom_function: Optional[Callable]=None,\n",
    "    ablate_not_selected: bool = False,\n",
    "    add_error: bool = False,\n",
    "):\n",
    "    allowed_methods = [\"mean\", \"zero\", \"max\", \"dt\"]\n",
    "    assert ablation_method in allowed_methods, f\"Invalid ablation method. Must be one of {allowed_methods}\"\n",
    "    game_batch_BL = torch.tensor(train_data[\"encoded_inputs\"])\n",
    "\n",
    "    simulated_activations = {}\n",
    "    mean_activations = {}\n",
    "\n",
    "    if ablation_method == \"dt\":\n",
    "        board_state_BLC = train_data[custom_function.__name__]\n",
    "        B, L, C = board_state_BLC.shape\n",
    "        X = einops.rearrange(board_state_BLC, \"b l c -> (b l) c\").cpu().numpy()\n",
    "\n",
    "        for layer in layers:\n",
    "            decision_tree = decision_trees[layer][custom_function.__name__][\"decision_tree\"][\"model\"]\n",
    "            simulated_activations_BF = decision_tree.predict(X)\n",
    "            simulated_activations_BF = torch.tensor(simulated_activations_BF, device=device, dtype=torch.float32)\n",
    "            simulated_activations_BLF = einops.rearrange(simulated_activations_BF, \"(b l) f -> b l f\", b=B, l=L)\n",
    "            simulated_activations[layer] = simulated_activations_BLF\n",
    "\n",
    "    # Get clean logits and mean submodule activations\n",
    "    with torch.no_grad(), model.trace(game_batch_BL, **tracer_kwargs):\n",
    "        for layer in layers:\n",
    "            submodule = submodule_dict[layer]\n",
    "            ae = ae_dict[layer]\n",
    "            original_BLD = submodule.output\n",
    "\n",
    "            encoded_BLF = ae.encode(original_BLD)\n",
    "\n",
    "            if ablation_method == \"mean\":\n",
    "                mean_activations[layer] = encoded_BLF.mean(dim=(0, 1)).save()\n",
    "            elif ablation_method == \"max\":\n",
    "                max_activations = encoded_BLF.max(dim=0).values\n",
    "                mean_activations[layer] = max_activations.max(dim=0).values.save()\n",
    "        \n",
    "        logits_clean_BLV = model.unembed.output.save()\n",
    "\n",
    "    # Get patch logits\n",
    "    with torch.no_grad(), model.trace(game_batch_BL, **tracer_kwargs):\n",
    "        for layer in layers:\n",
    "\n",
    "            submodule = submodule_dict[layer]\n",
    "            ae = ae_dict[layer]\n",
    "            original_BLD = submodule.output\n",
    "\n",
    "            encoded_BLF = ae.encode(original_BLD)\n",
    "            feat_idxs = selected_features[layer]\n",
    "\n",
    "            decoded_BLD = ae.decode(encoded_BLF)\n",
    "            error_BLD = original_BLD - decoded_BLD\n",
    "\n",
    "            if ablation_method == \"mean\":\n",
    "                encoded_BLF[:, :, feat_idxs] = mean_activations[layer][feat_idxs]\n",
    "            elif ablation_method == \"max\":\n",
    "                encoded_BLF[:, :, feat_idxs] = mean_activations[layer][feat_idxs]\n",
    "            elif ablation_method == \"dt\":\n",
    "                encoded_BLF[:, :, feat_idxs] = simulated_activations[layer][:, :, feat_idxs]\n",
    "                if ablate_not_selected:\n",
    "                    not_feature_idxs_F = ~feat_idxs\n",
    "                    encoded_BLF[:, :, not_feature_idxs_F] = 0\n",
    "            else:\n",
    "                encoded_BLF[:, :, feat_idxs] = 0\n",
    "            \n",
    "            modified_decoded_BLD = ae.decode(encoded_BLF)\n",
    "\n",
    "            submodule.output = modified_decoded_BLD \n",
    "            if add_error:\n",
    "                submodule.output += error_BLD\n",
    "\n",
    "        logits_patch_BLV = model.unembed.output.save()\n",
    "\n",
    "    return logits_clean_BLV, logits_patch_BLV\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_output_folders():\n",
    "    os.makedirs(\"decision_trees\", exist_ok=True)\n",
    "    os.makedirs(\"images\", exist_ok=True)\n",
    "add_output_folders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ablations = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(utils)\n",
    "\n",
    "repo_dir = \"/home/adam/OthelloUnderstanding\"\n",
    "model_name = \"Baidicoot/Othello-GPT-Transformer-Lens\"\n",
    "input_location = \"sae_feature\"\n",
    "# input_location = \"mlp_neuron\"\n",
    "# input_location = \"attention_out\"\n",
    "# input_location = \"mlp_out\"\n",
    "batch_size = 10\n",
    "n_batches = 10\n",
    "dataset_size = batch_size * n_batches\n",
    "layers = list(range(8))\n",
    "\n",
    "ablate_not_selected = True\n",
    "add_error = False\n",
    "\n",
    "custom_functions = [\n",
    "    othello_utils.games_batch_to_input_tokens_flipped_bs_classifier_input_BLC,\n",
    "    # othello_utils.games_batch_to_input_tokens_classifier_input_BLC,\n",
    "    # othello_utils.games_batch_to_board_state_and_input_tokens_classifier_input_BLC,\n",
    "    # othello_utils.games_batch_to_input_tokens_flipped_classifier_input_BLC,\n",
    "    # othello_utils.games_batch_to_board_state_classifier_input_BLC,\n",
    "    # othello_utils.games_batch_to_input_tokens_parity_classifier_input_BLC,\n",
    "]\n",
    "\n",
    "model, data = load_model_and_data(model_name, dataset_size, custom_functions)\n",
    "\n",
    "neuron_acts = cache_sae_activations(model, model_name, data, layers, batch_size, n_batches, repo_dir, input_location)\n",
    "\n",
    "binary_acts = calculate_binary_activations(neuron_acts)\n",
    "\n",
    "\n",
    "neuron_acts = utils.to_device(neuron_acts, \"cpu\")\n",
    "binary_acts = utils.to_device(binary_acts, \"cpu\")\n",
    "\n",
    "ae_dict = utils.get_aes(node_type=input_location, repo_dir=repo_dir)\n",
    "submodule_dict = get_submodule_dict(model, model_name, layers, input_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for layer in layers:\n",
    "#     max_activations_D = get_max_activations(neuron_acts, layer)\n",
    "#     plt.hist(max_activations_D.cpu().numpy(), bins=50)\n",
    "#     plt.xlim(0, 15)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Use all available cores, but max out at 10\n",
    "num_cores = min(8, multiprocessing.cpu_count())\n",
    "\n",
    "results = {}\n",
    "\n",
    "for layer in layers:\n",
    "    results[layer] = {}\n",
    "\n",
    "for custom_function in custom_functions:\n",
    "\n",
    "    print(f\"\\n{custom_function.__name__}\")\n",
    "    games_BLC = data[custom_function.__name__]\n",
    "    games_BLC = utils.to_device(games_BLC, \"cpu\")\n",
    "\n",
    "    layer_results = Parallel(n_jobs=num_cores)(\n",
    "        delayed(process_layer)(layer, games_BLC, neuron_acts, binary_acts)\n",
    "        for layer in layers\n",
    "    )\n",
    "\n",
    "    for layer_result in layer_results:\n",
    "        if layer_result is not None:\n",
    "            layer = layer_result['layer']\n",
    "            results[layer][custom_function.__name__] = {\n",
    "                'decision_tree': layer_result['regular_dt'],\n",
    "                'binary_decision_tree': layer_result['binary_dt']\n",
    "            }\n",
    "\n",
    "    # with ProcessPoolExecutor(max_workers=num_cores) as executor:\n",
    "    #     future_to_layer = {executor.submit(process_layer, layer, games_BLC, neuron_acts, binary_acts): layer for layer in layers}\n",
    "    #     for future in concurrent.futures.as_completed(future_to_layer):\n",
    "    #         layer_result = future.result()\n",
    "    #         layer = layer_result['layer']\n",
    "    #         results[layer][custom_function.__name__] = {\n",
    "    #             'decision_tree': layer_result['regular_dt'],\n",
    "    #             'binary_decision_tree': layer_result['binary_dt']\n",
    "    #         }\n",
    "\n",
    "# Print or process results\n",
    "# for layer, layer_results in results.items():\n",
    "#     print(f\"\\nLayer {layer} Results:\")\n",
    "#     print(f\"Regular Decision Tree - MSE: {layer_results['decision_tree']['mse']:.4f}, R2: {layer_results['decision_tree']['r2']:.4f}\")\n",
    "#     print(f\"Binary Decision Tree - Accuracy: {layer_results['binary_decision_tree']['accuracy']:.4f}, F1: {layer_results['binary_decision_tree']['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if input_location == \"sae_feature\":\n",
    "    input_file = \"decision_trees/results_sae_feature_100.pkl\"\n",
    "elif input_location == \"mlp_neuron\":\n",
    "    input_file = \"decision_trees/results_mlp_neuron_100.pkl\"\n",
    "else:\n",
    "    raise ValueError(f\"Invalid input location: {input_location}\")\n",
    "\n",
    "with open(input_file, \"rb\") as f:\n",
    "    decision_trees = pickle.load(f)\n",
    "\n",
    "if input_location == \"mlp_neuron\":\n",
    "    d_model = 2048\n",
    "elif input_location == \"sae_feature\":\n",
    "    d_model = 4096\n",
    "else:\n",
    "    d_model = 512\n",
    "\n",
    "\n",
    "custom_function = custom_functions[0]\n",
    "print(custom_function.__name__)\n",
    "\n",
    "effect_per_layer = []\n",
    "for i in range(8):\n",
    "\n",
    "    intervention_layers = [i]\n",
    "    selected_features = {}\n",
    "    # intervention_layers = list(range(8))\n",
    "\n",
    "    # for layer in intervention_layers:\n",
    "    #     selected_features[layer] = torch.ones(d_model, dtype=torch.bool)\n",
    "\n",
    "    for layer in intervention_layers:\n",
    "        threshold = 0.7\n",
    "        all_f1s = decision_trees[layer][custom_function.__name__]['decision_tree']['r2']\n",
    "        good_f1s = (all_f1s > threshold)\n",
    "        selected_features[layer] = good_f1s\n",
    "\n",
    "    # logits_clean_BLV, logits_patch_BLV = interventions(\n",
    "    #     model=model,\n",
    "    #     model_name=model_name,\n",
    "    #     train_data=data,\n",
    "    #     selected_features=selected_features,\n",
    "    #     decision_trees=None,\n",
    "    #     ae_dict=ae_dict,\n",
    "    #     submodule_dict=submodule_dict,\n",
    "    #     custom_function=None,\n",
    "    #     layers=intervention_layers,\n",
    "    #     ablation_method=\"zero\",\n",
    "    # )\n",
    "\n",
    "    logits_clean_BLV, logits_patch_BLV = interventions(\n",
    "        model=model,\n",
    "        model_name=model_name,\n",
    "        train_data=data,\n",
    "        selected_features=selected_features,\n",
    "        decision_trees=decision_trees,\n",
    "        ae_dict=ae_dict,\n",
    "        submodule_dict=submodule_dict,\n",
    "        custom_function=custom_function,\n",
    "        layers=intervention_layers,\n",
    "        ablation_method=\"dt\",\n",
    "        ablate_not_selected=ablate_not_selected,\n",
    "        add_error=add_error,\n",
    "    )\n",
    "\n",
    "    diff_BLV = logits_clean_BLV - logits_patch_BLV\n",
    "\n",
    "    def compute_kl_divergence(logits_clean_BLV, logits_patch_BLV):\n",
    "        # Apply softmax to get probability distributions\n",
    "        log_probs_clean_BLV = torch.nn.functional.log_softmax(logits_clean_BLV, dim=-1)\n",
    "        log_probs_patch_BLV = torch.nn.functional.log_softmax(logits_patch_BLV, dim=-1)\n",
    "        \n",
    "        # Compute KL divergence\n",
    "        kl_div_BLV = torch.nn.functional.kl_div(log_probs_patch_BLV, log_probs_clean_BLV.exp(), reduction='none', log_target=False)\n",
    "        \n",
    "        # Sum over the vocabulary dimension\n",
    "        kl_div_BL = kl_div_BLV.sum(dim=-1)\n",
    "        \n",
    "        return kl_div_BL\n",
    "    kl_div_BL = compute_kl_divergence(logits_clean_BLV, logits_patch_BLV)\n",
    "    print(kl_div_BL.mean())\n",
    "    effect_per_layer.append(kl_div_BL.mean())\n",
    "    # print(compute_kl_divergence(logits_clean_BLV, logits_patch_BLV))\n",
    "\n",
    "ablation_name = input_location\n",
    "if add_error:\n",
    "    ablation_name += \"_add_reconstruction_error\"\n",
    "ablations[input_location] = effect_per_layer\n",
    "print(ablations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(ablations)\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for input_location, effect_per_layer in ablations.items():\n",
    "    plt.plot(range(8), effect_per_layer, label=input_location)\n",
    "\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel('Effect (KL Divergence)')\n",
    "plt.title('Effect per Layer for Activating All Neurons with Decision Trees at Different Locations')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.yscale('log')\n",
    "\n",
    "# plt.show()\n",
    "plt.savefig(\"kl_divergence.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise ValueError(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to view metrics for a specific layer\n",
    "def view_layer_metrics(\n",
    "    layer: int,\n",
    "    threshold: float,\n",
    "    data: dict,\n",
    "    custom_function_name: str,\n",
    "    neuron_acts: dict,\n",
    "    results: dict,\n",
    "    linear_reg: bool = False,\n",
    "):\n",
    "    if layer not in results:\n",
    "        print(f\"Layer {layer} not found in results.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n\\nMetrics for Layer {layer}:\")\n",
    "\n",
    "    print(\"Decision Tree:\")\n",
    "    # print(f\"  MSE: {results[layer][custom_function_name]['decision_tree']['mse']}\")\n",
    "    # print(f\"  R2: {results[layer][custom_function_name]['decision_tree']['r2']}\")\n",
    "\n",
    "    dt_mse = results[layer][custom_function_name][\"decision_tree\"][\"mse\"]\n",
    "    dt_r2 = results[layer][custom_function_name][\"decision_tree\"][\"r2\"]\n",
    "\n",
    "    # print(\"\\nNeuron-specific metrics:\")\n",
    "    # print(f\"Decision Tree - Mean MSE: {np.mean(dt_mse)}, Mean R2: {np.mean(dt_r2)}\")\n",
    "\n",
    "    dt_r2_tensor = torch.tensor(dt_r2)\n",
    "\n",
    "    good_dt_r2 = (dt_r2_tensor > threshold).sum().item()\n",
    "    print(f\"Number of neurons with R2 > {threshold} (Decision Tree): {good_dt_r2}\")\n",
    "\n",
    "    f1 = results[layer][custom_function_name][\"binary_decision_tree\"][\"f1\"]\n",
    "\n",
    "    print(\"\\nBinary Decision Tree Metrics:\")\n",
    "    # print(f\"  Accuracy: {np.mean(accuracy)}\")\n",
    "    # print(f\"  Precision: {np.mean(precision)}\")\n",
    "    # print(f\"  Recall: {np.mean(recall)}\")\n",
    "    # print(f\"  F1: {np.mean(f1)}\")\n",
    "\n",
    "    good_f1 = (torch.tensor(f1) > threshold).sum().item()\n",
    "    print(f\"Number of neurons with F1 > {threshold} {good_f1}\")\n",
    "\n",
    "    return good_f1\n",
    "\n",
    "accuracy_by_layer = {}\n",
    "thresholds = [0.7, 0.8, 0.9]\n",
    "\n",
    "for custom_function in custom_functions:\n",
    "    print(f\"\\n\\n{custom_function.__name__}\\n\")\n",
    "    accuracy_by_layer[custom_function.__name__] = {}\n",
    "    for threshold in thresholds:\n",
    "        print(f\"\\nThreshold: {threshold}\")\n",
    "        accuracy_by_layer[custom_function.__name__][threshold] = []\n",
    "        for layer in layers:\n",
    "            f1_count = view_layer_metrics(layer, threshold, data, custom_function.__name__, neuron_acts, results)\n",
    "            accuracy_by_layer[custom_function.__name__][threshold].append(f1_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming layers, custom_functions, and accuracy_by_layer are defined\n",
    "\n",
    "function_labels = {othello_utils.games_batch_to_input_tokens_classifier_input_BLC: \"Input Tokens\",\n",
    "                     othello_utils.games_batch_to_input_tokens_parity_classifier_input_BLC: \"Input Tokens and Parity\",\n",
    "                     othello_utils.games_batch_to_input_tokens_flipped_classifier_input_BLC: \"Input Tokens and Flipped\",\n",
    "                   othello_utils.games_batch_to_board_state_and_input_tokens_classifier_input_BLC: \"Board State and Input Tokens\",\n",
    "                   othello_utils.games_batch_to_board_state_classifier_input_BLC: \"Board State\",\n",
    "                   othello_utils.games_batch_to_input_tokens_flipped_bs_classifier_input_BLC: \"Input Tokens and Flipped with Board\"}\n",
    "\n",
    "colors = ['b', 'g', 'r']  # Colors for different thresholds\n",
    "markers = ['o', 's', '^']  # Markers for different thresholds\n",
    "\n",
    "for custom_function in custom_functions:\n",
    "    function_name = custom_function.__name__\n",
    "    \n",
    "    plt.figure(figsize=(12, 7))\n",
    "    \n",
    "    for i, threshold in enumerate(thresholds):\n",
    "        f1_counts = accuracy_by_layer[function_name][threshold]\n",
    "        \n",
    "        plt.plot(layers, f1_counts, color=colors[i], marker=markers[i], \n",
    "                 label=f'Threshold: {threshold}', linewidth=2, markersize=8)\n",
    "        \n",
    "        # Optionally, add value labels on each point\n",
    "        for j, count in enumerate(f1_counts):\n",
    "            plt.annotate(str(count), (layers[j], count), \n",
    "                         textcoords=\"offset points\", xytext=(0,5), ha='center', \n",
    "                         fontsize=8, color=colors[i])\n",
    "    \n",
    "    plt.title(f'Number of Neurons with F1 > Threshold for \\n{function_labels[custom_function]} by Layer \\n{dataset_size} datapoints Input location: {input_location} depth: {MAX_DEPTH}', fontsize=14)\n",
    "    plt.xlabel('Layer Number', fontsize=12)\n",
    "    plt.ylabel('F1 Count', fontsize=12)\n",
    "    plt.legend(loc='best', fontsize=10)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    output_filename = f\"images/{input_location}_{function_name}_inputs_{dataset_size}_depth_{MAX_DEPTH}_f1_count_by_layer_all_thresholds.png\"\n",
    "    plt.savefig(output_filename, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "print(\"All graphs have been created and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "output_filename = f\"decision_trees/results_{input_location}_{dataset_size}.pkl\"\n",
    "with open(output_filename, \"wb\") as encoded_BLF:\n",
    "    pickle.dump(results, encoded_BLF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 1\n",
    "neuron_idx = 421\n",
    "custom_function_name = custom_functions[0].__name__\n",
    "layer_dt = results[layer][custom_function_name]['decision_tree']['model']\n",
    "layer_dt = results[layer][custom_function_name]['binary_decision_tree']['model']\n",
    "\n",
    "games_BLC = data[custom_function_name]\n",
    "\n",
    "feature_names = []\n",
    "\n",
    "X_binary_train, X_binary_test, y_binary_train, y_binary_test = prepare_data(\n",
    "    games_BLC, binary_acts[layer]\n",
    ")\n",
    "accuracy, precision, recall, f1 = calculate_binary_metrics(\n",
    "    results[layer][custom_function_name][\"binary_decision_tree\"][\"model\"], X_binary_test, y_binary_test\n",
    ")\n",
    "\n",
    "print(f\"Neuron 421 F1: {f1[neuron_idx]}\")\n",
    "\n",
    "for i in range(games_BLC.shape[2]):\n",
    "    if i < 64:\n",
    "        square = idx_to_square_notation(i)\n",
    "        feature_names.append(f\"Input_{square}\")\n",
    "    elif i < 128:\n",
    "        j = i - 64\n",
    "        square = idx_to_square_notation(j)\n",
    "        feature_names.append(f\"Occupied_{square}\")\n",
    "    else:\n",
    "        feature_names.append(f\"Output_{i}\")\n",
    "\n",
    "print_decision_tree_rules(layer_dt, feature_names=feature_names, neuron_index=neuron_idx, max_depth=5)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "othello",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
